# -*- coding: utf-8 -*-
"""Max Activating Samples

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sweZiGGRDqX1ioLvqJdxh-tYkspjzk9h
"""

import torch
import pandas as pd
from tqdm import tqdm
from datasets import load_dataset
from sae_lens import SAE, HookedSAETransformer

device = "cuda" if torch.cuda.is_available() else "cpu"

model = HookedSAETransformer.from_pretrained("gpt2-small", device=device)

layer_name = "blocks.7.hook_resid_pre"
sae = SAE.from_pretrained(
    release="gpt2-small-res-jb",
    sae_id=layer_name,
    device=device,
)

dataset = load_dataset("glue", "sst2", split="validation[:500]")  # subset for speed
texts = dataset["sentence"]
labels = dataset["label"]

label_map = {0: "negative", 1: "positive"}

batch_size = 8
max_len = 64

feature_counts = torch.zeros(sae.cfg.d_sae, device=device)

print("Counting feature frequencies...")
for i in tqdm(range(0, len(texts), batch_size)):
    batch_texts = texts[i:i+batch_size]
    tokens = model.to_tokens(batch_texts)[:, :max_len].to(device)

    _, cache = model.run_with_cache(tokens, names_filter=[sae.cfg.metadata.hook_name])
    sae_in = cache[sae.cfg.metadata.hook_name]

    feature_acts = sae.encode(sae_in).squeeze()  # (batch, seq, d_sae)
    feature_counts += (feature_acts > 0).sum(dim=(0, 1))

#top 10 frequent features
top_features = torch.topk(feature_counts, 10).indices.tolist()
print("\nTop 10 most frequent features:", top_features)


max_activation_examples = {f: {"sentence": None, "label": None, "activation": -float("inf")} for f in top_features}

print("Collecting max-activation sentences...")
for i in tqdm(range(0, len(texts), batch_size)):
    batch_texts = texts[i:i+batch_size]
    batch_labels = labels[i:i+batch_size]

    tokens = model.to_tokens(batch_texts)[:, :max_len].to(device)

    _, cache = model.run_with_cache(tokens, names_filter=[sae.cfg.metadata.hook_name])
    sae_in = cache[sae.cfg.metadata.hook_name]

    feature_acts = sae.encode(sae_in).squeeze()
    seq_max = feature_acts.max(dim=1).values

    for j, sentence in enumerate(batch_texts):
        for f_idx, f in enumerate(top_features):
            act = seq_max[j, f_idx].item()
            if act > max_activation_examples[f]["activation"]:
                max_activation_examples[f] = {
                    "sentence": sentence,
                    "label": label_map[batch_labels[j]],
                    "activation": act
                }

rows = []
for f in top_features:
    data = max_activation_examples[f]
    print(f"\nFeature {f}:")
    print(f"Activation: {data['activation']:.4f}")
    print(f"Sentence: {data['sentence']}")
    print(f"Label: {data['label']}")
    rows.append({
        "feature": f,
        "activation": data['activation'],
        "sentence": data['sentence'],
        "label": data['label']
    })

df = pd.DataFrame(rows)
df.to_csv("top_feature_max_sentences.csv", index=False)
print("\nâœ… Saved to top_feature_max_sentences.csv")