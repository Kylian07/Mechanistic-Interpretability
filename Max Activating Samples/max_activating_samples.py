# -*- coding: utf-8 -*-
"""max-activating-samples.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12q2PS-P7QjsR7VF-O6M9ojFL7dxAMtf0
"""

!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn

import torch
import torch.nn as nn
from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2Model
import numpy as np
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
gpt2 = GPT2Model.from_pretrained("gpt2", output_hidden_states=True).to(device)
gpt2.eval()
dataset = load_dataset("glue", "sst2")

pos_samples = [(s, l) for s, l in zip(dataset["validation"]["sentence"], dataset["validation"]["label"]) if l == 1][:25]
neg_samples = [(s, l) for s, l in zip(dataset["validation"]["sentence"], dataset["validation"]["label"]) if l == 0][:25]
samples = pos_samples + neg_samples
texts, labels = zip(*samples)
enc = tokenizer(list(texts), padding=True, truncation=True, max_length=64, return_tensors="pt")
input_ids = enc["input_ids"].to(device)
attention_mask = enc["attention_mask"].to(device)

class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=512, sparsity_weight=1e-3):
        super().__init__()
        self.encoder = nn.Linear(input_dim, hidden_dim)
        self.decoder = nn.Linear(hidden_dim, input_dim)
        self.sparsity_weight = sparsity_weight
    def forward(self, x):
        z = torch.relu(self.encoder(x))
        x_hat = self.decoder(z)
        return z, x_hat
    def loss_fn(self, x, x_hat, z):
        recon_loss = torch.nn.functional.mse_loss(x_hat, x)
        sparsity_loss = torch.mean(torch.abs(z))
        return recon_loss + self.sparsity_weight * sparsity_loss

with torch.no_grad():
    outputs = gpt2(input_ids, attention_mask=attention_mask)
    last_layer_hidden = outputs.hidden_states[-1].mean(dim=1).to(device)

sae = SparseAutoencoder(768, hidden_dim=512).to(device)
opt = torch.optim.Adam(sae.parameters(), lr=1e-4)
for _ in tqdm(range(100), desc="Training SAE (Last Layer)"):
    z, x_hat = sae(last_layer_hidden)
    loss = sae.loss_fn(last_layer_hidden, x_hat, z)
    opt.zero_grad()
    loss.backward()
    opt.step()

with torch.no_grad():
    sae_codes, _ = sae(last_layer_hidden)
sae_codes = sae_codes.cpu().numpy()


mean_activations = sae_codes.mean(axis=0)
top_features = np.argsort(mean_activations)[-10:][::-1]

print(f"\nTop 10 SAE features with highest average activation in the last GPT2 layer: {top_features}")

print("Max activating samples for each feature (last layer):")
for feature_idx in top_features:
    sample_idx = np.argmax(sae_codes[:, feature_idx])
    activation = sae_codes[sample_idx, feature_idx]
    sentence = texts[sample_idx]
    label_str = "Positive" if labels[sample_idx]==1 else "Negative"
    print(f"  Feature {feature_idx}: Sample S{sample_idx+1} ({label_str}), Activation={activation:.4f}")
    print(f"    Text: '{sentence}'")

print("\nDone: Listed max activating samples for top 10 SAE features in the last GPT2 layer.")