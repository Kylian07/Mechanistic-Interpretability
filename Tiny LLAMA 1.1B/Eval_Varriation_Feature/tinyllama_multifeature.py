# -*- coding: utf-8 -*-
"""TinyLlama_multifeature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jRhHy41g9-XYB8K3RgPhnFK6CmZEDYR7
"""

# -*- coding: utf-8 -*-
"""
Complete SAE Dimension Sweep for TinyLlama-1.1B Model
Trains and evaluates SAEs with different dictionary dimensions
(Custom model loading for unsupported models)
"""

import os
import torch
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import gc
from tqdm import tqdm
from torch.utils.data import DataLoader
from datasets import load_dataset, load_from_disk
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.nn.functional as F

# SAE imports
from sae_lens import (
    LanguageModelSAERunnerConfig,
    SAETrainingRunner,
    StandardTrainingSAE,
    StandardTrainingSAEConfig,
    LoggingConfig,
)
from transformer_lens import HookedTransformer
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig

# ========================
# SETUP AND DATA PREPARATION
# ========================

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Model configuration for TinyLlama-1.1B
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# TinyLlama uses eos_token as pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"Loaded tokenizer for {model_name}")

# Load and prepare SST-2 dataset
try:
    # Try loading existing tokenized dataset
    tokenized_train = load_from_disk("/kaggle/working/tokenized_sst2_tinyllama/train")
    tokenized_val = load_from_disk("/kaggle/working/tokenized_sst2_tinyllama/validation")
    print("✓ Loaded existing tokenized SST-2 dataset")
except:
    print("Creating tokenized SST-2 dataset...")

    # Load SST-2 dataset
    dataset = load_dataset("glue", "sst2")

    # Tokenize function
    def tokenize(batch):
        return tokenizer(batch["sentence"],
                         padding='max_length',
                         truncation=True,
                         max_length=128)

    tokenized_train = dataset["train"].map(tokenize, batched=True)
    tokenized_val = dataset["validation"].map(tokenize, batched=True)
    tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask"])
    tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])

    # Save tokenized dataset locally
    save_dir = "/kaggle/working/tokenized_sst2_tinyllama"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    tokenized_train.save_to_disk(f"{save_dir}/train")
    tokenized_val.save_to_disk(f"{save_dir}/validation")

    print(f"✓ Tokenized SST-2 saved to {save_dir}")

# ========================
# CUSTOM HOOKED TRANSFORMER SETUP
# ========================

def create_hooked_tinyllama(device):
    """Create HookedTransformer for TinyLlama"""
    print("Creating HookedTransformer for TinyLlama...")

    # Load HF model
    hf_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    )

    # Create config
    cfg = HookedTransformerConfig(
        n_layers=22,
        d_model=2048,
        d_head=64,
        n_heads=32,
        d_mlp=5632,
        d_vocab=32000,
        n_ctx=2048,
        act_fn="silu",
        normalization_type="RMS",
        positional_embedding_type="rotary",
        rotary_dim=64,
        final_rms=True,
        gated_mlp=True,
    )

    # Create HookedTransformer and load weights
    model = HookedTransformer(cfg)
    model.load_and_process_state_dict(
        hf_model.state_dict(),
        fold_ln=False,
        center_writing_weights=False,
        center_unembed=False,
        refactor_factored_attn_matrices=False,
    )
    model = model.to(device)
    model.set_tokenizer(tokenizer)

    # Clean up HF model
    del hf_model
    torch.cuda.empty_cache()
    gc.collect()

    print("✓ HookedTransformer created successfully!")
    return model

# ========================
# SAE TRAINING FUNCTION
# ========================

def train_sae(d_sae, train_dataset, hooked_model, training_steps=20000):
    """Train a single SAE with specified dimension"""
    print(f"\n{'='*60}")
    print(f"Training SAE with d_sae={d_sae} (expansion factor: {d_sae/2048:.1f}x)")
    print(f"{'='*60}")

    # Training parameters
    batch_size = 64
    total_training_tokens = training_steps * batch_size
    lr_warm_up_steps = 0
    lr_decay_steps = training_steps // 5
    l1_warm_up_steps = training_steps // 20

    # Create checkpoint directory BEFORE training
    checkpoint_dir = f"/kaggle/working/checkpoints_d{d_sae}"
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir, exist_ok=True)
        print(f"✓ Created checkpoint directory: {checkpoint_dir}")

    # SAE configuration
    cfg = LanguageModelSAERunnerConfig(
        model_name="custom-tinyllama",
        hook_name="blocks.21.hook_mlp_out",
        dataset_path=None,
        is_dataset_tokenized=True,
        streaming=False,
        sae=StandardTrainingSAEConfig(
            d_in=2048,
            d_sae=d_sae,
            apply_b_dec_to_input=False,
            normalize_activations="expected_average_only_in",
            l1_coefficient=0.01,
            l1_warm_up_steps=l1_warm_up_steps,
        ),
        lr=5e-5,
        adam_beta1=0.9,
        adam_beta2=0.999,
        lr_scheduler_name="constant",
        lr_warm_up_steps=lr_warm_up_steps,
        lr_decay_steps=lr_decay_steps,
        train_batch_size_tokens=batch_size,
        context_size=128,
        n_batches_in_buffer=64,
        training_tokens=total_training_tokens,
        store_batch_size_prompts=16,
        feature_sampling_window=1000,
        dead_feature_window=1000,
        dead_feature_threshold=1e-4,
        logger=LoggingConfig(
            log_to_wandb=False,
            wandb_project=f"sae_tinyllama_d{d_sae}",
            wandb_log_frequency=30,
            eval_every_n_wandb_logs=20,
        ),
        device=device,
        seed=42,
        n_checkpoints=1,
        checkpoint_path=checkpoint_dir,
        dtype="float32",
    )

    # Train SAE with custom model
    try:
        print(f"Starting training for d_sae={d_sae}...")
        runner = SAETrainingRunner(
            cfg,
            override_dataset=train_dataset,
            override_model=hooked_model
        )
        sparse_autoencoder = runner.run()
        print(f"✓ Training completed for d_sae={d_sae}")
    except KeyboardInterrupt:
        print(f"✗ Training interrupted for d_sae={d_sae}")
        raise
    except Exception as e:
        print(f"✗ Training failed for d_sae={d_sae}: {str(e)}")
        raise

    # Save the trained SAE
    save_path = f"/kaggle/working/sae_tinyllama_d{d_sae}"
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    # Save model state dict
    torch.save(sparse_autoencoder.state_dict(),
               os.path.join(save_path, "model_state_dict.pt"))

    print(f"✓ SAE model saved to {save_path}")

    return sparse_autoencoder

# ========================
# SAE EVALUATION FUNCTION
# ========================

def evaluate_sae(sae_model, d_sae, val_dataset, tinyllama_model):
    """Evaluate a single SAE configuration"""
    print(f"Evaluating SAE with d_sae={d_sae}...")

    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)
    ce_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum')
    epsilon = 1e-10

    # Metrics accumulators
    total_ce_loss_without_sae = 0.0
    total_ce_loss_with_sae = 0.0
    total_ce_loss_with_ablation = 0.0
    total_kl_div_with_sae = 0.0
    total_kl_div_with_ablation = 0.0
    total_mse = 0.0
    total_explained_variance = 0.0
    total_cossim = 0.0
    total_l2_norm_in = 0.0
    total_l2_norm_out = 0.0
    total_l2_ratio = 0.0
    total_rel_reconstruction_bias = 0.0
    total_l0 = 0
    total_l1 = 0.0
    total_tokens = 0
    total_elements = 0
    total_samples = 0

    hooked_activations = {}
    replace_mode = [None]

    def activation_hook(module, input, output):
        """Hook function to capture and optionally replace MLP activations."""
        if isinstance(output, tuple):
            actual_output = output[0]
        else:
            actual_output = output

        hooked_activations['acts'] = actual_output.detach()

        if replace_mode[0] == "sae":
            batch_size, seq_len, hidden_dim = actual_output.shape
            flat_acts = actual_output.view(batch_size * seq_len, hidden_dim)
            with torch.no_grad():
                sae_out = sae_model(flat_acts)
            replacement = sae_out.view(batch_size, seq_len, hidden_dim)

            if isinstance(output, tuple):
                return (replacement,) + output[1:]
            else:
                return replacement

        elif replace_mode[0] == "ablation":
            replacement = torch.zeros_like(actual_output)
            if isinstance(output, tuple):
                return (replacement,) + output[1:]
            else:
                return replacement

        return output

    # Register hook on TinyLlama layer 21 MLP
    hook_handle = tinyllama_model.model.layers[21].mlp.register_forward_hook(activation_hook)

    try:
        for batch_idx, batch in enumerate(tqdm(val_loader, desc=f"Eval d_sae={d_sae}")):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            batch_size, seq_len = input_ids.size()
            num_tokens = torch.sum(attention_mask).item()

            # ===== BASELINE (no replacement) =====
            replace_mode[0] = None
            with torch.no_grad():
                outputs_baseline = tinyllama_model(input_ids, attention_mask=attention_mask)
                logits_baseline = outputs_baseline.logits

                ce_loss_baseline = ce_loss_fn(
                    logits_baseline[:, :-1, :].reshape(-1, logits_baseline.size(-1)),
                    input_ids[:, 1:].reshape(-1),
                )
                total_ce_loss_without_sae += ce_loss_baseline.item()

                baseline_probs = torch.softmax(logits_baseline[:, :-1, :], dim=-1)
                del logits_baseline, outputs_baseline
                torch.cuda.empty_cache()

            # ===== WITH SAE REPLACEMENT =====
            replace_mode[0] = "sae"
            with torch.no_grad():
                outputs_sae = tinyllama_model(input_ids, attention_mask=attention_mask)
                logits_sae = outputs_sae.logits

                ce_loss_sae = ce_loss_fn(
                    logits_sae[:, :-1, :].reshape(-1, logits_sae.size(-1)),
                    input_ids[:, 1:].reshape(-1),
                )
                total_ce_loss_with_sae += ce_loss_sae.item()

                sae_log_probs = torch.log_softmax(logits_sae[:, :-1, :], dim=-1)
                kl_div_sae = torch.sum(baseline_probs * (torch.log(baseline_probs + epsilon) - sae_log_probs))
                total_kl_div_with_sae += kl_div_sae.item()

                del logits_sae, outputs_sae, sae_log_probs, kl_div_sae
                torch.cuda.empty_cache()

            # ===== WITH ABLATION REPLACEMENT =====
            replace_mode[0] = "ablation"
            with torch.no_grad():
                outputs_abl = tinyllama_model(input_ids, attention_mask=attention_mask)
                logits_abl = outputs_abl.logits

                ce_loss_abl = ce_loss_fn(
                    logits_abl[:, :-1, :].reshape(-1, logits_abl.size(-1)),
                    input_ids[:, 1:].reshape(-1),
                )
                total_ce_loss_with_ablation += ce_loss_abl.item()

                abl_log_probs = torch.log_softmax(logits_abl[:, :-1, :], dim=-1)
                kl_div_abl = torch.sum(baseline_probs * (torch.log(baseline_probs + epsilon) - abl_log_probs))
                total_kl_div_with_ablation += kl_div_abl.item()

                del logits_abl, outputs_abl, abl_log_probs, kl_div_abl, baseline_probs
                torch.cuda.empty_cache()

            # ===== RECONSTRUCTION METRICS =====
            replace_mode[0] = None
            with torch.no_grad():
                _ = tinyllama_model(input_ids, attention_mask=attention_mask)

            original_activations = hooked_activations['acts']
            batch_size_, seq_len_, d_in = original_activations.shape

            flat_acts = original_activations.view(batch_size_ * seq_len_, d_in)
            with torch.no_grad():
                reconstruction = sae_model(flat_acts)

            # MSE
            mse = F.mse_loss(reconstruction, flat_acts, reduction='mean').item()
            total_mse += mse * batch_size

            # Explained Variance
            var_target = torch.var(flat_acts, dim=0, unbiased=False)
            var_residual = torch.var(flat_acts - reconstruction, dim=0, unbiased=False)
            ev = 1 - (var_residual / (var_target + epsilon))
            explained_variance = torch.mean(ev).item()
            total_explained_variance += explained_variance * batch_size
            del var_target, var_residual, ev

            # Cosine similarity
            cossim = F.cosine_similarity(reconstruction, flat_acts, dim=1).mean().item()
            total_cossim += cossim * batch_size

            # Shrinkage metrics
            l2_norm_in = torch.norm(flat_acts, dim=1).mean().item()
            l2_norm_out = torch.norm(reconstruction, dim=1).mean().item()
            l2_ratio = l2_norm_out / (l2_norm_in + epsilon)
            rel_bias = reconstruction.abs().sum(dim=1).mean().item() / (flat_acts.abs().sum(dim=1).mean().item() + epsilon)

            total_l2_norm_in += l2_norm_in * batch_size
            total_l2_norm_out += l2_norm_out * batch_size
            total_l2_ratio += l2_ratio * batch_size
            total_rel_reconstruction_bias += rel_bias * batch_size

            # Sparsity metrics
            l0 = torch.count_nonzero(reconstruction).item()
            l1 = reconstruction.abs().sum().item()

            total_l0 += l0
            total_l1 += l1
            total_elements += reconstruction.numel()

            # Clean up
            del flat_acts, reconstruction, original_activations

            total_tokens += num_tokens
            total_samples += batch_size

            # Periodic garbage collection
            if batch_idx % 10 == 0:
                torch.cuda.empty_cache()
                gc.collect()

    finally:
        hook_handle.remove()

    # Calculate averages
    ce_loss_without_sae_avg = total_ce_loss_without_sae / total_tokens
    ce_loss_with_sae_avg = total_ce_loss_with_sae / total_tokens
    ce_loss_with_ablation_avg = total_ce_loss_with_ablation / total_tokens

    kl_div_with_sae_avg = total_kl_div_with_sae / total_tokens
    kl_div_with_ablation_avg = total_kl_div_with_ablation / total_tokens

    ce_loss_score = (ce_loss_with_ablation_avg - ce_loss_with_sae_avg) / (ce_loss_with_ablation_avg - ce_loss_without_sae_avg + epsilon)
    kl_div_score = (kl_div_with_ablation_avg - kl_div_with_sae_avg) / (kl_div_with_ablation_avg + epsilon)

    avg_mse = total_mse / total_samples
    avg_explained_variance = total_explained_variance / total_samples
    avg_cossim = total_cossim / total_samples
    avg_l2_norm_in = total_l2_norm_in / total_samples
    avg_l2_norm_out = total_l2_norm_out / total_samples
    avg_l2_ratio = total_l2_ratio / total_samples
    avg_rel_reconstruction_bias = total_rel_reconstruction_bias / total_samples
    avg_l0 = total_l0 / total_elements
    avg_l1 = total_l1 / total_samples

    return {
        'dimension': d_sae,
        'ce_loss_without_sae': ce_loss_without_sae_avg,
        'ce_loss_with_sae': ce_loss_with_sae_avg,
        'ce_loss_with_ablation': ce_loss_with_ablation_avg,
        'ce_loss_score': ce_loss_score,
        'kl_div_with_sae': kl_div_with_sae_avg,
        'kl_div_with_ablation': kl_div_with_ablation_avg,
        'kl_div_score': kl_div_score,
        'mse': avg_mse,
        'explained_variance': avg_explained_variance,
        'cosine_similarity': avg_cossim,
        'l2_norm_in': avg_l2_norm_in,
        'l2_norm_out': avg_l2_norm_out,
        'l2_ratio': avg_l2_ratio,
        'relative_bias': avg_rel_reconstruction_bias,
        'l0_sparsity': avg_l0,
        'l1_sparsity': avg_l1,
        'total_tokens': total_tokens,
        'total_samples': total_samples,
    }

# ========================
# MAIN EXECUTION
# ========================

if __name__ == "__main__":
    print("="*70)
    print("SAE DIMENSION SWEEP FOR TINYLLAMA-1.1B")
    print("="*70)

    # Create custom HookedTransformer for TinyLlama
    print("\nStep 1: Setting up TinyLlama HookedTransformer...")
    hooked_tinyllama_template = create_hooked_tinyllama(device)

    # Define SAE dimensions to test
    d_in = 2048
    sae_dimensions = [2048, 4096, 8192, 16384, 32768]

    # Results storage
    all_results = []

    # Training parameters
    TRAIN_NEW_SAES = True  # Set to False to skip training
    TRAINING_STEPS = 20000  # Reduced for faster experimentation

    print(f"\nStep 2: Training SAEs with dimensions: {sae_dimensions}")
    print(f"Training steps per SAE: {TRAINING_STEPS}")
    print(f"Train new SAEs: {TRAIN_NEW_SAES}")

    for i, d_sae in enumerate(sae_dimensions, 1):
        print(f"\n[{i}/{len(sae_dimensions)}] ", end="")

        # Configure SAE
        sae_cfg = StandardTrainingSAEConfig(
            d_in=d_in,
            d_sae=d_sae,
            apply_b_dec_to_input=False,
            normalize_activations="expected_average_only_in",
            l1_coefficient=0.01,
            l1_warm_up_steps=TRAINING_STEPS//20,
        )

        if TRAIN_NEW_SAES:
            # Train new SAE with custom model
            try:
                sae_model = train_sae(d_sae, tokenized_train, hooked_tinyllama_template, TRAINING_STEPS)
            except Exception as e:
                print(f"✗ Failed to train SAE d_sae={d_sae}: {str(e)}")
                continue
        else:
            # Load existing SAE
            try:
                sae_model = StandardTrainingSAE(sae_cfg).to(device)
                weight_file = f"/kaggle/working/sae_tinyllama_d{d_sae}/model_state_dict.pt"
                state_dict = torch.load(weight_file, map_location=device)
                sae_model.load_state_dict(state_dict)
                print(f"✓ Loaded existing SAE from {weight_file}")
            except FileNotFoundError:
                print(f"✗ SAE weights not found for d_sae={d_sae}, skipping...")
                continue

        sae_model.eval()

        # Evaluate SAE using AutoModel
        try:
            print("Loading TinyLlama AutoModel for evaluation...")
            tinyllama_eval_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                device_map=device,
                low_cpu_mem_usage=True
            )
            tinyllama_eval_model.eval()

            results = evaluate_sae(sae_model, d_sae, tokenized_val, tinyllama_eval_model)
            all_results.append(results)

            print(f"\n✓ Results for d_sae={d_sae}:")
            print(f"  CE Loss with SAE: {results['ce_loss_with_sae']:.4f}")
            print(f"  Cosine Similarity: {results['cosine_similarity']:.4f}")
            print(f"  L0 Sparsity: {results['l0_sparsity']:.4f}")
            print(f"  Explained Variance: {results['explained_variance']:.4f}")

            # Clean up
            del sae_model, tinyllama_eval_model
            torch.cuda.empty_cache()
            gc.collect()

        except Exception as e:
            print(f"✗ Failed to evaluate SAE d_sae={d_sae}: {str(e)}")
            continue

    # ========================
    # PLOTTING AND RESULTS
    # ========================

    if all_results:
        print(f"\n{'='*70}")
        print("Step 3: Generating plots and saving results...")
        print(f"{'='*70}")

        # Convert to DataFrame
        results_df = pd.DataFrame(all_results)

        # Create comprehensive plots
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('SAE Metrics vs Dictionary Dimension (TinyLlama-1.1B Layer 21)',
                     fontsize=16, fontweight='bold')

        # Plot 1: CE Loss
        axes[0, 0].plot(results_df['dimension'], results_df['ce_loss_with_sae'],
                        marker='o', linewidth=3, markersize=10, color='#E74C3C')
        axes[0, 0].set_xlabel('SAE Dimension (d_sae)', fontsize=12)
        axes[0, 0].set_ylabel('Cross-Entropy Loss', fontsize=12)
        axes[0, 0].set_title('CE Loss with SAE vs Dimension', fontsize=13, fontweight='bold')
        axes[0, 0].grid(True, alpha=0.3, linestyle='--')
        axes[0, 0].set_xscale('log', base=2)

        # Plot 2: Cosine Similarity
        axes[0, 1].plot(results_df['dimension'], results_df['cosine_similarity'],
                        marker='s', linewidth=3, markersize=10, color='#3498DB')
        axes[0, 1].set_xlabel('SAE Dimension (d_sae)', fontsize=12)
        axes[0, 1].set_ylabel('Cosine Similarity', fontsize=12)
        axes[0, 1].set_title('Reconstruction Cosine Similarity vs Dimension', fontsize=13, fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3, linestyle='--')
        axes[0, 1].set_xscale('log', base=2)

        # Plot 3: L0 Sparsity
        axes[1, 0].plot(results_df['dimension'], results_df['l0_sparsity'],
                        marker='^', linewidth=3, markersize=10, color='#2ECC71')
        axes[1, 0].set_xlabel('SAE Dimension (d_sae)', fontsize=12)
        axes[1, 0].set_ylabel('L0 Sparsity (fraction active)', fontsize=12)
        axes[1, 0].set_title('L0 Sparsity vs Dimension', fontsize=13, fontweight='bold')
        axes[1, 0].grid(True, alpha=0.3, linestyle='--')
        axes[1, 0].set_xscale('log', base=2)

        # Plot 4: Explained Variance
        axes[1, 1].plot(results_df['dimension'], results_df['explained_variance'],
                        marker='D', linewidth=3, markersize=10, color='#9B59B6')
        axes[1, 1].set_xlabel('SAE Dimension (d_sae)', fontsize=12)
        axes[1, 1].set_ylabel('Explained Variance', fontsize=12)
        axes[1, 1].set_title('Explained Variance vs Dimension', fontsize=13, fontweight='bold')
        axes[1, 1].grid(True, alpha=0.3, linestyle='--')
        axes[1, 1].set_xscale('log', base=2)

        plt.tight_layout()
        plt.savefig('/kaggle/working/sae_tinyllama_dimension_sweep_comprehensive.png',
                    dpi=300, bbox_inches='tight')
        print("✓ Comprehensive plot saved to: /kaggle/working/sae_tinyllama_dimension_sweep_comprehensive.png")

        # Create focused plot for CE Loss and Cosine Similarity
        fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        fig2.suptitle('Key SAE Metrics: CE Loss and Cosine Similarity vs Dimension (TinyLlama)',
                      fontsize=16, fontweight='bold')

        # CE Loss plot
        ax1.plot(results_df['dimension'], results_df['ce_loss_with_sae'],
                 marker='o', linewidth=4, markersize=12, color='#E74C3C', label='CE Loss with SAE')
        ax1.set_xlabel('SAE Dimension (d_sae)', fontsize=14)
        ax1.set_ylabel('Cross-Entropy Loss', fontsize=14)
        ax1.set_title('Cross-Entropy Loss with SAE', fontsize=15, fontweight='bold')
        ax1.grid(True, alpha=0.3, linestyle='--')
        ax1.set_xscale('log', base=2)
        ax1.legend(fontsize=12)

        # Add expansion factor labels
        for i, (dim, loss) in enumerate(zip(results_df['dimension'], results_df['ce_loss_with_sae'])):
            expansion = dim / d_in
            ax1.annotate(f'{expansion:.0f}x', (dim, loss),
                        textcoords="offset points", xytext=(0,10), ha='center', fontsize=10)

        # Cosine similarity plot
        ax2.plot(results_df['dimension'], results_df['cosine_similarity'],
                 marker='s', linewidth=4, markersize=12, color='#3498DB', label='Cosine Similarity')
        ax2.set_xlabel('SAE Dimension (d_sae)', fontsize=14)
        ax2.set_ylabel('Cosine Similarity', fontsize=14)
        ax2.set_title('Reconstruction Cosine Similarity', fontsize=15, fontweight='bold')
        ax2.grid(True, alpha=0.3, linestyle='--')
        ax2.set_xscale('log', base=2)
        ax2.legend(fontsize=12)

        # Add expansion factor labels
        for i, (dim, cossim) in enumerate(zip(results_df['dimension'], results_df['cosine_similarity'])):
            expansion = dim / d_in
            ax2.annotate(f'{expansion:.0f}x', (dim, cossim),
                        textcoords="offset points", xytext=(0,10), ha='center', fontsize=10)

        plt.tight_layout()
        plt.savefig('/kaggle/working/sae_tinyllama_ce_cossim_focused.png', dpi=300, bbox_inches='tight')
        print("✓ Focused plot saved to: /kaggle/working/sae_tinyllama_ce_cossim_focused.png")

        # Save results to CSV
        results_df.to_csv('/kaggle/working/sae_tinyllama_dimension_sweep_results.csv', index=False)
        print("✓ Results saved to: /kaggle/working/sae_tinyllama_dimension_sweep_results.csv")

        # Display results table
        print("\n" + "="*80)
        print("COMPLETE RESULTS SUMMARY - TINYLLAMA-1.1B SAE DIMENSION SWEEP")
        print("="*80)
        print(results_df[['dimension', 'ce_loss_with_sae', 'cosine_similarity',
                         'l0_sparsity', 'explained_variance', 'ce_loss_score']].round(4).to_string(index=False))
        print("="*80)

        # Print expansion factors
        print("\nExpansion Factors:")
        for dim in results_df['dimension']:
            print(f"  d_sae={dim}: {dim/d_in:.1f}x expansion")

        plt.show()
    else:
        print("\n✗ No results to plot. Make sure you have trained or have pre-trained SAEs saved.")

print("\n✓ SAE dimension sweep completed!")