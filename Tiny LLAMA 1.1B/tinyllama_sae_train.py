# -*- coding: utf-8 -*-
"""TinyLlama SAE Train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jRhHy41g9-XYB8K3RgPhnFK6CmZEDYR7
"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import torch
from transformer_lens import HookedTransformer
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig

# Step 1: Load TinyLlama from HuggingFace
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

hf_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32
)

# Step 2: Create custom TransformerLens config for TinyLlama
cfg = HookedTransformerConfig(
    n_layers=22,
    d_model=2048,
    d_head=64,
    n_heads=32,
    d_mlp=5632,
    d_vocab=32000,
    n_ctx=2048,
    act_fn="silu",
    normalization_type="RMS",
    positional_embedding_type="rotary",
    rotary_dim=64,
    final_rms=True,
    gated_mlp=True,
)

# Step 3: Convert HuggingFace model to HookedTransformer
device = "cuda" if torch.cuda.is_available() else "cpu"
tinyllama_hooked = HookedTransformer(cfg)
tinyllama_hooked.load_and_process_state_dict(
    hf_model.state_dict(),
    fold_ln=False,
    center_writing_weights=False,
    center_unembed=False,
    refactor_factored_attn_matrices=False,
)
tinyllama_hooked = tinyllama_hooked.to(device)
print("TinyLlama loaded into TransformerLens!")

# Step 4: Prepare SST-2 dataset
dataset = load_dataset("glue", "sst2")

def tokenize(batch):
    return tokenizer(batch["sentence"],
                     padding='max_length',
                     truncation=True,
                     max_length=53)

tokenized_train = dataset["train"].map(tokenize, batched=True)
tokenized_val = dataset["validation"].map(tokenize, batched=True)
tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask"])
tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])

save_dir = "/kaggle/working/tokenized_sst2_tinyllama"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

tokenized_train.save_to_disk(f"{save_dir}/train")
tokenized_val.save_to_disk(f"{save_dir}/validation")

print(f"Tokenized SST-2 saved to {save_dir}")
# Step 5: SAE Training with custom model
from datasets import load_from_disk
from sae_lens import (
    LanguageModelSAERunnerConfig,
    SAETrainingRunner,
    StandardTrainingSAEConfig,
    LoggingConfig,
)

train_dataset = load_from_disk("/kaggle/working/tokenized_sst2_tinyllama/train")
validation_dataset = load_from_disk("/kaggle/working/tokenized_sst2_tinyllama/validation")

print("Train dataset columns:", train_dataset.column_names)
print("Validation dataset columns:", validation_dataset.column_names)

# Define training parameters
total_training_steps = 80000
batch_size = 64
total_training_tokens = total_training_steps * batch_size

lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5
l1_warm_up_steps = total_training_steps // 20

# SAE Configuration (model_name can be arbitrary for custom models)
sae_cfg = LanguageModelSAERunnerConfig(
    model_name="custom-tinyllama",  # Arbitrary name for custom model
    hook_name="blocks.21.hook_mlp_out",  # TinyLlama layer 21 (last layer)
    dataset_path=None,
    is_dataset_tokenized=True,
    streaming=False,
    sae=StandardTrainingSAEConfig(
        d_in=2048,  # TinyLlama hidden dimension
        d_sae=4096,
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=l1_warm_up_steps,
    ),
    lr=5e-5,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=lr_warm_up_steps,
    lr_decay_steps=lr_decay_steps,
    train_batch_size_tokens=batch_size,
    context_size=53,
    n_batches_in_buffer=64,
    training_tokens=total_training_tokens,
    store_batch_size_prompts=16,
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    logger=LoggingConfig(
        log_to_wandb=False,
        wandb_project="sae_lens_sst2_tinyllama_custom_lastlayer",
        wandb_log_frequency=30,
        eval_every_n_wandb_logs=20,
    ),
    device=device,
    seed=42,
    n_checkpoints=1,
    checkpoint_path="checkpoints",
    dtype="float32",
)

# Run SAE training with custom TinyLlama model
print("Starting SAE training on TinyLlama (custom model)...")
runner = SAETrainingRunner(
    sae_cfg,
    override_dataset=train_dataset,
    override_model=tinyllama_hooked  # Pass custom HookedTransformer
)
sparse_autoencoder = runner.run()

save_path = "/kaggle/working/saved_sae_model_tinyllama"
os.makedirs(save_path, exist_ok=True)

if hasattr(sparse_autoencoder, "save"):
    sparse_autoencoder.save(save_path)
else:
    torch.save(sparse_autoencoder.state_dict(),
               os.path.join(save_path, "model_state_dict.pt"))

print(f"SAE model saved to {save_path}")