# -*- coding: utf-8 -*-
"""TinyLlama_evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jRhHy41g9-XYB8K3RgPhnFK6CmZEDYR7
"""

# -*- coding: utf-8 -*-
"""SAE Evaluation - TinyLlama-1.1B.ipynb"""

import torch
from torch.utils.data import DataLoader
from datasets import load_from_disk, load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.nn.functional as F
from tqdm import tqdm
import gc

# SAE imports
from sae_lens import StandardTrainingSAE, StandardTrainingSAEConfig, LanguageModelSAERunnerConfig

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load tokenizer and TinyLlama model
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# TinyLlama uses eos_token as pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("Loading TinyLlama model...")
tinyllama_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
tinyllama_model.eval()
print("TinyLlama model loaded!")

# SAE Config - adjusted for TinyLlama-1.1B
# TinyLlama: 22 layers, hidden_size=2048
sae_cfg = StandardTrainingSAEConfig(
    d_in=2048,  # TinyLlama hidden size
    d_sae=4096,  # 2x expansion factor
    apply_b_dec_to_input=False,
    normalize_activations="expected_average_only_in",
    l1_coefficient=0.01,
    l1_warm_up_steps=4000,
)
cfg = LanguageModelSAERunnerConfig(
    model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    hook_name="blocks.21.hook_mlp_out",  # Last layer for TinyLlama
    is_dataset_tokenized=True,
    streaming=False,
    sae=sae_cfg,
    device=device,
)

# Load SAE model weights
print("Loading SAE model weights...")
local_sae_path = "/kaggle/input/tiny/pytorch/default/1"  # Change path if needed
weight_file = "model_state_dict (1).pt"

try:
    state_dict = torch.load(f"{local_sae_path}/{weight_file}", map_location=device)
    sae_model = StandardTrainingSAE(cfg.sae).to(device)
    sae_model.load_state_dict(state_dict)
    sae_model.eval()
    print("SAE model loaded successfully!")
except FileNotFoundError:
    print(f"Error: SAE weights not found at {local_sae_path}/{weight_file}")
    print("Please check the path and try again")
    exit()

# Load validation dataset with smaller batch size
print("Loading validation dataset...")
val_dataset = load_from_disk("/kaggle/working/tokenized_sst2_tinyllama/validation")
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Reduced batch size

ce_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum')
epsilon = 1e-10
replace_mode = None
hooked_activations = {}

def activation_hook(module, input, output):
    """Hook function to capture and optionally replace MLP activations."""
    global replace_mode

    if isinstance(output, tuple):
        actual_output = output[0]
    else:
        actual_output = output

    hooked_activations['acts'] = actual_output.detach()

    if replace_mode == "sae":
        batch_size, seq_len, hidden_dim = actual_output.shape
        flat_acts = actual_output.view(batch_size * seq_len, hidden_dim)
        with torch.no_grad():
            sae_out = sae_model(flat_acts)
        replacement = sae_out.view(batch_size, seq_len, hidden_dim)

        if isinstance(output, tuple):
            return (replacement,) + output[1:]
        else:
            return replacement

    elif replace_mode == "ablation":
        replacement = torch.zeros_like(actual_output)
        if isinstance(output, tuple):
            return (replacement,) + output[1:]
        else:
            return replacement

    else:
        return output

# Register hook on TinyLlama model's layer 21 MLP
# TinyLlama architecture: model.layers[21].mlp
print("Registering activation hook...")
hook_handle = tinyllama_model.model.layers[21].mlp.register_forward_hook(activation_hook)

# Metric accumulators
total_ce_loss_without_sae = 0.0
total_ce_loss_with_sae = 0.0
total_ce_loss_with_ablation = 0.0

total_kl_div_with_sae = 0.0
total_kl_div_with_ablation = 0.0

total_mse = 0.0
total_explained_variance = 0.0
total_cossim = 0.0

total_l2_norm_in = 0.0
total_l2_norm_out = 0.0
total_l2_ratio = 0.0
total_rel_reconstruction_bias = 0.0

total_l0 = 0
total_l1 = 0.0

total_tokens_eval_reconstruction = 0
total_tokens_eval_sparsity_variance = 0

total_elements = 0
total_samples = 0

print("Starting evaluation...")
print("="*60)

for batch_idx, batch in enumerate(tqdm(val_loader, desc="Unified Evaluation")):
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)

    batch_size, seq_len = input_ids.size()
    num_tokens = torch.sum(attention_mask).item()

    # ===== BASELINE (no replacement) =====
    replace_mode = None
    with torch.no_grad():
        outputs_baseline = tinyllama_model(input_ids, attention_mask=attention_mask)
        logits_baseline = outputs_baseline.logits

        # Compute CE loss immediately
        ce_loss_baseline = ce_loss_fn(
            logits_baseline[:, :-1, :].reshape(-1, logits_baseline.size(-1)),
            input_ids[:, 1:].reshape(-1),
        )
        total_ce_loss_without_sae += ce_loss_baseline.item()

        # Keep only probabilities for KL computation (more memory efficient)
        baseline_probs = torch.softmax(logits_baseline[:, :-1, :], dim=-1)
        del logits_baseline, outputs_baseline
        torch.cuda.empty_cache()

    # ===== WITH SAE REPLACEMENT =====
    replace_mode = "sae"
    with torch.no_grad():
        outputs_sae = tinyllama_model(input_ids, attention_mask=attention_mask)
        logits_sae = outputs_sae.logits

        # Compute CE loss
        ce_loss_sae = ce_loss_fn(
            logits_sae[:, :-1, :].reshape(-1, logits_sae.size(-1)),
            input_ids[:, 1:].reshape(-1),
        )
        total_ce_loss_with_sae += ce_loss_sae.item()

        # Compute KL divergence in chunks to save memory
        sae_log_probs = torch.log_softmax(logits_sae[:, :-1, :], dim=-1)
        kl_div_sae = torch.sum(baseline_probs * (torch.log(baseline_probs + epsilon) - sae_log_probs))
        total_kl_div_with_sae += kl_div_sae.item()

        del logits_sae, outputs_sae, sae_log_probs, kl_div_sae
        torch.cuda.empty_cache()

    # ===== WITH ABLATION REPLACEMENT =====
    replace_mode = "ablation"
    with torch.no_grad():
        outputs_abl = tinyllama_model(input_ids, attention_mask=attention_mask)
        logits_abl = outputs_abl.logits

        # Compute CE loss
        ce_loss_abl = ce_loss_fn(
            logits_abl[:, :-1, :].reshape(-1, logits_abl.size(-1)),
            input_ids[:, 1:].reshape(-1),
        )
        total_ce_loss_with_ablation += ce_loss_abl.item()

        # Compute KL divergence
        abl_log_probs = torch.log_softmax(logits_abl[:, :-1, :], dim=-1)
        kl_div_abl = torch.sum(baseline_probs * (torch.log(baseline_probs + epsilon) - abl_log_probs))
        total_kl_div_with_ablation += kl_div_abl.item()

        del logits_abl, outputs_abl, abl_log_probs, kl_div_abl, baseline_probs
        torch.cuda.empty_cache()

    # ===== RECONSTRUCTION METRICS =====
    replace_mode = None
    with torch.no_grad():
        _ = tinyllama_model(input_ids, attention_mask=attention_mask)

    original_activations = hooked_activations['acts']
    batch_size_, seq_len_, d_in = original_activations.shape

    flat_acts = original_activations.view(batch_size_ * seq_len_, d_in)
    with torch.no_grad():
        reconstruction = sae_model(flat_acts)

    # MSE
    mse = F.mse_loss(reconstruction, flat_acts, reduction='mean').item()
    total_mse += mse * batch_size

    # Explained Variance
    var_target = torch.var(flat_acts, dim=0, unbiased=False)
    var_residual = torch.var(flat_acts - reconstruction, dim=0, unbiased=False)
    ev = 1 - (var_residual / (var_target + epsilon))
    explained_variance = torch.mean(ev).item()
    total_explained_variance += explained_variance * batch_size
    del var_target, var_residual, ev

    # Cosine similarity
    cossim = F.cosine_similarity(reconstruction, flat_acts, dim=1).mean().item()
    total_cossim += cossim * batch_size

    # Shrinkage metrics
    l2_norm_in = torch.norm(flat_acts, dim=1).mean().item()
    l2_norm_out = torch.norm(reconstruction, dim=1).mean().item()
    l2_ratio = l2_norm_out / (l2_norm_in + epsilon)
    rel_bias = reconstruction.abs().sum(dim=1).mean().item() / (flat_acts.abs().sum(dim=1).mean().item() + epsilon)

    total_l2_norm_in += l2_norm_in * batch_size
    total_l2_norm_out += l2_norm_out * batch_size
    total_l2_ratio += l2_ratio * batch_size
    total_rel_reconstruction_bias += rel_bias * batch_size

    # Sparsity metrics
    l0 = torch.count_nonzero(reconstruction).item()
    l1 = reconstruction.abs().sum().item()

    total_l0 += l0
    total_l1 += l1

    total_elements += reconstruction.numel()

    # Clean up
    del flat_acts, reconstruction, original_activations

    # Token statistics
    total_tokens_eval_reconstruction += num_tokens
    total_tokens_eval_sparsity_variance += num_tokens

    total_samples += batch_size

    # Periodic garbage collection
    if batch_idx % 10 == 0:
        torch.cuda.empty_cache()
        gc.collect()

# Remove hook
hook_handle.remove()

# Compute averages
num_tokens_total = total_tokens_eval_reconstruction
ce_loss_without_sae_avg = total_ce_loss_without_sae / num_tokens_total
ce_loss_with_sae_avg = total_ce_loss_with_sae / num_tokens_total
ce_loss_with_ablation_avg = total_ce_loss_with_ablation / num_tokens_total

# Convert KL to per-token averages
kl_div_with_sae_avg = total_kl_div_with_sae / num_tokens_total
kl_div_with_ablation_avg = total_kl_div_with_ablation / num_tokens_total

ce_loss_score = (ce_loss_with_ablation_avg - ce_loss_with_sae_avg) / (ce_loss_with_ablation_avg - ce_loss_without_sae_avg + epsilon)
kl_div_score = (kl_div_with_ablation_avg - kl_div_with_sae_avg) / (kl_div_with_ablation_avg + epsilon)

avg_mse = total_mse / total_samples
avg_explained_variance = total_explained_variance / total_samples
avg_cossim = total_cossim / total_samples

avg_l2_norm_in = total_l2_norm_in / total_samples
avg_l2_norm_out = total_l2_norm_out / total_samples
avg_l2_ratio = total_l2_ratio / total_samples
avg_rel_reconstruction_bias = total_rel_reconstruction_bias / total_samples

avg_l0 = total_l0 / total_elements
avg_l1 = total_l1 / total_samples

# Print all metrics
print("\n" + "="*60)
print("SAE EVALUATION RESULTS FOR TINYLLAMA-1.1B")
print("="*60)

print("\n### Model Behavior Preservation ###")
print(f"Cross-Entropy Loss Without SAE: {ce_loss_without_sae_avg:.4f}")
print(f"Cross-Entropy Loss With SAE: {ce_loss_with_sae_avg:.4f}")
print(f"Cross-Entropy Loss With Ablation: {ce_loss_with_ablation_avg:.4f}")
print(f"CE Loss Score: {ce_loss_score:.4f}")

print(f"\nKL Divergence With SAE: {kl_div_with_sae_avg:.6f}")
print(f"KL Divergence With Ablation: {kl_div_with_ablation_avg:.6f}")
print(f"KL Divergence Score: {kl_div_score:.6f}")

print("\n### Reconstruction Quality ###")
print(f"Reconstruction MSE: {avg_mse:.6f}")
print(f"Reconstruction Explained Variance: {avg_explained_variance:.6f}")
print(f"Reconstruction Cosine Similarity: {avg_cossim:.6f}")

print("\n### Shrinkage Analysis ###")
print(f"L2 Norm Input: {avg_l2_norm_in:.6f}")
print(f"L2 Norm Output: {avg_l2_norm_out:.6f}")
print(f"L2 Norm Ratio: {avg_l2_ratio:.6f}")
print(f"Relative Reconstruction Bias: {avg_rel_reconstruction_bias:.6f}")

print("\n### Sparsity Metrics ###")
print(f"Sparsity L0 (fraction non-zero): {avg_l0:.6f}")
print(f"Sparsity L1 (avg L1 norm): {avg_l1:.6f}")

print("\n### Token Statistics ###")
print(f"Total tokens evaluated: {total_tokens_eval_reconstruction}")
print(f"Total samples processed: {total_samples}")

print("="*60)

# Save results to file
results_file = "/kaggle/working/tinyllama_sae_evaluation_results.txt"
with open(results_file, "w") as f:
    f.write("SAE EVALUATION RESULTS FOR TINYLLAMA-1.1B\n")
    f.write("="*60 + "\n\n")

    f.write("### Model Behavior Preservation ###\n")
    f.write(f"Cross-Entropy Loss Without SAE: {ce_loss_without_sae_avg:.4f}\n")
    f.write(f"Cross-Entropy Loss With SAE: {ce_loss_with_sae_avg:.4f}\n")
    f.write(f"Cross-Entropy Loss With Ablation: {ce_loss_with_ablation_avg:.4f}\n")
    f.write(f"CE Loss Score: {ce_loss_score:.4f}\n\n")

    f.write(f"KL Divergence With SAE: {kl_div_with_sae_avg:.6f}\n")
    f.write(f"KL Divergence With Ablation: {kl_div_with_ablation_avg:.6f}\n")
    f.write(f"KL Divergence Score: {kl_div_score:.6f}\n\n")

    f.write("### Reconstruction Quality ###\n")
    f.write(f"Reconstruction MSE: {avg_mse:.6f}\n")
    f.write(f"Reconstruction Explained Variance: {avg_explained_variance:.6f}\n")
    f.write(f"Reconstruction Cosine Similarity: {avg_cossim:.6f}\n\n")

    f.write("### Shrinkage Analysis ###\n")
    f.write(f"L2 Norm Input: {avg_l2_norm_in:.6f}\n")
    f.write(f"L2 Norm Output: {avg_l2_norm_out:.6f}\n")
    f.write(f"L2 Norm Ratio: {avg_l2_ratio:.6f}\n")
    f.write(f"Relative Reconstruction Bias: {avg_rel_reconstruction_bias:.6f}\n\n")

    f.write("### Sparsity Metrics ###\n")
    f.write(f"Sparsity L0 (fraction non-zero): {avg_l0:.6f}\n")
    f.write(f"Sparsity L1 (avg L1 norm): {avg_l1:.6f}\n\n")

    f.write("### Token Statistics ###\n")
    f.write(f"Total tokens evaluated: {total_tokens_eval_reconstruction}\n")
    f.write(f"Total samples processed: {total_samples}\n")

print(f"\nResults saved to {results_file}")