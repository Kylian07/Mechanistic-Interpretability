# -*- coding: utf-8 -*-
"""notebook60bedb9de6

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/rajdeeppal2/notebook60bedb9de6.1936dbfc-a21a-4516-ba38-0a33c7ecfcd5.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251107/auto/storage/goog4_request%26X-Goog-Date%3D20251107T071141Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7fc650e4bc28ac70375f0397e3145c50cc7bed71c2fe10e628c71f277c7b629384fbd644f3b9d284fa941298c4fb37010561d618c2b6e655d350dd6f2d1baa151d7f597c971102d92a61b7f68d9552836702aadcb3153753cab7dda7b331861fb66b51d9e8fe61508ed931fe604c719d720addb558e444ea5a5ba951b1bdfbf4837524cc8419a3bc10ff54c6cce31d8bd9cc985f53a5a8f9e5602fcdbb4974b52f07d370af86c37270a564bdc4762195cbb02a0ad52573371054850f63bd49bb212c49c4df39d2dfe3cfa10e5a17a54df50362ce244020410f9087c1d1dd25d092d0dee77421337fad5902825ecb25348331abd9c20e3587a4c66e2787dfa235
"""

import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load tokenizer and TinyLlama-1.1B-Chat model
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Set pad token if not defined
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

tinyllama_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency
    device_map="auto"
)
tinyllama_model.eval()

# Load SST-2 dataset
original_dataset = load_dataset("glue", "sst2")
val_dataset_original = original_dataset["validation"]

all_predictions = []
all_labels = []

print("Evaluating TinyLlama-1.1B-Chat on SST-2 Classification Task...")
print("="*60)

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset_original)), desc="Generating Predictions"):
        sentence = val_dataset_original[idx]["sentence"]
        true_label = val_dataset_original[idx]["label"]

        # Use chat template format for instruction-tuned model
        messages = [
            {
                "role": "system",
                "content": "You are a sentiment analysis assistant. Respond with only one word: 'positive' or 'negative'."
            },
            {
                "role": "user",
                "content": f"Classify the sentiment of this sentence: {sentence}"
            }
        ]

        # Apply chat template
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Tokenize the prompt
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)

        # Generate response
        output_ids = tinyllama_model.generate(
            **inputs,
            max_new_tokens=5,      # Only need a few tokens
            do_sample=False,        # Deterministic generation
            temperature=1.0,        # Required when do_sample=False
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

        # Decode only the generated part (exclude the prompt)
        generated_text = tokenizer.decode(
            output_ids[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        ).strip().lower()

        # Parse the response to extract sentiment
        if "positive" in generated_text:
            prediction = 1
        elif "negative" in generated_text:
            prediction = 0
        else:
            # Fallback: check first word
            first_word = generated_text.split()[0] if generated_text.split() else ""
            if first_word.startswith("pos"):
                prediction = 1
            elif first_word.startswith("neg"):
                prediction = 0
            else:
                # Default to negative if unclear
                prediction = 0

        all_predictions.append(prediction)
        all_labels.append(true_label)

# Convert to numpy arrays
all_predictions = np.array(all_predictions)
all_labels = np.array(all_labels)

# Calculate metrics
accuracy = accuracy_score(all_labels, all_predictions)
precision, recall, f1, _ = precision_recall_fscore_support(
    all_labels, all_predictions, average='binary', zero_division=0
)

cm = confusion_matrix(all_labels, all_predictions)
class_accuracies = cm.diagonal() / cm.sum(axis=1)

# Print results
print("\n" + "="*60)
print("TINYLLAMA-1.1B-CHAT RESULTS ON SST-2 (ZERO-SHOT)")
print("="*60)

print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

print(f"\n### Per-Class Performance ###")
print(f"Negative Class (0) Accuracy: {class_accuracies[0]:.4f} ({class_accuracies[0]*100:.2f}%)")
print(f"Positive Class (1) Accuracy: {class_accuracies[1]:.4f} ({class_accuracies[1]*100:.2f}%)")

print(f"\n### Confusion Matrix ###")
print(f"                Predicted")
print(f"              Neg    Pos")
print(f"Actual Neg  [{cm[0,0]:5d}  {cm[0,1]:5d}]")
print(f"       Pos  [{cm[1,0]:5d}  {cm[1,1]:5d}]")

print(f"\n### Dataset Statistics ###")
print(f"Total Samples: {len(all_labels)}")
print(f"Positive Samples: {np.sum(all_labels == 1)} ({np.sum(all_labels == 1)/len(all_labels)*100:.2f}%)")
print(f"Negative Samples: {np.sum(all_labels == 0)} ({np.sum(all_labels == 0)/len(all_labels)*100:.2f}%)")

print("="*60)

