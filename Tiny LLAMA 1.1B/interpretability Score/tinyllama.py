# -*- coding: utf-8 -*-
"""TinyLlama

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/tinyllama.7f1bdcea-33eb-402f-9ac0-ebbce3c37fe1.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251113/auto/storage/goog4_request%26X-Goog-Date%3D20251113T092753Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D67846e7bc6698344da24e14549948f2f0985cff22833174d7386bbc31b1581d61e659d01148e8302d2514c3d285f63a77a7c3b5ec03f18d7c8ddbd2efcb5a708772454e4523e20823be013ebb2fd77c76efa5c07915684bf1f3403a51d9ac2b50438f61825127bf0069150e0fe29978719ef763e491b433f2fd734182643c87bffbfc4546a55eeaf495e8211ab4d9e945e9801d66b1e80687e0596b81515f7dd2633ef9061f0a719f2eab848e39476662bc0126014c0ddfabbbcca94eeb02c5b795c18cfbab3787dff8075c861938951b59e329e33d4ea1b81dc737c999fc10d7cc3eddadf13719148237fb783586c3324c5398fb943a694329363b00db6f817
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

kylian007_tinylama_pytorch_default_1_path = kagglehub.model_download('kylian007/tinylama/PyTorch/default/1')

print('Data source import complete.')

!pip install sae-lens transformers datasets wandb
!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn
!pip install groq

# -*- coding: utf-8 -*-
"""Create Tokenized Validation Dataset for TinyLlama"""

from datasets import load_dataset
from transformers import AutoTokenizer
import os

print("Creating tokenized validation dataset for TinyLlama...")

# Load TinyLlama tokenizer
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load SST-2 validation split
print("Loading SST-2 validation split...")
validation_dataset = load_dataset("glue", "sst2", split="validation")
print(f"Loaded {len(validation_dataset)} validation samples")

# Tokenize function
def tokenize(batch):
    return tokenizer(batch["sentence"],
                     padding='max_length',
                     truncation=True,
                     max_length=128)

# Tokenize validation split
print("Tokenizing validation split...")
tokenized_val = validation_dataset.map(tokenize, batched=True, desc="Tokenizing")
tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])

# Save tokenized validation dataset
save_dir = "/kaggle/working/tokenized_sst2_tinyllama"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

print(f"Saving tokenized validation split to {save_dir}/validation...")
tokenized_val.save_to_disk(f"{save_dir}/validation")

print(f"âœ“ Tokenized validation dataset saved successfully!")
print(f"  Location: {save_dir}/validation")
print(f"  Samples: {len(tokenized_val)}")

# -*- coding: utf-8 -*-
"""TinyLlama SAE Interpretability Score - COMPLETE CORRECTED VERSION.ipynb"""

import os
import torch
import numpy as np
from torch.utils.data import DataLoader
from datasets import load_from_disk, load_dataset
from scipy.stats import pearsonr
import random
from kaggle_secrets import UserSecretsClient
from groq import Groq
from tqdm import tqdm
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformer_lens import HookedTransformer, HookedTransformerConfig
from sae_lens import StandardTrainingSAE, StandardTrainingSAEConfig

# === Initialize Groq Llama 3 client ===
user_secrets = UserSecretsClient()
groq_api_key = user_secrets.get_secret("GROQ_API_KEY")
os.environ["GROQ_API_KEY"] = groq_api_key
client = Groq()

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    response = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

# --- SAE wrapper class for TinyLlama ---
class SAEWithLastLayerActivations(torch.nn.Module):
    def __init__(self, sae_model):
        super().__init__()
        self.sae = sae_model
        self.activations = None

        # Hook into SAE activations
        hook_module = getattr(self.sae, "hook_sae_acts_post", None)
        if hook_module is None:
            raise ValueError("hook_sae_acts_post not found in SAE model")
        hook_module.register_forward_hook(self._hook_fn)

    def _hook_fn(self, module, input, output):
        self.activations = output.detach()

    def forward(self, sae_input):
        _ = self.sae(sae_input)
        return self.activations

    def get_activations(self, sae_input):
        self.eval()
        with torch.no_grad():
            _ = self.forward(sae_input)
            return self.activations.cpu()

# --- Setup device and tokenizer ---
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load TinyLlama tokenizer
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Load TinyLlama using HookedTransformer
print("Loading TinyLlama model...")
hf_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32
)

# Create custom TransformerLens config for TinyLlama
cfg = HookedTransformerConfig(
    n_layers=22,
    d_model=2048,
    d_head=64,
    n_heads=32,
    d_mlp=5632,
    d_vocab=32000,
    n_ctx=2048,
    act_fn="silu",
    normalization_type="RMS",
    positional_embedding_type="rotary",
    rotary_dim=64,
    final_rms=True,
    gated_mlp=True,
    tokenizer_name=model_name,
)

# Convert to HookedTransformer
tinyllama_model = HookedTransformer(cfg)
tinyllama_model.load_and_process_state_dict(
    hf_model.state_dict(),
    fold_ln=False,
    center_writing_weights=False,
    center_unembed=False,
    refactor_factored_attn_matrices=False,
)

# Set the tokenizer explicitly
tinyllama_model.tokenizer = tokenizer
tinyllama_model.cfg.tokenizer_prepends_bos = False

tinyllama_model = tinyllama_model.to(device)
tinyllama_model.eval()
print("TinyLlama model loaded successfully!")

# --- Function to extract activations with sentences ---
def extract_activations_and_texts(model_wrapper, tinyllama_model, tokenized_dataset, sentences, feature_idx, device, max_batches=None):
    """
    Extract per-token SAE feature activations for TinyLlama
    """
    model_wrapper.eval()
    all_activations = []
    all_texts = []
    all_tokens = []

    dataloader = DataLoader(tokenized_dataset, batch_size=8)

    for i, batch in enumerate(tqdm(dataloader, desc="Extracting activations")):
        if max_batches is not None and i >= max_batches:
            break

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        with torch.no_grad():
            # Run TinyLlama and get hidden states from layer 21
            _, cache = tinyllama_model.run_with_cache(
                input_ids,
                names_filter=["blocks.21.hook_mlp_out"]
            )
            hidden_states = cache["blocks.21.hook_mlp_out"]

            # Get SAE activations
            activations = model_wrapper.get_activations(hidden_states)

        batch_acts = activations[:, :, feature_idx].cpu().numpy()
        batch_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids.cpu().tolist()]
        batch_attn = attention_mask.cpu().numpy()

        start_idx = i * dataloader.batch_size
        batch_texts = sentences[start_idx : start_idx + len(batch["input_ids"])]

        for tokens, acts, attn_mask, text in zip(batch_tokens, batch_acts, batch_attn, batch_texts):
            filtered_tokens = []
            filtered_acts = []
            for t, a, m in zip(tokens, acts, attn_mask):
                if m == 1:
                    filtered_tokens.append(t)
                    filtered_acts.append(a)
            all_tokens.append(filtered_tokens)
            all_activations.append(np.array(filtered_acts))
            all_texts.append(text)

    return all_activations, all_texts, all_tokens

# --- Select top sentences ---
def select_top_sentences(acts, texts, num_top=20):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:num_top]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

# --- Generate interpretation ---
def get_llama3_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a specific feature from a neural network that activates on certain input sentences. \n\n"
        "Given the following sentences along with the per-token activation strengths of this feature "
        "(higher numbers indicate stronger activation), provide a concise and clear single-sentence explanation "
        "describing what causes this feature to activate strongly. Focus on semantic or syntactic patterns.\n\n"
    )
    for i, (text, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{text}\"\nActivations: {acts.tolist()}\n\n"
    prompt += "Provide your interpretation below:"

    print("Sending interpretation prompt to Llama 3...")
    interpretation = chat_with_llama(prompt)
    print("Llama 3 interpretation:", interpretation)
    return interpretation

# --- Predict activation ---
def get_llama3_activation(sentence, interpretation):
    prompt = (
        f'You are given an interpretation of a neural network feature as:\n\n"{interpretation}"\n\n'
        'Given the above interpretation, analyze the following sentence and estimate how strongly this '
        'feature activates over the entire sentence on a scale from 0 (not active) to 10 (very active).\n\n'
        f'Sentence: "{sentence}"\n\nPlease provide only a single numeric score between 0 and 10 as your activation estimate.\nActivation:'
    )
    response = chat_with_llama(prompt)
    match = re.search(r"\d+(\.\d+)?", response)
    if match:
        return float(match.group())
    else:
        print(f"Warning: Could not parse activation, got response: {response}")
        return 0.0

# --- Improved Compute correlation with NaN handling ---
def compute_correlation(actual, predicted):
    """
    Compute Pearson correlation with proper NaN handling and diagnostics
    """
    actual_scalar = np.array([np.mean(a) for a in actual]) * 10.0
    predicted_scalar = np.array(predicted)

    # Check for insufficient variance
    if len(actual_scalar) < 2:
        print("Warning: Less than 2 samples for correlation")
        return np.nan

    actual_std = np.std(actual_scalar)
    predicted_std = np.std(predicted_scalar)

    print(f"Actual activations - Mean: {np.mean(actual_scalar):.4f}, Std: {actual_std:.4f}")
    print(f"Predicted activations - Mean: {np.mean(predicted_scalar):.4f}, Std: {predicted_std:.4f}")

    # Check if either array has zero variance
    if actual_std < 1e-10:
        print("Warning: Actual activations have zero/near-zero variance")
        return np.nan

    if predicted_std < 1e-10:
        print("Warning: Predicted activations have zero/near-zero variance")
        return np.nan

    try:
        corr, p_value = pearsonr(actual_scalar, predicted_scalar)
        print(f"Correlation: {corr:.4f}, p-value: {p_value:.4f}")

        # Note: With n=5, statistical power is low
        if p_value > 0.05:
            print(f"Note: Correlation not statistically significant (p={p_value:.4f}) - expected with small sample")

        return corr
    except Exception as e:
        print(f"Error computing correlation: {e}")
        return np.nan

# --- Save interpretability report ---
def save_interpretability_report(
    filename,
    feature_name,
    feature_type,
    interpretation_text,
    eval_sentences,
    eval_actual_acts,
    eval_predicted_acts,
    interpretability_score
):
    with open(filename, "a", encoding="utf-8") as f:
        f.write(f"Feature: {feature_name} ({feature_type} sentiment)\n\n")

        f.write("Llama 3 Interpretation:\n")
        f.write(interpretation_text.strip() + "\n\n")

        f.write("Evaluation sentences used for prediction:\n")
        for i, (sent, actual, pred) in enumerate(zip(eval_sentences, eval_actual_acts, eval_predicted_acts), 1):
            actual_score = np.mean(actual) * 10 if isinstance(actual, np.ndarray) else actual * 10
            f.write(f"{i}. {sent}\n   Actual Activation: {actual_score:.4f}\n   Predicted Activation: {pred:.4f}\n")
        f.write("\n")

        f.write(f"Overall Interpretability Pearson Correlation Score: {interpretability_score:.4f}\n")
        f.write("\n" + "="*70 + "\n\n")

    print(f"Feature {feature_name} report appended to {filename}")

# --- Full interpretability pipeline ---
def interpretability_score_pipeline(sparse_autoencoder, tinyllama_model, validation_tokenized, validation_sentences, feature_index, feature_type, device, sae_with_acts, report_filename):
    print("Extracting per-token activations and sentences...")
    all_acts, all_texts, all_tokens = extract_activations_and_texts(
        sae_with_acts, tinyllama_model, validation_tokenized, validation_sentences, feature_index, device)

    # Check feature activation statistics
    all_scores = [np.sum(a) for a in all_acts]
    print(f"\nFeature {feature_index} activation stats:")
    print(f"  Total sentences: {len(all_scores)}")
    print(f"  Non-zero activations: {sum(1 for s in all_scores if s > 0)}")
    print(f"  Mean activation: {np.mean(all_scores):.6f}")
    print(f"  Std activation: {np.std(all_scores):.6f}")
    print(f"  Max activation: {np.max(all_scores):.6f}")

    # Skip if feature barely activates
    if np.max(all_scores) < 1e-6:
        print(f"WARNING: Feature {feature_index} has near-zero activation. Skipping.")
        return np.nan

    print("Selecting top 20 sentences for interpretation prompt...")
    top_acts, top_texts = select_top_sentences(all_acts, all_texts, 20)

    interpretation_acts = top_acts[:5]
    interpretation_texts = top_texts[:5]

    print("\nTop 5 sentences used for interpretation prompt:")
    for i, sent in enumerate(interpretation_texts, 1):
        print(f"{i}. {sent}")

    print("Getting interpretation from Llama 3...")
    interpretation = get_llama3_interpretation(interpretation_texts, interpretation_acts)
    print("\n--- Feature Interpretation ---")
    print(interpretation)
    print("--- End of Interpretation ---\n")

    interpretation_set = set(interpretation_texts)
    rest_acts_texts = [(act, txt) for act, txt in zip(all_acts, all_texts) if txt not in interpretation_set]

    rest_acts, rest_texts = zip(*rest_acts_texts) if rest_acts_texts else ([], [])
    rest_acts = list(rest_acts)
    rest_texts = list(rest_texts)

    print("Selecting top 5 activating sentences (excluding interpretation ones) for evaluation...")
    scores = [np.sum(a) for a in rest_acts]

    # Check if we have enough variation
    if len(set(scores)) < 3:
        print(f"Warning: Low variance in activation scores (only {len(set(scores))} unique values)")

    idxs = np.argsort(scores)[::-1][:5]
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    if len(eval_texts) < 5:
        print(f"WARNING: Only {len(eval_texts)} evaluation sentences available. Results may be unreliable.")
        if len(eval_texts) < 2:
            return np.nan

    print(f"\nTop {len(eval_texts)} evaluation sentences (held-out from interpretation):")
    for i, sent in enumerate(eval_texts, 1):
        print(f"{i}. {sent}")

    print("Getting predicted activations from Llama 3 for evaluation sentences...")
    predicted_acts = [get_llama3_activation(sent, interpretation) for sent in eval_texts]

    print("\nEvaluation Results:")
    for i, (sent, actual, pred) in enumerate(zip(eval_texts, eval_acts, predicted_acts), 1):
        actual_score = np.mean(actual) * 10
        print(f"{i}. Actual: {actual_score:.2f}, Predicted: {pred:.2f}")

    print("\nCalculating interpretability score on evaluation sentences...")
    score = compute_correlation(eval_acts, predicted_acts)
    print(f"\nInterpretability Pearson Correlation Score: {score:.4f}")

    save_interpretability_report(
        filename=report_filename,
        feature_name=f"Feature {feature_index}",
        feature_type=feature_type,
        interpretation_text=interpretation,
        eval_sentences=eval_texts,
        eval_actual_acts=eval_acts,
        eval_predicted_acts=predicted_acts,
        interpretability_score=score
    )

    return score

# === Main execution ===
if __name__ == "__main__":
    # Step 1: Find uncommon features
    print("="*70)
    print("STEP 1: Finding uncommon features between positive and negative sentences")
    print("="*70)

    # Load your trained SAE
    print("Loading trained TinyLlama SAE...")
    sae_cfg = StandardTrainingSAEConfig(
        d_in=2048,  # TinyLlama hidden_size
        d_sae=4096,  # Match your training config (8x expansion)
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=50,
    )
    sae = StandardTrainingSAE(sae_cfg)

    # Load your trained SAE weights
    local_sae_path = "/kaggle/input/tinylama/pytorch/default/1"
    weight_file = "model_state_dict (1) (1).pt"
    sae.load_state_dict(torch.load(os.path.join(local_sae_path, weight_file), map_location=device))
    sae.to(device)
    sae.eval()
    print("SAE loaded successfully!")

    # Hook point for TinyLlama last layer (layer 21)
    layer_name = "blocks.21.hook_mlp_out"
    max_len = 128  # Match your training context size

    # Load SST-2 train split
    dataset = load_dataset("glue", "sst2", split="train")

    # Positive sentences
    print("\nAnalyzing positive sentiment sentences...")
    positive_sentences = [x["sentence"] for x in dataset if x["label"] == 1]
    sample_sentences = random.sample(positive_sentences, 50)

    activated_features_per_sentence = []
    for sent in tqdm(sample_sentences, desc="Processing positive sentences"):
        tokens = tinyllama_model.to_tokens([sent], prepend_bos=False)[:, :max_len].to(device)
        _, cache = tinyllama_model.run_with_cache(tokens, names_filter=[layer_name])
        sae_in = cache[layer_name]
        feature_acts = sae.encode(sae_in).squeeze()
        nonzero_features = (feature_acts > 0).any(dim=0).nonzero(as_tuple=True)[0].cpu().tolist()
        activated_features_per_sentence.append(set(nonzero_features))

    common_features = set.intersection(*activated_features_per_sentence)
    print(f"Common activated features across all 50 positive sentences: {sorted(common_features)}")
    print(f"Number of activated features: {len(common_features)}")

    # Negative sentences
    print("\nAnalyzing negative sentiment sentences...")
    negative_sentences = [x["sentence"] for x in dataset if x["label"] == 0]
    sample_negative_sentences = random.sample(negative_sentences, 50)

    activated_features_per_sentence_neg = []
    for sent in tqdm(sample_negative_sentences, desc="Processing negative sentences"):
        tokens = tinyllama_model.to_tokens([sent], prepend_bos=False)[:, :max_len].to(device)
        _, cache = tinyllama_model.run_with_cache(tokens, names_filter=[layer_name])
        sae_in = cache[layer_name]
        feature_acts = sae.encode(sae_in).squeeze()
        nonzero_features = (feature_acts > 0).any(dim=0).nonzero(as_tuple=True)[0].cpu().tolist()
        activated_features_per_sentence_neg.append(set(nonzero_features))

    common_features_neg = set.intersection(*activated_features_per_sentence_neg)
    print(f"Common activated features across all 50 negative sentences: {sorted(common_features_neg)}")
    print(f"Number of activated features: {len(common_features_neg)}")

    # Uncommon features
    positive_only_features = list(common_features - common_features_neg)
    print(f"\nFeatures unique to positive sentences (not in negative): {sorted(positive_only_features)}")
    print(f"Number of positive-only features: {len(positive_only_features)}")

    negative_only_features = list(common_features_neg - common_features)
    print(f"\nFeatures unique to negative sentences (not in positive): {sorted(negative_only_features)}")
    print(f"Number of negative-only features: {len(negative_only_features)}")

    common_elements_both = list(set(common_features) & set(common_features_neg))
    print(f"\nCommon activated features across both positive and negative: {sorted(common_elements_both)}")
    print(f"Number of common features: {len(common_elements_both)}")

    # Step 2: Run interpretability analysis
    print("\n" + "="*70)
    print("STEP 2: Running interpretability analysis on uncommon features")
    print("="*70)

    # Load validation data
    validation_dataset_path = "/kaggle/working/tokenized_sst2_tinyllama/validation"
    validation_tokenized = load_from_disk(validation_dataset_path)
    validation_original = load_dataset("glue", "sst2", split="validation")

    sae_with_acts = SAEWithLastLayerActivations(sae)
    sae_with_acts.to(device)
    sae_with_acts.eval()

    # Combine uncommon features
    uncommon_features = positive_only_features + negative_only_features
    feature_types = ['positive'] * len(positive_only_features) + ['negative'] * len(negative_only_features)

    print(f"\nRunning interpretability analysis on {len(uncommon_features)} uncommon features...")

    # Clear/create the report file
    report_filename = "tinyllama_features_interpretability_report.txt"
    with open(report_filename, "w", encoding="utf-8") as f:
        f.write("INTERPRETABILITY ANALYSIS REPORT FOR UNCOMMON FEATURES - TINYLLAMA-1.1B\n")
        f.write("="*70 + "\n\n")

    interpretability_results = []

    for feature_idx, feature_type in zip(uncommon_features, feature_types):
        print(f"\n{'='*60}")
        print(f"Analyzing Feature {feature_idx} ({feature_type} sentiment)")
        print(f"{'='*60}")

        try:
            score = interpretability_score_pipeline(
                sparse_autoencoder=sae,
                tinyllama_model=tinyllama_model,
                validation_tokenized=validation_tokenized,
                validation_sentences=validation_original["sentence"],
                feature_index=feature_idx,
                feature_type=feature_type,
                device=device,
                sae_with_acts=sae_with_acts,
                report_filename=report_filename
            )

            # Only add valid scores
            if not np.isnan(score):
                interpretability_results.append({
                    'feature': feature_idx,
                    'type': feature_type,
                    'score': score
                })
            else:
                print(f"Feature {feature_idx} returned NaN score - excluding from summary")

        except Exception as e:
            print(f"Error analyzing feature {feature_idx}: {e}")
            import traceback
            traceback.print_exc()
            continue

    # Save summary at the end
    with open(report_filename, "a", encoding="utf-8") as f:
        f.write("\n\n" + "="*70 + "\n")
        f.write("SUMMARY OF INTERPRETABILITY SCORES\n")
        f.write("="*70 + "\n\n")

        f.write("POSITIVE-ONLY FEATURES:\n")
        f.write("-"*70 + "\n")
        pos_scores = [r for r in interpretability_results if r['type'] == 'positive']
        for r in pos_scores:
            f.write(f"Feature {r['feature']}: Score = {r['score']:.4f}\n")
        if pos_scores:
            scores_array = [r['score'] for r in pos_scores]
            avg_pos = np.mean(scores_array)
            median_pos = np.median(scores_array)
            f.write(f"\nTotal Positive Features Analyzed: {len(pos_scores)}\n")
            f.write(f"Average Positive Feature Score: {avg_pos:.4f}\n")
            f.write(f"Median Positive Feature Score: {median_pos:.4f}\n")
            f.write(f"Features with positive correlation (>0): {sum(1 for s in scores_array if s > 0)}/{len(pos_scores)}\n")
            f.write(f"Features with strong correlation (>0.5): {sum(1 for s in scores_array if s > 0.5)}/{len(pos_scores)}\n")
        else:
            f.write("No valid positive features analyzed.\n")
        f.write("\n")

        f.write("NEGATIVE-ONLY FEATURES:\n")
        f.write("-"*70 + "\n")
        neg_scores = [r for r in interpretability_results if r['type'] == 'negative']
        for r in neg_scores:
            f.write(f"Feature {r['feature']}: Score = {r['score']:.4f}\n")
        if neg_scores:
            scores_array = [r['score'] for r in neg_scores]
            avg_neg = np.mean(scores_array)
            median_neg = np.median(scores_array)
            f.write(f"\nTotal Negative Features Analyzed: {len(neg_scores)}\n")
            f.write(f"Average Negative Feature Score: {avg_neg:.4f}\n")
            f.write(f"Median Negative Feature Score: {median_neg:.4f}\n")
            f.write(f"Features with positive correlation (>0): {sum(1 for s in scores_array if s > 0)}/{len(neg_scores)}\n")
            f.write(f"Features with strong correlation (>0.5): {sum(1 for s in scores_array if s > 0.5)}/{len(neg_scores)}\n")
        else:
            f.write("No valid negative features analyzed.\n")

        # Overall statistics
        f.write("\n")
        f.write("OVERALL STATISTICS:\n")
        f.write("-"*70 + "\n")
        all_valid_scores = [r['score'] for r in interpretability_results]
        if all_valid_scores:
            f.write(f"Total Valid Features: {len(all_valid_scores)}\n")
            f.write(f"Overall Average Score: {np.mean(all_valid_scores):.4f}\n")
            f.write(f"Overall Median Score: {np.median(all_valid_scores):.4f}\n")
            f.write(f"Overall Features with positive correlation: {sum(1 for s in all_valid_scores if s > 0)}/{len(all_valid_scores)}\n")

    print(f"\nAll reports saved to {report_filename}")

    print("\n" + "="*70)
    print("Analysis complete!")
    print(f"Total features analyzed: {len(interpretability_results)}")
    if pos_scores:
        print(f"Positive features: {len(pos_scores)}, Avg score: {avg_pos:.4f}, Median: {median_pos:.4f}")
    if neg_scores:
        print(f"Negative features: {len(neg_scores)}, Avg score: {avg_neg:.4f}, Median: {median_neg:.4f}")
    print("="*70)

