# -*- coding: utf-8 -*-
"""TinyLlama_interpretability_score.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jRhHy41g9-XYB8K3RgPhnFK6CmZEDYR7
"""

# -*- coding: utf-8 -*-
"""SAE Interpretability Analysis - TinyLlama (Validation Split - FIXED)"""

import os
import torch
import numpy as np
from torch.utils.data import DataLoader
from datasets import load_from_disk, load_dataset
from scipy.stats import pearsonr
import random
from kaggle_secrets import UserSecretsClient
from groq import Groq
from tqdm import tqdm
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from sae_lens import StandardTrainingSAE, StandardTrainingSAEConfig
from transformer_lens import HookedTransformer
from transformer_lens.HookedTransformerConfig import HookedTransformerConfig
import gc

# === CRITICAL: Clear GPU memory ===
print("Clearing GPU memory...")
torch.cuda.empty_cache()
gc.collect()
torch.cuda.empty_cache()
print("GPU memory cleared!")

# === STEP 0: Tokenize and save validation split ===
print("="*70)
print("STEP 0: Tokenizing and saving validation split")
print("="*70)

# Load TinyLlama tokenizer
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load SST-2 validation split
print("Loading SST-2 validation split...")
validation_dataset = load_dataset("glue", "sst2", split="validation")

# Tokenize validation split
def tokenize(batch):
    return tokenizer(batch["sentence"],
                     padding='max_length',
                     truncation=True,
                     max_length=128)

print("Tokenizing validation split...")
tokenized_val = validation_dataset.map(tokenize, batched=True)
tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])

# Save tokenized validation dataset
save_dir = "/kaggle/working/tokenized_sst2_tinyllama"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

tokenized_val.save_to_disk(f"{save_dir}/validation")
print(f"Tokenized validation split saved to {save_dir}/validation")

# === Initialize Groq Llama 3 client ===
user_secrets = UserSecretsClient()
groq_api_key = user_secrets.get_secret("GROQ_API_KEY")
os.environ["GROQ_API_KEY"] = groq_api_key
client = Groq()

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    response = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

# --- Setup device ---
device = "cuda" if torch.cuda.is_available() else "cpu"

# --- Function to extract SAE feature activations (FIXED) ---
def extract_activations_and_texts(sae_model, tinyllama_model, tokenized_dataset, sentences, feature_idx, device, max_batches=None):
    """Extract SAE feature activations (not raw hidden states)"""
    sae_model.eval()
    tinyllama_model.eval()
    all_activations = []
    all_texts = []
    all_tokens = []

    dataloader = DataLoader(tokenized_dataset, batch_size=8)
    for i, batch in enumerate(tqdm(dataloader, desc="Extracting activations")):
        if max_batches is not None and i >= max_batches:
            break
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        with torch.no_grad():
            # Get hidden states from TinyLlama
            outputs = tinyllama_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
            hidden_states = outputs.hidden_states[-1]  # Shape: [batch, seq_len, 2048]

            # Pass through SAE to get feature activations
            batch_size, seq_len, d_in = hidden_states.shape
            flat_hidden = hidden_states.view(batch_size * seq_len, d_in)  # Flatten

            # Encode through SAE to get features
            sae_features = sae_model.encode(flat_hidden)  # Shape: [batch*seq_len, d_sae]
            sae_features = sae_features.view(batch_size, seq_len, -1)  # Reshape back

            # Now index the SAE features (not raw hidden states)
            batch_acts = sae_features[:, :, feature_idx].cpu().numpy()

        batch_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids.cpu().tolist()]
        batch_attn = attention_mask.cpu().numpy()

        start_idx = i * dataloader.batch_size
        batch_texts = sentences[start_idx : start_idx + len(batch["input_ids"])]

        for tokens, acts, attn_mask, text in zip(batch_tokens, batch_acts, batch_attn, batch_texts):
            filtered_tokens = []
            filtered_acts = []
            for t, a, m in zip(tokens, acts, attn_mask):
                if m == 1:
                    filtered_tokens.append(t)
                    filtered_acts.append(a)
            all_tokens.append(filtered_tokens)
            all_activations.append(np.array(filtered_acts))
            all_texts.append(text)

    return all_activations, all_texts, all_tokens

# --- Select top sentences ---
def select_top_sentences(acts, texts, num_top=20):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:num_top]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

# --- Generate interpretation ---
def get_llama3_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a specific feature from a neural network that activates on certain input sentences. \n\n"
        "Given the following sentences along with the per-token activation strengths of this feature "
        "(higher numbers indicate stronger activation), provide a concise and clear single-sentence explanation "
        "describing what causes this feature to activate strongly. Focus on semantic or syntactic patterns.\n\n"
    )
    for i, (text, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{text}\"\nActivations: {acts.tolist()}\n\n"
    prompt += "Provide your interpretation below:"

    print("Sending interpretation prompt to Llama 3...")
    interpretation = chat_with_llama(prompt)
    print("Llama 3 interpretation:", interpretation)
    return interpretation

# --- Predict activation ---
def get_llama3_activation(sentence, interpretation):
    prompt = (
        f'You are given an interpretation of a neural network feature as:\n\n"{interpretation}"\n\n'
        'Given the above interpretation, analyze the following sentence and estimate how strongly this '
        'feature activates over the entire sentence on a scale from 0 (not active) to 10 (very active).\n\n'
        f'Sentence: "{sentence}"\n\nPlease provide only a single numeric score between 0 and 10 as your activation estimate.\nActivation:'
    )
    print(f"Predicting activation for sentence:\n{sentence}")
    response = chat_with_llama(prompt)
    match = re.search(r"\d+(\.\d+)?", response)
    if match:
        return float(match.group())
    else:
        print(f"Warning: Could not parse activation, got response: {response}")
        return 0.0

# --- Compute correlation ---
def compute_correlation(actual, predicted):
    actual_scalar = np.array([np.mean(a) for a in actual]) * 10.0
    predicted_scalar = np.array(predicted)
    corr, _ = pearsonr(actual_scalar, predicted_scalar)
    return corr

# --- Save interpretability report ---
def save_interpretability_report(
    filename,
    feature_name,
    feature_type,
    interpretation_text,
    eval_sentences,
    eval_actual_acts,
    eval_predicted_acts,
    interpretability_score
):
    with open(filename, "a", encoding="utf-8") as f:
        f.write(f"Feature: {feature_name} ({feature_type} sentiment)\n\n")

        f.write("Llama 3 Interpretation:\n")
        f.write(interpretation_text.strip() + "\n\n")

        f.write("Evaluation sentences used for prediction:\n")
        for i, (sent, actual, pred) in enumerate(zip(eval_sentences, eval_actual_acts, eval_predicted_acts), 1):
            actual_score = np.mean(actual) * 10 if isinstance(actual, np.ndarray) else actual * 10
            f.write(f"{i}. {sent}\n   Actual Activation: {actual_score:.4f}\n   Predicted Activation: {pred:.4f}\n")
        f.write("\n")

        f.write(f"Overall Interpretability Pearson Correlation Score: {interpretability_score:.4f}\n")
        f.write("\n" + "="*70 + "\n\n")

    print(f"Feature {feature_name} report appended to {filename}")

# --- Full interpretability pipeline ---
def interpretability_score_pipeline(sparse_autoencoder, tinyllama_model, validation_tokenized, validation_sentences, feature_index, feature_type, device, report_filename):
    print("Extracting per-token activations and sentences...")

    # IMPORTANT: Pass both SAE and TinyLlama model
    all_acts, all_texts, all_tokens = extract_activations_and_texts(
        sparse_autoencoder,  # SAE model
        tinyllama_model,     # TinyLlama model
        validation_tokenized,
        validation_sentences,
        feature_index,
        device
    )

    print("Selecting top 20 sentences for interpretation prompt...")
    top_acts, top_texts = select_top_sentences(all_acts, all_texts, 20)

    interpretation_acts = top_acts[:5]
    interpretation_texts = top_texts[:5]

    print("\nTop 5 sentences used for interpretation prompt:")
    for i, sent in enumerate(interpretation_texts, 1):
        print(f"{i}. {sent}")

    print("Getting interpretation from Llama 3...")
    interpretation = get_llama3_interpretation(interpretation_texts, interpretation_acts)
    print("\n--- Feature Interpretation ---")
    print(interpretation)
    print("--- End of Interpretation ---\n")

    interpretation_set = set(interpretation_texts)
    rest_acts_texts = [(act, txt) for act, txt in zip(all_acts, all_texts) if txt not in interpretation_set]

    rest_acts, rest_texts = zip(*rest_acts_texts) if rest_acts_texts else ([], [])
    rest_acts = list(rest_acts)
    rest_texts = list(rest_texts)

    print("Selecting top 5 activating sentences (excluding interpretation ones) for evaluation...")
    scores = [np.sum(a) for a in rest_acts]
    idxs = np.argsort(scores)[::-1][:5]
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    print("\nTop 5 evaluation sentences (held-out from interpretation):")
    for i, sent in enumerate(eval_texts, 1):
        print(f"{i}. {sent}")

    print("Getting predicted activations from Llama 3 for evaluation sentences...")
    predicted_acts = [get_llama3_activation(sent, interpretation) for sent in eval_texts]

    print("Calculating interpretability score on evaluation sentences...")
    score = compute_correlation(eval_acts, predicted_acts)
    print(f"\nInterpretability Pearson Correlation Score on held-out top 5 sentences: {score:.4f}")

    save_interpretability_report(
        filename=report_filename,
        feature_name=f"Feature {feature_index}",
        feature_type=feature_type,
        interpretation_text=interpretation,
        eval_sentences=eval_texts,
        eval_actual_acts=eval_acts,
        eval_predicted_acts=predicted_acts,
        interpretability_score=score
    )

    return score

# === Main execution ===
if __name__ == "__main__":
    # Step 1: Find uncommon features USING VALIDATION SPLIT
    print("\n" + "="*70)
    print("STEP 1: Finding uncommon features on VALIDATION split")
    print("="*70)

    # Load HF model for feature finding
    print("Loading TinyLlama HF model...")
    hf_model = AutoModelForCausalLM.from_pretrained(
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    )

    print("Converting to HookedTransformer...")
    cfg = HookedTransformerConfig(
        n_layers=22,
        d_model=2048,
        d_head=64,
        n_heads=32,
        d_mlp=5632,
        d_vocab=32000,
        n_ctx=2048,
        act_fn="silu",
        normalization_type="RMS",
        positional_embedding_type="rotary",
        rotary_dim=64,
        final_rms=True,
        gated_mlp=True,
    )

    model = HookedTransformer(cfg)
    model.load_and_process_state_dict(
        hf_model.state_dict(),
        fold_ln=False,
        center_writing_weights=False,
        center_unembed=False,
        refactor_factored_attn_matrices=False,
    )
    model = model.to(device)
    model.set_tokenizer(tokenizer)

    # Delete the HF model immediately
    del hf_model
    torch.cuda.empty_cache()
    gc.collect()
    print("HF model deleted, memory cleared!")

    # Load SAE configuration for TinyLlama
    sae_cfg = StandardTrainingSAEConfig(
        d_in=2048,
        d_sae=4096,
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=50,
    )
    sae = StandardTrainingSAE(sae_cfg)

    # Load your trained SAE weights
    local_sae_path = "/kaggle/input/tiny-llama/pytorch/default/1"
    weight_file = "model_state_dict (1).pt"

    try:
        sae.load_state_dict(torch.load(os.path.join(local_sae_path, weight_file), map_location=device))
        print(f"SAE weights loaded successfully!")
    except FileNotFoundError:
        print(f"Error: SAE weights not found at {os.path.join(local_sae_path, weight_file)}")
        print("Available files in directory:")
        if os.path.exists(local_sae_path):
            print(os.listdir(local_sae_path))
        exit()

    sae.to(device)
    sae.eval()

    # Hook point for TinyLlama last layer (layer 21)
    layer_name = "blocks.21.hook_mlp_out"
    max_len = 128

    # Load SST-2 VALIDATION split for feature finding
    print("Loading SST-2 VALIDATION split...")
    dataset = load_dataset("glue", "sst2", split="validation")

    # Positive sentences (from validation)
    positive_sentences = [x["sentence"] for x in dataset if x["label"] == 1]
    sample_sentences = random.sample(positive_sentences, min(50, len(positive_sentences)))

    print(f"Sampled {len(sample_sentences)} positive sentences from validation set")

    activated_features_per_sentence = []
    for sent in sample_sentences:
        tokens = model.to_tokens([sent])[:, :max_len].to(device)
        _, cache = model.run_with_cache(tokens, names_filter=[layer_name])
        sae_in = cache[layer_name]
        feature_acts = sae.encode(sae_in).squeeze()
        nonzero_features = (feature_acts > 0).any(dim=0).nonzero(as_tuple=True)[0].cpu().tolist()
        activated_features_per_sentence.append(set(nonzero_features))

    common_features = set.intersection(*activated_features_per_sentence) if activated_features_per_sentence else set()
    print("Common activated features across positive validation sentences:", sorted(common_features)[:20])
    print("No of activated features:", len(common_features))

    # Negative sentences (from validation)
    negative_sentences = [x["sentence"] for x in dataset if x["label"] == 0]
    sample_negative_sentences = random.sample(negative_sentences, min(50, len(negative_sentences)))

    print(f"Sampled {len(sample_negative_sentences)} negative sentences from validation set")

    activated_features_per_sentence_neg = []
    for sent in sample_negative_sentences:
        tokens = model.to_tokens([sent])[:, :max_len].to(device)
        _, cache = model.run_with_cache(tokens, names_filter=[layer_name])
        sae_in = cache[layer_name]
        feature_acts = sae.encode(sae_in).squeeze()
        nonzero_features = (feature_acts > 0).any(dim=0).nonzero(as_tuple=True)[0].cpu().tolist()
        activated_features_per_sentence_neg.append(set(nonzero_features))

    common_features_neg = set.intersection(*activated_features_per_sentence_neg) if activated_features_per_sentence_neg else set()
    print("Common activated features across negative validation sentences:", sorted(common_features_neg)[:20])
    print("No of activated features:", len(common_features_neg))

    # Uncommon features
    positive_only_features = list(common_features - common_features_neg)
    print("\nFeatures unique to positive validation sentences (not in negative):", sorted(positive_only_features)[:20])
    print("No of positive-only features:", len(positive_only_features))

    negative_only_features = list(common_features_neg - common_features)
    print("\nFeatures unique to negative validation sentences (not in positive):", sorted(negative_only_features)[:20])
    print("No of negative-only features:", len(negative_only_features))

    common_elements_both = list(set(common_features) & set(common_features_neg))
    print("\nCommon activated features across both positive and negative validation:", sorted(common_elements_both)[:20])
    print("No of common features:", len(common_elements_both))

    # Step 2: Run interpretability analysis
    print("\n" + "="*70)
    print("STEP 2: Running interpretability analysis on uncommon features")
    print("="*70)

    # Load validation data for interpretability analysis
    validation_dataset_path = "/kaggle/working/tokenized_sst2_tinyllama/validation"
    validation_tokenized = load_from_disk(validation_dataset_path)
    validation_original = load_dataset("glue", "sst2", split="validation")

    # Load a fresh copy of TinyLlama for the interpretability analysis
    print("Loading fresh TinyLlama model for interpretability analysis...")
    tinyllama_model = AutoModelForCausalLM.from_pretrained(
        "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        torch_dtype=torch.float32,
        device_map=device,
        low_cpu_mem_usage=True
    )
    tinyllama_model.eval()

    # Delete the HookedTransformer model
    del model
    torch.cuda.empty_cache()
    gc.collect()
    print("HookedTransformer model deleted, using AutoModel for interpretability!")

    # Combine uncommon features (limit to top 3 of each for memory constraints)
    uncommon_features = positive_only_features[:3] + negative_only_features[:3]
    feature_types = ['positive'] * min(3, len(positive_only_features)) + ['negative'] * min(3, len(negative_only_features))

    print(f"\nRunning interpretability analysis on {len(uncommon_features)} uncommon features...")

    # Create report file
    report_filename = "/kaggle/working/tinyllama_validation_interpretability_report.txt"
    with open(report_filename, "w", encoding="utf-8") as f:
        f.write("INTERPRETABILITY ANALYSIS REPORT FOR UNCOMMON FEATURES - TINYLLAMA-1.1B\n")
        f.write("Analysis performed on VALIDATION SPLIT\n")
        f.write("="*70 + "\n\n")

    interpretability_results = []

    for feature_idx, feature_type in zip(uncommon_features, feature_types):
        print(f"\n{'='*60}")
        print(f"Analyzing Feature {feature_idx} ({feature_type} sentiment)")
        print(f"{'='*60}")

        try:
            score = interpretability_score_pipeline(
                sparse_autoencoder=sae,
                tinyllama_model=tinyllama_model,
                validation_tokenized=validation_tokenized,
                validation_sentences=validation_original["sentence"],
                feature_index=feature_idx,
                feature_type=feature_type,
                device=device,
                report_filename=report_filename
            )

            interpretability_results.append({
                'feature': feature_idx,
                'type': feature_type,
                'score': score
            })

        except Exception as e:
            print(f"Error analyzing feature {feature_idx}: {e}")
            import traceback
            traceback.print_exc()
            continue

    # Save summary
    with open(report_filename, "a", encoding="utf-8") as f:
        f.write("\n\n" + "="*70 + "\n")
        f.write("SUMMARY OF INTERPRETABILITY SCORES\n")
        f.write("="*70 + "\n\n")

        f.write("POSITIVE-ONLY FEATURES:\n")
        f.write("-"*70 + "\n")
        for r in interpretability_results:
            if r['type'] == 'positive':
                f.write(f"Feature {r['feature']}: Score = {r['score']:.4f}\n")
        f.write("\n")

        f.write("NEGATIVE-ONLY FEATURES:\n")
        f.write("-"*70 + "\n")
        for r in interpretability_results:
            if r['type'] == 'negative':
                f.write(f"Feature {r['feature']}: Score = {r['score']:.4f}\n")

    print(f"\nAll reports saved to {report_filename}")

    print("\n" + "="*70)
    print("Analysis complete!")
    print("="*70)