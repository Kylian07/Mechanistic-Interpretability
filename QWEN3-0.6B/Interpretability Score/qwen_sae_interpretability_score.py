# -*- coding: utf-8 -*-
"""Qwen SAE interpretability score.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ESTL2465KWDbZ38LKyicwTnxC6tOf2s1
"""

!pip install transformers datasets sentence-transformers torch sae-lens
!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn
!pip install groq

from datasets import load_dataset
from transformers import AutoTokenizer
import os
from torch.utils.data import DataLoader
from datasets import load_dataset, load_from_disk
from transformers import AutoModelForCausalLM
import torch

# Model configuration for Qwen3-0.6B
model_name = "Qwen/Qwen3-0.6B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Qwen3 uses <|endoftext|> as pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load SST-2 dataset
dataset = load_dataset("glue", "sst2")

# Tokenize function
def tokenize(batch):
    return tokenizer(batch["sentence"],
                     padding='max_length',
                     truncation=True,
                     max_length=128)

tokenized_train = dataset["train"].map(tokenize, batched=True)
tokenized_val = dataset["validation"].map(tokenize, batched=True)
tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask"])
tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])

# Save tokenized dataset locally
save_dir = "/kaggle/working/tokenized_sst2_qwen3"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

tokenized_train.save_to_disk(f"{save_dir}/train")
tokenized_val.save_to_disk(f"{save_dir}/validation")

print(f"Tokenized SST-2 saved to {save_dir}")

from sae_lens import (
    LanguageModelSAERunnerConfig,
    SAETrainingRunner,
    StandardTrainingSAEConfig,
    LoggingConfig,
)

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load local tokenized SST-2 datasets
train_dataset = load_from_disk("/kaggle/working/tokenized_sst2_qwen3/train")
validation_dataset = load_from_disk("/kaggle/working/tokenized_sst2_qwen3/validation")

print("Train dataset columns:", train_dataset.column_names)
print("Validation dataset columns:", validation_dataset.column_names)

# Define training parameters
total_training_steps = 80000
batch_size = 64
total_training_tokens = total_training_steps * batch_size

lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5
l1_warm_up_steps = total_training_steps // 20

# Qwen3-0.6B architecture: 28 layers, hidden_size=1024
# Using layer 27 (last layer, 0-indexed) for SAE training
cfg = LanguageModelSAERunnerConfig(
    model_name="Qwen/Qwen3-0.6B",
    hook_name="blocks.27.hook_mlp_out",  # Last layer MLP output (28 layers, 0-indexed)
    dataset_path=None,
    is_dataset_tokenized=True,
    streaming=False,
    sae=StandardTrainingSAEConfig(
        d_in=1024,  # Qwen3-0.6B has hidden_size=1024
        d_sae=2048,  # 2x expansion factor (can use 8192 for 8x)
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=l1_warm_up_steps,
    ),
    lr=5e-5,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=lr_warm_up_steps,
    lr_decay_steps=lr_decay_steps,
    train_batch_size_tokens=batch_size,
    context_size=128,
    n_batches_in_buffer=64,
    training_tokens=total_training_tokens,
    store_batch_size_prompts=16,
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    logger=LoggingConfig(
        log_to_wandb=False,
        wandb_project="sae_lens_sst2_qwen3_0.6b_lastlayer",
        wandb_log_frequency=30,
        eval_every_n_wandb_logs=20,
    ),
    device=device,
    seed=42,
    n_checkpoints=1,
    checkpoint_path="checkpoints",
    dtype="float32",
)

# Train SAE using the loaded dataset
runner = SAETrainingRunner(cfg, override_dataset=train_dataset)
sparse_autoencoder = runner.run()

# Save the trained SAE
save_path = "/kaggle/working"

# If the SAE object has a save method
if hasattr(sparse_autoencoder, "save"):
    sparse_autoencoder.save(save_path)
else:
    # Fallback: save model state dict manually
    torch.save(sparse_autoencoder.state_dict(), os.path.join(save_path, "model_state_dict.pt"))

print(f"SAE model saved to {save_path}")

import os
import torch
import numpy as np
from torch.utils.data import DataLoader
from datasets import load_from_disk, load_dataset
from scipy.stats import pearsonr
import random
from kaggle_secrets import UserSecretsClient
from groq import Groq
from tqdm import tqdm
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from sae_lens import HookedSAETransformer, StandardTrainingSAE, StandardTrainingSAEConfig

# === Initialize Groq Llama 3 client ===
user_secrets = UserSecretsClient()
groq_api_key = user_secrets.get_secret("GROQ_API_KEY")
os.environ["GROQ_API_KEY"] = groq_api_key
client = Groq()

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    response = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

# --- SAE wrapper class ---
class SAEWithLastLayerActivations(torch.nn.Module):
    def __init__(self, sae_model):
        super().__init__()
        self.sae = sae_model
        self.activations = None

        hook_module = getattr(self.sae, "hook_sae_acts_post", None)
        if hook_module is None:
            raise ValueError("hook_sae_acts_post not found in SAE model")
        hook_module.register_forward_hook(self._hook_fn)

    def _hook_fn(self, module, input, output):
        self.activations = output.detach()

    def forward(self, sae_input):
        _ = self.sae(sae_input)
        return self.activations

    def get_activations(self, sae_input):
        self.eval()
        with torch.no_grad():
            _ = self.forward(sae_input)
            return self.activations.cpu()

# --- Setup device and tokenizer ---
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load Qwen3-0.6B tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")
# Qwen3 uses <|endoftext|> as pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load Qwen3-0.6B model
qwen_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-0.6B",
    torch_dtype=torch.float32,
    device_map=device
)
qwen_model.eval()

# --- Function to extract activations with sentences ---
def extract_activations_and_texts(model_wrapper, tokenized_dataset, sentences, feature_idx, device, max_batches=None):
    model_wrapper.eval()
    all_activations = []
    all_texts = []
    all_tokens = []

    dataloader = DataLoader(tokenized_dataset, batch_size=8)
    for i, batch in enumerate(tqdm(dataloader, desc="Extracting activations")):
        if max_batches is not None and i >= max_batches:
            break
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        with torch.no_grad():
            outputs = qwen_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
            # Get the last hidden state from Qwen3
            hidden_states = outputs.hidden_states[-1]  # Last layer hidden states

            activations = model_wrapper.get_activations(hidden_states)

        batch_acts = activations[:, :, feature_idx].cpu().numpy()
        batch_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids.cpu().tolist()]
        batch_attn = attention_mask.cpu().numpy()

        start_idx = i * dataloader.batch_size
        batch_texts = sentences[start_idx : start_idx + len(batch["input_ids"])]

        for tokens, acts, attn_mask, text in zip(batch_tokens, batch_acts, batch_attn, batch_texts):
            filtered_tokens = []
            filtered_acts = []
            for t, a, m in zip(tokens, acts, attn_mask):
                if m == 1:
                    filtered_tokens.append(t)
                    filtered_acts.append(a)
            all_tokens.append(filtered_tokens)
            all_activations.append(np.array(filtered_acts))
            all_texts.append(text)

    return all_activations, all_texts, all_tokens

# --- Select top sentences ---
def select_top_sentences(acts, texts, num_top=20):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:num_top]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

# --- Generate interpretation ---
def get_llama3_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a specific feature from a neural network that activates on certain input sentences. \n\n"
        "Given the following sentences along with the per-token activation strengths of this feature "
        "(higher numbers indicate stronger activation), provide a concise and clear single-sentence explanation "
        "describing what causes this feature to activate strongly. Focus on semantic or syntactic patterns.\n\n"
    )
    for i, (text, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{text}\"\nActivations: {acts.tolist()}\n\n"
    prompt += "Provide your interpretation below:"

    print("Sending interpretation prompt to Llama 3...")
    interpretation = chat_with_llama(prompt)
    print("Llama 3 interpretation:", interpretation)
    return interpretation

# --- Select eval sentences ---
def select_eval_sentences(acts, texts, n=5):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1]
    high_idxs = idxs[:n]
    remaining = [i for i in range(len(acts)) if i not in high_idxs]
    nonzero = [i for i in remaining if np.std(acts[i]) > 0]
    if len(nonzero) < n:
        rand_idxs = nonzero
    else:
        rand_idxs = random.sample(nonzero, n)
    eval_idxs = list(high_idxs) + rand_idxs
    return [acts[i] for i in eval_idxs], [texts[i] for i in eval_idxs]

# --- Predict activation ---
def get_llama3_activation(sentence, interpretation):
    prompt = (
        f'You are given an interpretation of a neural network feature as:\n\n"{interpretation}"\n\n'
        'Given the above interpretation, analyze the following sentence and estimate how strongly this '
        'feature activates over the entire sentence on a scale from 0 (not active) to 10 (very active).\n\n'
        f'Sentence: "{sentence}"\n\nPlease provide only a single numeric score between 0 and 10 as your activation estimate.\nActivation:'
    )
    print(f"Predicting activation for sentence:\n{sentence}")
    response = chat_with_llama(prompt)
    match = re.search(r"\d+(\.\d+)?", response)
    if match:
        return float(match.group())
    else:
        print(f"Warning: Could not parse activation, got response: {response}")
        return 0.0

# --- Compute correlation ---
def compute_correlation(actual, predicted):
    actual_scalar = np.array([np.mean(a) for a in actual]) * 10.0
    predicted_scalar = np.array(predicted)
    corr, _ = pearsonr(actual_scalar, predicted_scalar)
    return corr

# --- Save interpretability report (append mode for single file) ---
def save_interpretability_report(
    filename,
    feature_name,
    feature_type,
    interpretation_text,
    eval_sentences,
    eval_actual_acts,
    eval_predicted_acts,
    interpretability_score
):
    with open(filename, "a", encoding="utf-8") as f:
        f.write(f"Feature: {feature_name} ({feature_type} sentiment)\n\n")

        f.write("Llama 3 Interpretation:\n")
        f.write(interpretation_text.strip() + "\n\n")

        f.write("Evaluation sentences used for prediction:\n")
        for i, (sent, actual, pred) in enumerate(zip(eval_sentences, eval_actual_acts, eval_predicted_acts), 1):
            actual_score = np.mean(actual) * 10 if isinstance(actual, np.ndarray) else actual * 10
            f.write(f"{i}. {sent}\n   Actual Activation: {actual_score:.4f}\n   Predicted Activation: {pred:.4f}\n")
        f.write("\n")

        f.write(f"Overall Interpretability Pearson Correlation Score: {interpretability_score:.4f}\n")
        f.write("\n" + "="*70 + "\n\n")

    print(f"Feature {feature_name} report appended to {filename}")

# --- Full interpretability pipeline ---
def interpretability_score_pipeline(sparse_autoencoder, validation_tokenized, validation_sentences, feature_index, feature_type, device, sae_with_acts, report_filename):
    print("Extracting per-token activations and sentences...")
    all_acts, all_texts, all_tokens = extract_activations_and_texts(
        sae_with_acts, validation_tokenized, validation_sentences, feature_index, device)

    print("Selecting top 20 sentences for interpretation prompt...")
    top_acts, top_texts = select_top_sentences(all_acts, all_texts, 20)

    interpretation_acts = top_acts[:5]
    interpretation_texts = top_texts[:5]

    print("\nTop 5 sentences used for interpretation prompt:")
    for i, sent in enumerate(interpretation_texts, 1):
        print(f"{i}. {sent}")

    print("Getting interpretation from Llama 3...")
    interpretation = get_llama3_interpretation(interpretation_texts, interpretation_acts)
    print("\n--- Feature Interpretation ---")
    print(interpretation)
    print("--- End of Interpretation ---\n")

    interpretation_set = set(interpretation_texts)
    rest_acts_texts = [(act, txt) for act, txt in zip(all_acts, all_texts) if txt not in interpretation_set]

    rest_acts, rest_texts = zip(*rest_acts_texts) if rest_acts_texts else ([], [])
    rest_acts = list(rest_acts)
    rest_texts = list(rest_texts)

    print("Selecting top 5 activating sentences (excluding interpretation ones) for evaluation...")
    scores = [np.sum(a) for a in rest_acts]
    idxs = np.argsort(scores)[::-1][:5]
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    print("\nTop 5 evaluation sentences (held-out from interpretation):")
    for i, sent in enumerate(eval_texts, 1):
        print(f"{i}. {sent}")

    print("Getting predicted activations from Llama 3 for evaluation sentences...")
    predicted_acts = [get_llama3_activation(sent, interpretation) for sent in eval_texts]

    print("Calculating interpretability score on evaluation sentences...")
    score = compute_correlation(eval_acts, predicted_acts)
    print(f"\nInterpretability Pearson Correlation Score on held-out top 5 sentences: {score:.4f}")

    save_interpretability_report(
        filename=report_filename,
        feature_name=f"Feature {feature_index}",
        feature_type=feature_type,
        interpretation_text=interpretation,
        eval_sentences=eval_texts,
        eval_actual_acts=eval_acts,
        eval_predicted_acts=predicted_acts,
        interpretability_score=score
    )

    return score

# === Main execution ===
if __name__ == "__main__":
    # Step 1: Find uncommon features
    print("="*70)
    print("STEP 1: Finding uncommon features between positive and negative sentences")
    print("="*70)

    # Load HookedSAETransformer for Qwen3-0.6B
    model = HookedSAETransformer.from_pretrained("Qwen/Qwen3-0.6B", device=device)

    # Load SAE configuration for Qwen3-0.6B
    sae_cfg = StandardTrainingSAEConfig(
        d_in=1024,  # Qwen3-0.6B has hidden_size=1024
        d_sae=2048,  # 2x expansion (update to match your trained SAE)
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=50,
    )
    sae = StandardTrainingSAE(sae_cfg)

    # Load your trained SAE weights
    local_sae_path = "/kaggle/working"
    weight_file = "model_state_dict.pt"
    sae.load_state_dict(torch.load(os.path.join(local_sae_path, weight_file), map_location=device))
    sae.to(device)
    sae.eval()

    # Hook point for Qwen3-0.6B last layer (layer 27)
    layer_name = "blocks.27.hook_mlp_out"
    max_len = 128  # Updated to match Qwen3 context

    # Load SST-2 train split
    dataset = load_dataset("glue", "sst2", split="train")

    # Positive sentences
    positive_sentences = [x["sentence"] for x in dataset if x["label"] == 1]
    sample_sentences = random.sample(positive_sentences, 50)

    activated_features_per_sentence = []
    for sent in sample_sentences:
        tokens = model.to_tokens([sent])[:, :max_len].to(device)
        _, cache = model.run_with_cache(tokens, names_filter=[layer_name])
        sae_in = cache[layer_name]
        feature_acts = sae.encode(sae_in).squeeze()
        nonzero_features = (feature_acts > 0).any(dim=0).nonzero(as_tuple=True)[0].cpu().tolist()
        activated_features_per_sentence.append(set(nonzero_features))

    common_features = set.intersection(*activated_features_per_sentence)
    print("Common activated features across all 50 positive sentences:", sorted(common_features))
    print("No of activated features:", len(common_features))

    # Negative sentences
    negative_sentences = [x["sentence"] for x in dataset if x["label"] == 0]
    sample_negative_sentences = random.sample(negative_sentences, 50)

    activated_features_per_sentence_neg = []
    for sent in sample_negative_sentences:
        tokens = model.to_tokens([sent])[:, :max_len].to(device)
        _, cache = model.run_with_cache(tokens, names_filter=[layer_name])
        sae_in = cache[layer_name]
        feature_acts = sae.encode(sae_in).squeeze()
        nonzero_features = (feature_acts > 0).any(dim=0).nonzero(as_tuple=True)[0].cpu().tolist()
        activated_features_per_sentence_neg.append(set(nonzero_features))

    common_features_neg = set.intersection(*activated_features_per_sentence_neg)
    print("Common activated features across all 50 negative sentences:", sorted(common_features_neg))
    print("No of activated features:", len(common_features_neg))

    # Uncommon features
    positive_only_features = list(common_features - common_features_neg)
    print("\nFeatures unique to positive sentences (not in negative):", sorted(positive_only_features))
    print("No of positive-only features:", len(positive_only_features))

    negative_only_features = list(common_features_neg - common_features)
    print("\nFeatures unique to negative sentences (not in positive):", sorted(negative_only_features))
    print("No of negative-only features:", len(negative_only_features))

    common_elements_both = list(set(common_features) & set(common_features_neg))
    print("\nCommon activated features across both positive and negative:", sorted(common_elements_both))
    print("No of common features:", len(common_elements_both))

    # Step 2: Run interpretability analysis
    print("\n" + "="*70)
    print("STEP 2: Running interpretability analysis on uncommon features")
    print("="*70)

    # Load validation data (tokenized with Qwen3 tokenizer)
    validation_dataset_path = "/kaggle/working/tokenized_sst2_qwen3/validation"
    validation_tokenized = load_from_disk(validation_dataset_path)
    validation_original = load_dataset("glue", "sst2", split="validation")

    sae_with_acts = SAEWithLastLayerActivations(sae)
    sae_with_acts.to(device)
    sae_with_acts.eval()

    # Combine uncommon features
    uncommon_features = positive_only_features + negative_only_features
    feature_types = ['positive'] * len(positive_only_features) + ['negative'] * len(negative_only_features)

    print(f"\nRunning interpretability analysis on {len(uncommon_features)} uncommon features...")

    # Clear/create the report file
    report_filename = "qwen3_features_interpretability_report.txt"
    with open(report_filename, "w", encoding="utf-8") as f:
        f.write("INTERPRETABILITY ANALYSIS REPORT FOR UNCOMMON FEATURES - QWEN3-0.6B\n")
        f.write("="*70 + "\n\n")

    interpretability_results = []

    for feature_idx, feature_type in zip(uncommon_features, feature_types):
        print(f"\n{'='*60}")
        print(f"Analyzing Feature {feature_idx} ({feature_type} sentiment)")
        print(f"{'='*60}")

        try:
            score = interpretability_score_pipeline(
                sparse_autoencoder=sae,
                validation_tokenized=validation_tokenized,
                validation_sentences=validation_original["sentence"],
                feature_index=feature_idx,
                feature_type=feature_type,
                device=device,
                sae_with_acts=sae_with_acts,
                report_filename=report_filename
            )

            interpretability_results.append({
                'feature': feature_idx,
                'type': feature_type,
                'score': score
            })

        except Exception as e:
            print(f"Error analyzing feature {feature_idx}: {e}")
            continue

    # Save summary at the end of the same file
    with open(report_filename, "a", encoding="utf-8") as f:
        f.write("\n\n" + "="*70 + "\n")
        f.write("SUMMARY OF INTERPRETABILITY SCORES\n")
        f.write("="*70 + "\n\n")

        f.write("POSITIVE-ONLY FEATURES:\n")
        f.write("-"*70 + "\n")
        for r in interpretability_results:
            if r['type'] == 'positive':
                f.write(f"Feature {r['feature']}: Score = {r['score']:.4f}\n")
        f.write("\n")

        f.write("NEGATIVE-ONLY FEATURES:\n")
        f.write("-"*70 + "\n")
        for r in interpretability_results:
            if r['type'] == 'negative':
                f.write(f"Feature {r['feature']}: Score = {r['score']:.4f}\n")

    print(f"\nAll reports saved to {report_filename}")

    print("\n" + "="*70)
    print("Analysis complete!")
    print("="*70)