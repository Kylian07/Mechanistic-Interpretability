# -*- coding: utf-8 -*-
"""Qwen3-0.6B each Layer Frequent Feature.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUBv2T_mZvISPj-D_5ooeYgaIGPXyEvN
"""

!pip install transformers datasets sentence-transformers torch sae-lens

!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel
from collections import defaultdict
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "qwen/Qwen3-0.6B"

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)
model.eval()

# Dictionary to store activations per layer
activation_storage = defaultdict(list)

# Hook function to store forward outputs
def hook_store(module, input, output, name):
    activation_storage[name].append(output.detach().cpu())

# Register hooks on MLP module outputs of each transformer layer
handles = []
for i, layer in enumerate(model.layers):
    handle = layer.mlp.register_forward_hook(
        lambda m, i, o, name=f"layer_{i}_mlp": hook_store(m, None, o, name)
    )
    handles.append(handle)

# Load small subset of SST-2 for activations extraction
dataset = load_dataset("glue", "sst2", split="validation[:500]")
batch_size = 8
max_len = 64

dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)

print("Collecting activations from model...")
with torch.no_grad():
    for batch in tqdm(dataloader):
        sentences = batch["sentence"]
        inputs = tokenizer(sentences, padding="max_length", truncation=True, max_length=max_len, return_tensors="pt").to(device)
        _ = model(**inputs)

for h in handles:
    h.remove()

# Concatenate lists of tensors into single tensor per layer
for key in activation_storage:
    activation_storage[key] = torch.cat(activation_storage[key], dim=0)
    print(f"{key} activation shape: {activation_storage[key].shape}")

# Define a simple Sparse Autoencoder network
class SAENetwork(nn.Module):
    def __init__(self, d_in, d_sae):
        super().__init__()
        self.encoder = nn.Linear(d_in, d_sae)
        self.decoder = nn.Linear(d_sae, d_in)

    def forward(self, x):
        z = torch.relu(self.encoder(x))
        x_recon = self.decoder(z)
        return x_recon, z

# Training function for SAE
def train_sae(acts, d_sae=128, epochs=10, batch_size=512, lr=1e-3, device='cuda'):
    d_in = acts.shape[-1]
    dataset = TensorDataset(acts)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    model = SAENetwork(d_in, d_sae).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for (batch,) in loader:
            batch = batch.to(device)
            optimizer.zero_grad()
            recon, _ = model(batch)
            loss = criterion(recon, batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * batch.size(0)
        print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataset):.6f}")
    return model

# Train SAE model on each layer's activations
sae_models = {}
for layer_name, acts in activation_storage.items():
    print(f"\nTraining SAE on activations from {layer_name} layer with data shape {acts.shape}")
    sae_model = train_sae(acts, d_sae=128, epochs=10, batch_size=512, lr=1e-3, device=device)
    sae_models[layer_name] = sae_model

print("\nFinished training SAEs for all layers.")

import torch
from collections import defaultdict
from torch.utils.data import DataLoader
from tqdm import tqdm

def extract_top_sae_features(model, tokenizer, sae_models, texts, batch_size=8, max_len=64, top_k=10):
    device = next(iter(sae_models.values())).encoder.weight.device
    activation_storage = defaultdict(list)

    # Hook to store activations from MLP output of each transformer layer
    def hook_store(module, input, output, name):
        activation_storage[name].append(output.detach().cpu())

    handles = []
    for i, layer in enumerate(model.layers):
        handle = layer.mlp.register_forward_hook(
            lambda m, i, o, name=f"layer_{i}_mlp": hook_store(m, None, o, name)
        )
        handles.append(handle)

    dataloader = DataLoader(texts, batch_size=batch_size)

    model.eval()
    with torch.no_grad():
        for batch_texts in tqdm(dataloader, desc="Extracting activations and encoding with SAE"):
            inputs = tokenizer(batch_texts, padding="max_length", truncation=True, max_length=max_len, return_tensors="pt").to(device)
            _ = model(**inputs)

    # Remove hooks
    for h in handles:
        h.remove()

    # Concatenate activations per layer
    for key in activation_storage:
        activation_storage[key] = torch.cat(activation_storage[key], dim=0)

    top_features_per_layer = {}

    for layer_name, acts in activation_storage.items():
        sae_model = sae_models[layer_name].to(device)
        sae_model.eval()

        with torch.no_grad():
            z = sae_model.encoder(acts.to(device))
            z = torch.relu(z)

        # Count positive activations per feature over batch and sequence dims
        feature_counts = (z > 0).sum(dim=(0,1))
        top_features = torch.topk(feature_counts, top_k).indices.cpu().tolist()
        top_features_per_layer[layer_name] = top_features

    return top_features_per_layer

# Example usage:

# Assuming you have:
# - `model` = loaded Qwen 3-0.6B Hooked model
# - `tokenizer` = corresponding tokenizer
# - `sae_models` = dict of trained SAE models per layer from previous training

# Load your texts as a list of strings, e.g. from GLUE SST-2 validation set subset:
from datasets import load_dataset
dataset = load_dataset("glue", "sst2", split="validation[:100]")  # smaller for speed
texts = dataset["sentence"]

# Extract top SAE features per layer on your texts
top_features = extract_top_sae_features(
    model=model,
    tokenizer=tokenizer,
    sae_models=sae_models,
    texts=texts,
    batch_size=8,
    max_len=64,
    top_k=10,
)

# Print the top 10 SAE features per layer
print("\nTop 10 SAE features per layer:")
for layer, features in top_features.items():
    print(f"{layer}: {features}")