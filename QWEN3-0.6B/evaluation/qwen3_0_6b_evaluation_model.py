# -*- coding: utf-8 -*-
"""Qwen3-0.6B model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/rajdeeppal2/qwen3-0-6b-model.bcd2729e-b5e6-496f-9fa2-96dea40b629c.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251013/auto/storage/goog4_request%26X-Goog-Date%3D20251013T082905Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D26685c3666ebc19b770cb50efcdec7d616f2a55b8fc40fb126cca845d41cc88026db7529cfd21b60b6b16e619c0ff9c6734431cb41169ee69f806c70acf96de2dc0731a089d3faac62bb6636fabc3d3f04263cdda4190ea1d8ef7457bd8f72576791747ab4a6af5a8912be8344eab15966ebc8408d2cb95ee38d3fbc7df846c382303857eabc03cd767a49a4d0ae3db69030e4d8820ecf0618d30b3a0527aff86aa7af84f8d1ad203e47609ef13e9432f036f2bea8d6148f1d83d9d420bf4fbcf75ff85bf074ca5afe2d44eac56f00baa3c8355b45a3dc0d20ce1570ee67045e0f83643368cfd0baefe3f09ea028144d7d8c1fcbf4003d5e836f15a4e7c2454d
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

rajdeeppal2_qwen_pytorch_default_1_path = kagglehub.model_download('rajdeeppal2/qwen/PyTorch/default/1')

print('Data source import complete.')

!pip install transformers datasets sentence-transformers torch sae-lens
!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn
!pip install groq

from datasets import load_dataset
from transformers import AutoTokenizer
import os
from torch.utils.data import DataLoader
from datasets import load_dataset, load_from_disk
from transformers import AutoModelForCausalLM
import torch

# Model configuration for Qwen3-0.6B
model_name = "Qwen/Qwen3-0.6B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Qwen3 uses <|endoftext|> as pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load SST-2 dataset
dataset = load_dataset("glue", "sst2")

# Tokenize function
def tokenize(batch):
    return tokenizer(batch["sentence"],
                     padding='max_length',
                     truncation=True,
                     max_length=128)

tokenized_train = dataset["train"].map(tokenize, batched=True)
tokenized_val = dataset["validation"].map(tokenize, batched=True)
tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask"])
tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])

# Save tokenized dataset locally
save_dir = "/kaggle/working/tokenized_sst2_qwen3"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

tokenized_train.save_to_disk(f"{save_dir}/train")
tokenized_val.save_to_disk(f"{save_dir}/validation")

print(f"Tokenized SST-2 saved to {save_dir}")

from sae_lens import (
    LanguageModelSAERunnerConfig,
    SAETrainingRunner,
    StandardTrainingSAEConfig,
    LoggingConfig,
)

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load local tokenized SST-2 datasets
train_dataset = load_from_disk("/kaggle/working/tokenized_sst2_qwen3/train")
validation_dataset = load_from_disk("/kaggle/working/tokenized_sst2_qwen3/validation")

print("Train dataset columns:", train_dataset.column_names)
print("Validation dataset columns:", validation_dataset.column_names)

# Define training parameters
total_training_steps = 80000
batch_size = 64
total_training_tokens = total_training_steps * batch_size

lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5
l1_warm_up_steps = total_training_steps // 20

# Qwen3-0.6B architecture: 28 layers, hidden_size=1024
# Using layer 27 (last layer, 0-indexed) for SAE training
cfg = LanguageModelSAERunnerConfig(
    model_name="Qwen/Qwen3-0.6B",
    hook_name="blocks.27.hook_mlp_out",  # Last layer MLP output (28 layers, 0-indexed)
    dataset_path=None,
    is_dataset_tokenized=True,
    streaming=False,
    sae=StandardTrainingSAEConfig(
        d_in=1024,  # Qwen3-0.6B has hidden_size=1024
        d_sae=2048,  # 2x expansion factor (can use 8192 for 8x)
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=l1_warm_up_steps,
    ),
    lr=5e-5,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=lr_warm_up_steps,
    lr_decay_steps=lr_decay_steps,
    train_batch_size_tokens=batch_size,
    context_size=128,
    n_batches_in_buffer=64,
    training_tokens=total_training_tokens,
    store_batch_size_prompts=16,
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    logger=LoggingConfig(
        log_to_wandb=False,
        wandb_project="sae_lens_sst2_qwen3_0.6b_lastlayer",
        wandb_log_frequency=30,
        eval_every_n_wandb_logs=20,
    ),
    device=device,
    seed=42,
    n_checkpoints=1,
    checkpoint_path="checkpoints",
    dtype="float32",
)

# Train SAE using the loaded dataset
runner = SAETrainingRunner(cfg, override_dataset=train_dataset)
sparse_autoencoder = runner.run()

# Save the trained SAE
save_path = "/kaggle/working"

# If the SAE object has a save method
if hasattr(sparse_autoencoder, "save"):
    sparse_autoencoder.save(save_path)
else:
    # Fallback: save model state dict manually
    torch.save(sparse_autoencoder.state_dict(), os.path.join(save_path, "model_state_dict.pt"))

print(f"SAE model saved to {save_path}")

import torch
from torch.utils.data import DataLoader
from datasets import load_from_disk
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.nn.functional as F
from tqdm import tqdm
import gc

# SAE imports
from sae_lens import StandardTrainingSAE, StandardTrainingSAEConfig, LanguageModelSAERunnerConfig

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load tokenizer and Qwen3-0.6B model
model_name = "Qwen/Qwen3-0.6B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Qwen3 uses <|endoftext|> as pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

qwen_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
qwen_model.eval()

# SAE Config - adjusted for Qwen3-0.6B
sae_cfg = StandardTrainingSAEConfig(
    d_in=1024,  # Qwen3-0.6B has hidden_size=1024
    d_sae=2048,  # 1.5x expansion factor
    apply_b_dec_to_input=False,
    normalize_activations="expected_average_only_in",
    l1_coefficient=0.01,
    l1_warm_up_steps=4000,
)
cfg = LanguageModelSAERunnerConfig(
    model_name="Qwen/Qwen3-0.6B",
    hook_name="blocks.27.hook_mlp_out",
    is_dataset_tokenized=True,
    streaming=False,
    sae=sae_cfg,
    device=device,
)

# Load SAE model weights
local_sae_path = "/kaggle/input/qwen/pytorch/default/1"
weight_file = "model_state_dict.pt"
state_dict = torch.load(f"{local_sae_path}/{weight_file}", map_location=device)
sae_model = StandardTrainingSAE(cfg.sae).to(device)
sae_model.load_state_dict(state_dict)
sae_model.eval()

# Load validation dataset with smaller batch size
val_dataset = load_from_disk("/kaggle/working/tokenized_sst2_qwen3/validation")
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)  # Reduced from 8 to 4

ce_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction='sum')
epsilon = 1e-10
replace_mode = None
hooked_activations = {}

def activation_hook(module, input, output):
    """Hook function to capture and optionally replace MLP activations."""
    global replace_mode

    if isinstance(output, tuple):
        actual_output = output[0]
    else:
        actual_output = output

    hooked_activations['acts'] = actual_output.detach()

    if replace_mode == "sae":
        batch_size, seq_len, hidden_dim = actual_output.shape
        flat_acts = actual_output.view(batch_size * seq_len, hidden_dim)
        with torch.no_grad():
            sae_out = sae_model(flat_acts)
        replacement = sae_out.view(batch_size, seq_len, hidden_dim)

        if isinstance(output, tuple):
            return (replacement,) + output[1:]
        else:
            return replacement

    elif replace_mode == "ablation":
        replacement = torch.zeros_like(actual_output)
        if isinstance(output, tuple):
            return (replacement,) + output[1:]
        else:
            return replacement

    else:
        return output

# Register hook on Qwen3 model's layer 27 MLP
hook_handle = qwen_model.model.layers[27].mlp.register_forward_hook(activation_hook)

# Metric accumulators
total_ce_loss_without_sae = 0.0
total_ce_loss_with_sae = 0.0
total_ce_loss_with_ablation = 0.0

total_kl_div_with_sae = 0.0
total_kl_div_with_ablation = 0.0

total_mse = 0.0
total_explained_variance = 0.0
total_cossim = 0.0

total_l2_norm_in = 0.0
total_l2_norm_out = 0.0
total_l2_ratio = 0.0
total_rel_reconstruction_bias = 0.0

total_l0 = 0
total_l1 = 0.0

total_tokens_eval_reconstruction = 0
total_tokens_eval_sparsity_variance = 0

total_elements = 0
total_samples = 0

for batch_idx, batch in enumerate(tqdm(val_loader, desc="Unified Evaluation")):
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)

    batch_size, seq_len = input_ids.size()
    num_tokens = torch.sum(attention_mask).item()

    # ===== BASELINE (no replacement) =====
    replace_mode = None
    with torch.no_grad():
        outputs_baseline = qwen_model(input_ids, attention_mask=attention_mask)
        logits_baseline = outputs_baseline.logits

        # Compute CE loss immediately
        ce_loss_baseline = ce_loss_fn(
            logits_baseline[:, :-1, :].reshape(-1, logits_baseline.size(-1)),
            input_ids[:, 1:].reshape(-1),
        )
        total_ce_loss_without_sae += ce_loss_baseline.item()

        # Keep only probabilities for KL computation (more memory efficient)
        baseline_probs = torch.softmax(logits_baseline[:, :-1, :], dim=-1)
        del logits_baseline, outputs_baseline
        torch.cuda.empty_cache()

    # ===== WITH SAE REPLACEMENT =====
    replace_mode = "sae"
    with torch.no_grad():
        outputs_sae = qwen_model(input_ids, attention_mask=attention_mask)
        logits_sae = outputs_sae.logits

        # Compute CE loss
        ce_loss_sae = ce_loss_fn(
            logits_sae[:, :-1, :].reshape(-1, logits_sae.size(-1)),
            input_ids[:, 1:].reshape(-1),
        )
        total_ce_loss_with_sae += ce_loss_sae.item()

        # Compute KL divergence in chunks to save memory
        sae_log_probs = torch.log_softmax(logits_sae[:, :-1, :], dim=-1)
        kl_div_sae = torch.sum(baseline_probs * (torch.log(baseline_probs + epsilon) - sae_log_probs))
        total_kl_div_with_sae += kl_div_sae.item()

        del logits_sae, outputs_sae, sae_log_probs, kl_div_sae
        torch.cuda.empty_cache()

    # ===== WITH ABLATION REPLACEMENT =====
    replace_mode = "ablation"
    with torch.no_grad():
        outputs_abl = qwen_model(input_ids, attention_mask=attention_mask)
        logits_abl = outputs_abl.logits

        # Compute CE loss
        ce_loss_abl = ce_loss_fn(
            logits_abl[:, :-1, :].reshape(-1, logits_abl.size(-1)),
            input_ids[:, 1:].reshape(-1),
        )
        total_ce_loss_with_ablation += ce_loss_abl.item()

        # Compute KL divergence
        abl_log_probs = torch.log_softmax(logits_abl[:, :-1, :], dim=-1)
        kl_div_abl = torch.sum(baseline_probs * (torch.log(baseline_probs + epsilon) - abl_log_probs))
        total_kl_div_with_ablation += kl_div_abl.item()

        del logits_abl, outputs_abl, abl_log_probs, kl_div_abl, baseline_probs
        torch.cuda.empty_cache()

    # ===== RECONSTRUCTION METRICS =====
    replace_mode = None
    with torch.no_grad():
        _ = qwen_model(input_ids, attention_mask=attention_mask)

    original_activations = hooked_activations['acts']
    batch_size_, seq_len_, d_in = original_activations.shape

    flat_acts = original_activations.view(batch_size_ * seq_len_, d_in)
    with torch.no_grad():
        reconstruction = sae_model(flat_acts)

    # MSE
    mse = F.mse_loss(reconstruction, flat_acts, reduction='mean').item()
    total_mse += mse * batch_size

    # Explained Variance
    var_target = torch.var(flat_acts, dim=0, unbiased=False)
    var_residual = torch.var(flat_acts - reconstruction, dim=0, unbiased=False)
    ev = 1 - (var_residual / (var_target + epsilon))
    explained_variance = torch.mean(ev).item()
    total_explained_variance += explained_variance * batch_size
    del var_target, var_residual, ev

    # Cosine similarity
    cossim = F.cosine_similarity(reconstruction, flat_acts, dim=1).mean().item()
    total_cossim += cossim * batch_size

    # Shrinkage metrics
    l2_norm_in = torch.norm(flat_acts, dim=1).mean().item()
    l2_norm_out = torch.norm(reconstruction, dim=1).mean().item()
    l2_ratio = l2_norm_out / (l2_norm_in + epsilon)
    rel_bias = reconstruction.abs().sum(dim=1).mean().item() / (flat_acts.abs().sum(dim=1).mean().item() + epsilon)

    total_l2_norm_in += l2_norm_in * batch_size
    total_l2_norm_out += l2_norm_out * batch_size
    total_l2_ratio += l2_ratio * batch_size
    total_rel_reconstruction_bias += rel_bias * batch_size

    # Sparsity metrics
    l0 = torch.count_nonzero(reconstruction).item()
    l1 = reconstruction.abs().sum().item()

    total_l0 += l0
    total_l1 += l1

    total_elements += reconstruction.numel()

    # Clean up
    del flat_acts, reconstruction, original_activations

    # Token statistics
    total_tokens_eval_reconstruction += num_tokens
    total_tokens_eval_sparsity_variance += num_tokens

    total_samples += batch_size

    # Periodic garbage collection
    if batch_idx % 10 == 0:
        torch.cuda.empty_cache()
        gc.collect()

# Remove hook
hook_handle.remove()

# Compute averages
num_tokens_total = total_tokens_eval_reconstruction
ce_loss_without_sae_avg = total_ce_loss_without_sae / num_tokens_total
ce_loss_with_sae_avg = total_ce_loss_with_sae / num_tokens_total
ce_loss_with_ablation_avg = total_ce_loss_with_ablation / num_tokens_total

# Convert KL to per-token averages
kl_div_with_sae_avg = total_kl_div_with_sae / num_tokens_total
kl_div_with_ablation_avg = total_kl_div_with_ablation / num_tokens_total

ce_loss_score = (ce_loss_with_ablation_avg - ce_loss_with_sae_avg) / (ce_loss_with_ablation_avg - ce_loss_without_sae_avg + epsilon)
kl_div_score = (kl_div_with_ablation_avg - kl_div_with_sae_avg) / (kl_div_with_ablation_avg + epsilon)

avg_mse = total_mse / total_samples
avg_explained_variance = total_explained_variance / total_samples
avg_cossim = total_cossim / total_samples

avg_l2_norm_in = total_l2_norm_in / total_samples
avg_l2_norm_out = total_l2_norm_out / total_samples
avg_l2_ratio = total_l2_ratio / total_samples
avg_rel_reconstruction_bias = total_rel_reconstruction_bias / total_samples

avg_l0 = total_l0 / total_elements
avg_l1 = total_l1 / total_samples

# Print all metrics
print("\n" + "="*60)
print("SAE EVALUATION RESULTS FOR QWEN3-0.6B")
print("="*60)

print("\n### Model Behavior Preservation ###")
print(f"Cross-Entropy Loss Without SAE: {ce_loss_without_sae_avg:.4f}")
print(f"Cross-Entropy Loss With SAE: {ce_loss_with_sae_avg:.4f}")
print(f"Cross-Entropy Loss With Ablation: {ce_loss_with_ablation_avg:.4f}")
print(f"CE Loss Score: {ce_loss_score:.4f}")

print(f"\nKL Divergence With SAE: {kl_div_with_sae_avg:.6f}")
print(f"KL Divergence With Ablation: {kl_div_with_ablation_avg:.6f}")
print(f"KL Divergence Score: {kl_div_score:.6f}")

print("\n### Reconstruction Quality ###")
print(f"Reconstruction MSE: {avg_mse:.6f}")
print(f"Reconstruction Explained Variance: {avg_explained_variance:.6f}")
print(f"Reconstruction Cosine Similarity: {avg_cossim:.6f}")

print("\n### Shrinkage Analysis ###")
print(f"L2 Norm Input: {avg_l2_norm_in:.6f}")
print(f"L2 Norm Output: {avg_l2_norm_out:.6f}")
print(f"L2 Norm Ratio: {avg_l2_ratio:.6f}")
print(f"Relative Reconstruction Bias: {avg_rel_reconstruction_bias:.6f}")

print("\n### Sparsity Metrics ###")
print(f"Sparsity L0 (fraction non-zero): {avg_l0:.6f}")
print(f"Sparsity L1 (avg L1 norm): {avg_l1:.6f}")

print("\n### Token Statistics ###")
print(f"Total tokens evaluated: {total_tokens_eval_reconstruction}")
print(f"Total samples processed: {total_samples}")
print("="*60)

