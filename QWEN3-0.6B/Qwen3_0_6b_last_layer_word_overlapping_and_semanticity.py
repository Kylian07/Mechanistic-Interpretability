# -*- coding: utf-8 -*-
"""Qwen3-0.6B last Layer word overlapping and semanticity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUBv2T_mZvISPj-D_5ooeYgaIGPXyEvN
"""

!pip install transformers datasets sentence-transformers torch sae-lens

!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn

import torch
from collections import defaultdict, Counter
from torch.utils.data import DataLoader
from tqdm import tqdm
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

device = next(iter(sae_models.values())).encoder.weight.device

# Last layer setup
last_layer_idx = max(int(k.split('_')[1]) for k in sae_models.keys())
last_layer_name = f"layer_{last_layer_idx}_mlp"

# Collect activations and sentences for last layer as done previously
feature_sentence_map = defaultdict(set)
activation_storage = defaultdict(list)

def hook_store(module, input, output, name):
    activation_storage[name].append(output.detach().cpu())

handles = []
for i, layer in enumerate(model.layers):
    if i == last_layer_idx:
        handle = layer.mlp.register_forward_hook(
            lambda m, i, o, name=f"layer_{i}_mlp": hook_store(m, None, o, name)
        )
        handles.append(handle)

batch_size = 8
max_len = 64
dataloader = DataLoader(texts, batch_size=batch_size)

model.eval()
with torch.no_grad():
    for batch_idx, batch_texts in enumerate(tqdm(dataloader, desc="Extract last layer activations")):
        inputs = tokenizer(batch_texts, padding="max_length", truncation=True, max_length=max_len, return_tensors="pt").to(device)
        _ = model(**inputs)

        layer_acts = activation_storage[last_layer_name][-1].to(device)
        sae_model = sae_models[last_layer_name].to(device)
        z = sae_model.encoder(layer_acts)
        z = torch.relu(z)

        active_feats = (z > 0).any(dim=1)

        for b_idx, feat_vec in enumerate(active_feats):
            global_idx = batch_idx * batch_size + b_idx
            for feat_idx, val in enumerate(feat_vec):
                if val:
                    feature_sentence_map[feat_idx].add(global_idx)

for h in handles:
    h.remove()

# Function to get sentence embeddings from CLS token
def get_sentence_embeddings(text_list, model, tokenizer, device):
    model.eval()
    embeddings = []
    with torch.no_grad():
        for text in text_list:
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=64).to(device)
            out = model(**inputs)
            cls_emb = out.last_hidden_state[:, 0, :].squeeze().cpu().numpy()
            embeddings.append(cls_emb)
    return np.vstack(embeddings) if embeddings else np.array([])

top_k = 10  # number of top features to analyze

# Count frequency of activation across all sentences per feature
feature_activation_counts = {f: len(sents) for f, sents in feature_sentence_map.items()}

# Get top K features by activation count
top_features_sorted = sorted(feature_activation_counts.items(), key=lambda x: x[1], reverse=True)[:top_k]

for feat_idx, count in top_features_sorted:
    sent_indices = list(feature_sentence_map[feat_idx])
    if count < 2:
        continue

    # Compute word frequency dictionary
    all_tokens = []
    for idx in sent_indices:
        tokens = texts[idx].lower().split()
        all_tokens.extend(tokens)
    word_freq = Counter(all_tokens).most_common(10)

    # Compute average semantic similarity
    selected_texts = [texts[i] for i in sent_indices]
    emb = get_sentence_embeddings(selected_texts, model, tokenizer, device)

    if emb.shape[0] > 1:
        sim_matrix = cosine_similarity(emb)
        avg_sim = (sim_matrix.sum() - emb.shape[0]) / (emb.shape[0] * (emb.shape[0] - 1))
    else:
        avg_sim = 0.0

    # Print output similar to your example
    print(f"\nFeature {feat_idx} activates in {count} sentences.")
    print(f"Top common words in activating sentences: {word_freq}")
    print(f"Average semantic similarity between activating sentences: {avg_sim:.4f}")