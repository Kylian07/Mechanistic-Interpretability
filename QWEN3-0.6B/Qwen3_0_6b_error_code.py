# -*- coding: utf-8 -*-
"""Qwen3-0.6B_error-code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1glWnZSsMTxH8aWmKiXfuPke_fgxSvB_w
"""

!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn

import torch
from tqdm import tqdm
from datasets import load_dataset
from sae_lens import SAE, HookedSAETransformer

device = "cuda" if torch.cuda.is_available() else "cpu"

model_name = "qwen/Qwen3-0.6B"

print("Loading HookedSAETransformer model...")
model = HookedSAETransformer.from_pretrained(model_name, device=device)

print("Loading dataset subset for speed...")
dataset = load_dataset("glue", "sst2", split="validation[:50]")  # smaller subset
texts = dataset["sentence"]

batch_size = 1  # small batch size to reduce memory pressure
max_len = 64

# Detect number of layers dynamically if possible
num_layers = model.cfg.n_layers if hasattr(model, "cfg") else 12
layers_to_analyze = [f"transformer.blocks.{i}.hook_resid_pre" for i in range(num_layers)]

top_features_per_layer = {}

for layer_name in layers_to_analyze:
    print(f"\nAnalyzing layer: {layer_name}")
    sae = SAE.from_pretrained(
        release="gpt2-small-res-jb",  # Replace if Qwen-specific SAE available
        sae_id=layer_name,
        device=device,
    )
    feature_counts = torch.zeros(sae.cfg.d_sae, device=device)
    print("Counting feature frequencies...")

    for i in tqdm(range(0, len(texts), batch_size)):
        batch_texts = texts[i : i + batch_size]
        tokens = model.to_tokens(batch_texts)[:, :max_len].to(device)
        _, cache = model.run_with_cache(tokens, names_filter=[sae.cfg.metadata.hook_name])
        sae_in = cache[sae.cfg.metadata.hook_name]
        feature_acts = sae.encode(sae_in).squeeze()  # (batch, seq, d_sae)
        feature_counts += (feature_acts > 0).sum(dim=(0, 1))

        if i % (batch_size * 5) == 0:
            print(f"  Processed {i}/{len(texts)} samples...")

    top_features = torch.topk(feature_counts, 10).indices.tolist()
    top_features_per_layer[layer_name] = top_features
    print(f"Top 10 most frequent features at {layer_name}: {top_features}")

print("\nSummary of top features per layer:")
for layer, features in top_features_per_layer.items():
    print(f"{layer}: {features}")