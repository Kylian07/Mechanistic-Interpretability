0%|          | 0/50 [00:00<?, ?it/s]

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_36/4075291834.py in <cell line: 0>()
     40         _, cache = model.run_with_cache(tokens, names_filter=[sae.cfg.metadata.hook_name]) 
     41         sae_in = cache[sae.cfg.metadata.hook_name]
---> 42         feature_acts = sae.encode(sae_in).squeeze()  # (batch, seq, d_sae)
     43         feature_counts += (feature_acts > 0).sum(dim=(0, 1))
     44 

/usr/local/lib/python3.11/dist-packages/sae_lens/saes/standard_sae.py in encode(self, x)
     62         """
     63         # Preprocess the SAE input (casting type, applying hooks, normalization)
---> 64         sae_in = self.process_sae_in(x)
     65         # Compute the pre-activation values
     66         hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)

/usr/local/lib/python3.11/dist-packages/sae_lens/saes/sae.py in process_sae_in(self, sae_in)
    471         # print(f"Bias term shape: {bias_term.shape}")
    472 
--> 473         return sae_in - bias_term
    474 
    475     def forward(self, x: torch.Tensor) -> torch.Tensor:

RuntimeError: The size of tensor a (1024) must match the size of tensor b (768) at non-singleton dimension 2