# -*- coding: utf-8 -*-
"""GPT2 sentiment classification accuracy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUBv2T_mZvISPj-D_5ooeYgaIGPXyEvN
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2Model
from tqdm import tqdm
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load SST-2 dataset
dataset = load_dataset("glue", "sst2")

# Load GPT2 tokenizer and model (without classification head)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
gpt2 = GPT2Model.from_pretrained("gpt2").to(device)

# Define a simple classification head on top of GPT2 pooled output
class GPT2ForSequenceClassification(nn.Module):
    def __init__(self, gpt2_model, hidden_dim=768, num_labels=2):
        super().__init__()
        self.gpt2 = gpt2_model
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(hidden_dim, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)
        # Pool by mean pooling of token embeddings
        hidden_states = outputs.last_hidden_state
        pooled_output = hidden_states.mean(dim=1)
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

model = GPT2ForSequenceClassification(gpt2).to(device)

# Load SST-2 train and validation data
def tokenize(batch):
    return tokenizer(batch["sentence"], padding=True, truncation=True, max_length=64)

train_dataset = dataset["train"].map(tokenize, batched=True)
val_dataset = dataset["validation"].map(tokenize, batched=True)

columns = ["input_ids", "attention_mask", "label"]

train_dataset.set_format(type="torch", columns=columns)
val_dataset.set_format(type="torch", columns=columns)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

def collate_fn(batch):
    # batch is a list of dicts, each dict with 'input_ids', 'attention_mask', 'label'
    labels = torch.tensor([item["label"] for item in batch], dtype=torch.long)
    batch = data_collator(batch)
    batch["labels"] = labels
    return batch

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)



# Training loop for classification head (train only for a few epochs to demonstrate)
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

print("Training GPT2 sentiment classification head...")
model.train()
for epoch in range(3):
    loop = tqdm(train_loader, desc=f"Epoch {epoch+1}")
    for batch in loop:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        loop.set_postfix(loss=loss.item())

# Evaluation function
def evaluate(model, dataloader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, attention_mask)
            preds = torch.argmax(outputs, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return correct / total

# Compute accuracy on validation set
val_accuracy = evaluate(model, val_loader)
print(f"\nValidation Accuracy of pretrained GPT2-small on SST-2: {val_accuracy * 100:.2f}%")