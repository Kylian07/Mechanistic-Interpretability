# -*- coding: utf-8 -*-
"""Interpretibility score

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/interpretibility-score.aaf95f18-b01c-43f4-a1b5-4c73b24e1a43.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251007/auto/storage/goog4_request%26X-Goog-Date%3D20251007T153935Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4113fd8e8548f8955e3bf80430a85cf20093308996fa816b30c2b8832622d671f1020c457563bf78b29433384429f0c31309405c5731204c2d51575afe33d526491f077726935a4314151aec1914543bb3b82f1800e188fdb1f9da5650ff155c0ebae205eb666d8d97b8403ce5028b80a36301361262323d840918c3911d8d668248b8071b46990eeb8ae36703dc0b65edeaf6ef6272df827b1d60c54e023d215da92008d7dfc853206e9a79e005d098d7a36e8f48bc1750716c7ea1e660c0230f50fc06b26e055279823461a3d0ce89fe174d24d523eb85652e247fc7ef62b7ad8035f3e5d319c5663955174b604261984931b8f92a4306def1a67494a4f0a0
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install sae-lens transformers datasets wandb
!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn
# %pip install sae-lens transformer-lens circuitsvis

from datasets import load_dataset
from transformers import AutoTokenizer
import os
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2Model

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Load SST-2 dataset
dataset = load_dataset("glue", "sst2")

# Load SST-2 train and validation data
def tokenize(batch):
    return tokenizer(batch["sentence"],
                     padding='max_length',  # Pad all sequences to max_length
                     truncation=True,
                     max_length=128)

tokenized_train = dataset["train"].map(tokenize, batched=True)
tokenized_val= dataset["validation"].map(tokenize, batched=True)
tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask"])
tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])


# Save tokenized dataset locally
save_dir = "/kaggle/working/tokenized_sst2_gpt2"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

tokenized_train.save_to_disk(f"{save_dir}/train")
tokenized_val.save_to_disk(f"{save_dir}/validation")

print(f"Tokenized SST-2 saved to {save_dir}")


from datasets import load_from_disk
import torch
import os

from sae_lens import (
    LanguageModelSAERunnerConfig,
    SAETrainingRunner,
    StandardTrainingSAEConfig,
    LoggingConfig,
)


import torch

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load local tokenized SST-2 datasets saved with save_to_disk()
train_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/train")
validation_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/validation")

# Check that expected columns exist
print("Train dataset columns:", train_dataset.column_names)
print("Validation dataset columns:", validation_dataset.column_names)

# Define training parameters
total_training_steps = 80000
batch_size = 64
total_training_tokens = total_training_steps * batch_size

lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5
l1_warm_up_steps = total_training_steps // 20

# Define SAE-Lens config
cfg = LanguageModelSAERunnerConfig(
    model_name="gpt2",  # Correct GPT-2 model ID
    hook_name="blocks.11.hook_mlp_out",  # Last layer MLP output hook
    dataset_path=None,                    # Not used when override_dataset is provided
    is_dataset_tokenized=True,
    streaming=False,
    sae=StandardTrainingSAEConfig(
        d_in=768,
        d_sae=1536,
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=l1_warm_up_steps,
    ),
    lr=5e-5,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=lr_warm_up_steps,
    lr_decay_steps=lr_decay_steps,
    train_batch_size_tokens=batch_size,
    context_size=128,
    n_batches_in_buffer=64,
    training_tokens=total_training_tokens,
    store_batch_size_prompts=16,
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    logger=LoggingConfig(
        log_to_wandb=False,
        wandb_project="sae_lens_sst2_gpt2small_lastlayer_tokenized_local",
        wandb_log_frequency=30,
        eval_every_n_wandb_logs=20,
    ),
    device=device,
    seed=42,
    n_checkpoints=1,
    checkpoint_path="checkpoints",
    dtype="float32",
)

# Instantiate and run SAE training using the loaded dataset
runner = SAETrainingRunner(cfg, override_dataset=train_dataset)
sparse_autoencoder = runner.run()
save_path = "/kaggle/working/saved_sae_model"
os.makedirs(save_path, exist_ok=True)

# If the SAE object has a save method
if hasattr(sparse_autoencoder, "save"):
    sparse_autoencoder.save(save_path)
else:
    # Fallback: save model state dict manually
    torch.save(sparse_autoencoder.state_dict(), os.path.join(save_path, "model_state_dict.pt"))

print(f"SAE model saved to {save_path}")

import torch
import random
from datasets import load_dataset
from transformers import GPT2Tokenizer
from sae_lens import HookedSAETransformer, StandardTrainingSAE, StandardTrainingSAEConfig
import os

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Load SST-2 train split
dataset = load_dataset("glue", "sst2", split="train")
# Filter positive sentences
positive_sentences = [x["sentence"] for x in dataset if x["label"] == 1]
# Select 50 random positive sentences
sample_sentences = random.sample(positive_sentences, 50)

# Load your HookedSAETransformer and SAE
model = HookedSAETransformer.from_pretrained("gpt2-small", device=device)
sae_cfg = StandardTrainingSAEConfig(
    d_in=768,
    d_sae=1536,
    apply_b_dec_to_input=False,
    normalize_activations="expected_average_only_in",
    l1_coefficient=0.01,
    l1_warm_up_steps=50,
)
sae = StandardTrainingSAE(sae_cfg)
# Load SAE weights
local_sae_path = "/kaggle/working/saved_sae_model"
weight_file = "model_state_dict.pt"
sae.load_state_dict(torch.load(os.path.join(local_sae_path, weight_file), map_location=device))
sae.to(device)
sae.eval()

layer_name = "blocks.11.hook_mlp_out"
max_len = 64

# For each sentence, get the set of activated (nonzero) features
activated_features_per_sentence = []

for sent in sample_sentences:
    tokens = model.to_tokens([sent])[:, :max_len].to(device)
    _, cache = model.run_with_cache(tokens, names_filter=[layer_name])
    sae_in = cache[layer_name]
    feature_acts = sae.encode(sae_in).squeeze()  # Shape: (seq, d_sae)
    # Aggregate activation across sequence: take union of nonzero indices (OR)
    nonzero_features = (feature_acts > 0).any(dim=0).nonzero(as_tuple=True)[0].cpu().tolist()
    activated_features_per_sentence.append(set(nonzero_features))


# Compute common (intersection) features across all 50 sentences
common_features = set.intersection(*activated_features_per_sentence)
print("Common activated features across all 50 positive sentences:", sorted(common_features))
print("No of activated features: ",len(common_features))

# Filter negative sentences
negative_sentences = [x["sentence"] for x in dataset if x["label"] == 0]
# Select 50 random negative sentences
sample_negative_sentences = random.sample(negative_sentences, 50)

activated_features_per_sentence_neg = []

for sent in sample_negative_sentences:
    tokens = model.to_tokens([sent])[:, :max_len].to(device)
    _, cache = model.run_with_cache(tokens, names_filter=[layer_name])
    sae_in = cache[layer_name]
    feature_acts = sae.encode(sae_in).squeeze()  # (seq, d_sae)
    nonzero_features = (feature_acts > 0).any(dim=0).nonzero(as_tuple=True)[0].cpu().tolist()
    activated_features_per_sentence_neg.append(set(nonzero_features))

common_features_neg = set.intersection(*activated_features_per_sentence_neg)
print("Common activated features across all 50 negative sentences:", sorted(common_features_neg))
print("No of activated features: ",len(common_features_neg))

common_elements_both = list(set(common_features) & set(common_features_neg))
print("Common activated features across all 50 negative sentences:", sorted(common_elements_both))
print("No of activated features: ",len(common_elements_both))

"""# Overall interpretibility score"""

import os
import torch
import numpy as np
from scipy.stats import pearsonr
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from tqdm.auto import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

# Step A: Get SAE activations for both sets (shape: num_samples x d_sae)
def get_sae_activations_for_sentences(sentences, model, sae, max_len=64, batch_size=16):
    activations_list = []
    for i in range(0, len(sentences), batch_size):
        batch = sentences[i : i + batch_size]
        tokens = model.to_tokens(batch)[:, :max_len].to(device)           # (batch, seq_len)
        _, cache = model.run_with_cache(tokens, names_filter=[layer_name])
        sae_in = cache[layer_name].to(device)                             # (batch, seq_len, d_in)
        with torch.no_grad():
            # encode expects shape (batch, seq_len, d_in) and returns (batch, seq_len, d_sae)
            feats = sae.encode(sae_in)
            # Pool across sequence (mean) to get per-example vector; other pooling (max) is possible
            pooled = feats.mean(dim=1)                                    # (batch, d_sae)
        activations_list.append(pooled.cpu())
    return torch.cat(activations_list, dim=0)                             # (num_samples, d_sae)

# Compute activations
pos_activations = get_sae_activations_for_sentences(sample_sentences, model, sae, max_len=max_len)
neg_activations = get_sae_activations_for_sentences(sample_negative_sentences, model, sae, max_len=max_len)

# Combined inputs for simulation (texts)
combined_inputs = sample_sentences + sample_negative_sentences  # list length = N_pos + N_neg

# Step B: Setup small proxy LLM for explanations & simulation (replaceable)
gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token
gpt2_lm = GPT2LMHeadModel.from_pretrained("gpt2").to(device)
gpt2_lm.eval()

def generate_feature_explanation(feature_idx, top_texts, max_new_tokens=60, temp=0.7):
    prompt = f"Feature {feature_idx} activates on examples like:\n"
    prompt += "\n".join(top_texts[:8]) + "\n"
    prompt += "Describe what this feature represents in one concise sentence."
    inputs = gpt2_tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    outputs = gpt2_lm.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temp, top_p=0.9)
    text = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Post-process: return model's completion after the prompt
    explanation = text[len(prompt):].strip() if text.startswith(prompt) else text.strip()
    # Truncate to first sentence
    explanation = explanation.split("\n")[0].strip()
    return explanation

def simulate_activation(explanation, texts, max_new_tokens=8):
    preds = []
    for t in texts:
        prompt = f"Feature description: {explanation}\nFor the sentence: \"{t}\"\nRate how strongly this feature would activate on a scale from 0 (not at all) to 1 (very strongly). Answer with a single number between 0 and 1."
        inputs = gpt2_tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
        outputs = gpt2_lm.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
        resp = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Try to parse a decimal number from the end of response
        try:
            # take last token(s) and extract numeric substring
            candidate = resp.strip().split()[-1]
            score = float(candidate)
        except:
            # fallback: try to find first occurrence of a number in the response
            import re
            m = re.search(r"([01](?:\.\d+)?)", resp)
            if m:
                score = float(m.group(1))
            else:
                score = 0.5
        # clamp
        score = max(0.0, min(1.0, score))
        preds.append(score)
    return np.array(preds)

# Step C: Interpretability scoring for common features
def interpretability_for_common_features(common_features, pos_acts, neg_acts, pos_texts, neg_texts, top_k=10):
    scores = {}
    combined_texts = pos_texts + neg_texts
    combined_acts = torch.cat([pos_acts, neg_acts], dim=0).cpu().numpy()   # shape (N_total, d_sae)

    for feat in tqdm(sorted(common_features), desc="Features"):
        # get top activating examples from positive and negative separately
        pos_vals = pos_acts[:, feat].cpu().numpy()
        neg_vals = neg_acts[:, feat].cpu().numpy()
        top_pos_idx = np.argsort(pos_vals)[-top_k:]
        top_neg_idx = np.argsort(neg_vals)[-top_k:]
        top_texts = [pos_texts[i] for i in top_pos_idx] + [neg_texts[i] for i in top_neg_idx]

        # generate explanation
        explanation = generate_feature_explanation(feat, top_texts)

        # simulate activations on combined texts
        predicted = simulate_activation(explanation, combined_texts)

        actual = combined_acts[:, feat]

        # safe Pearson
        if np.isnan(actual).any() or np.isnan(predicted).any() or np.std(actual) == 0 or np.std(predicted) == 0:
            corr = 0.0
        else:
            corr, _ = pearsonr(actual, predicted)

        scores[feat] = {"corr": float(corr), "explanation": explanation}
    # overall
    overall = float(np.mean([v["corr"] for v in scores.values()])) if scores else 0.0
    return overall, scores

# Call the scoring function
overall_score, per_feat_info = interpretability_for_common_features(
    common_elements_both, pos_activations, neg_activations, sample_sentences, sample_negative_sentences, top_k=10
)

print("Overall interpretability score for common features:", overall_score)
for f, info in list(per_feat_info.items())[:10]:
    print(f"Feature {f}: corr={info['corr']:.3f}, explanation='{info['explanation']}'")

"""# Single feature interpretibility score"""

import random
import numpy as np
from scipy.stats import pearsonr

# Pick one random feature from existing common features
if not common_elements_both:
    raise ValueError("common_elements_both list is empty!")
random_feature = random.choice(list(common_elements_both))
print(f"Selected random feature for interpretability: {random_feature}")

# Gather activations and combined inputs
pos_vals = pos_activations[:, random_feature].cpu().numpy()
neg_vals = neg_activations[:, random_feature].cpu().numpy()
combined_acts = np.concatenate([pos_vals, neg_vals])
combined_texts = sample_sentences + sample_negative_sentences

# Collect top activating samples from both sets
top_k = 10
top_pos_idx = np.argsort(pos_vals)[-top_k:]
top_neg_idx = np.argsort(neg_vals)[-top_k:]
top_texts = [sample_sentences[i] for i in top_pos_idx] + [sample_negative_sentences[i] for i in top_neg_idx]

# Generate description for this feature
def generate_feature_explanation(feature_idx, example_texts):
    prompt = f"Feature {feature_idx} activates on examples like:\n"
    prompt += "\n".join(example_texts[:8])
    prompt += "\nDescribe what this feature represents in one concise sentence."
    inputs = gpt2_tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    outputs = gpt2_lm.generate(**inputs, max_new_tokens=60, do_sample=True, temperature=0.7, top_p=0.9)
    text = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)
    explanation = text[len(prompt):].strip() if text.startswith(prompt) else text.strip()
    explanation = explanation.split("\n")[0].strip()
    return explanation

# Simulate activations using LLM and feature description
def simulate_activation(explanation, texts):
    preds = []
    for t in texts:
        prompt = f"Feature description: {explanation}\nFor the sentence: \"{t}\"\nRate how strongly this feature would activate on a scale from 0 (not at all) to 1 (very strongly). Answer with a single number between 0 and 1."
        inputs = gpt2_tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
        outputs = gpt2_lm.generate(**inputs, max_new_tokens=8, do_sample=False)
        resp = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)
        try:
            candidate = resp.strip().split()[-1]
            score = float(candidate)
        except:
            import re
            m = re.search(r"([01](?:\.\d+)?)", resp)
            if m:
                score = float(m.group(1))
            else:
                score = 0.5
        preds.append(max(0.0, min(1.0, score)))
    return np.array(preds)

# Run explanation and simulation
explanation = generate_feature_explanation(random_feature, top_texts)
print(f"\nExplanation for feature {random_feature}:", explanation)

predicted_acts = simulate_activation(explanation, combined_texts)

# Calculate (safe) Pearson correlation
if np.std(combined_acts) == 0 or np.std(predicted_acts) == 0 or np.isnan(combined_acts).any() or np.isnan(predicted_acts).any():
    corr = 0.0
else:
    corr, _ = pearsonr(combined_acts, predicted_acts)
print(f"\nInterpretability score (Pearson correlation) for feature {random_feature}: {corr:.3f}")

