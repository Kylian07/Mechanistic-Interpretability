# -*- coding: utf-8 -*-
"""SAE training and evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xKJjOcZhCxrxspxY9r2zFjRMo9bL2t-I
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install sae-lens transformers datasets wandb
!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn
# %pip install sae-lens transformer-lens circuitsvis

from datasets import load_dataset
from transformers import AutoTokenizer
import os
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2Model

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Load SST-2 dataset
dataset = load_dataset("glue", "sst2")

# Load SST-2 train and validation data
def tokenize(batch):
    return tokenizer(batch["sentence"],
                     padding='max_length',  # Pad all sequences to max_length
                     truncation=True,
                     max_length=128)

tokenized_train = dataset["train"].map(tokenize, batched=True)
tokenized_val= dataset["validation"].map(tokenize, batched=True)
tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask"])
tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])


# Save tokenized dataset locally
save_dir = "/kaggle/working/tokenized_sst2_gpt2"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

tokenized_train.save_to_disk(f"{save_dir}/train")
tokenized_val.save_to_disk(f"{save_dir}/validation")

print(f"Tokenized SST-2 saved to {save_dir}")
lengths = [len(x["input_ids"]) for x in tokenized_train]
print(f"Min length: {min(lengths)}, Max length: {max(lengths)}")

from datasets import load_from_disk
import torch
import os

from sae_lens import (
    LanguageModelSAERunnerConfig,
    SAETrainingRunner,
    StandardTrainingSAEConfig,
    LoggingConfig,
)


import torch

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load local tokenized SST-2 datasets saved with save_to_disk()
train_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/train")
validation_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/validation")

# Check that expected columns exist
print("Train dataset columns:", train_dataset.column_names)
print("Validation dataset columns:", validation_dataset.column_names)

# Define training parameters
total_training_steps = 1000
batch_size = 64
total_training_tokens = total_training_steps * batch_size

lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5
l1_warm_up_steps = total_training_steps // 20

# Define SAE-Lens config
cfg = LanguageModelSAERunnerConfig(
    model_name="gpt2",  # Correct GPT-2 model ID
    hook_name="blocks.11.hook_mlp_out",  # Last layer MLP output hook
    dataset_path=None,                    # Not used when override_dataset is provided
    is_dataset_tokenized=True,
    streaming=False,
    sae=StandardTrainingSAEConfig(
        d_in=768,
        d_sae=512,
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=l1_warm_up_steps,
    ),
    lr=5e-5,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=lr_warm_up_steps,
    lr_decay_steps=lr_decay_steps,
    train_batch_size_tokens=batch_size,
    context_size=128,
    n_batches_in_buffer=64,
    training_tokens=total_training_tokens,
    store_batch_size_prompts=16,
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    logger=LoggingConfig(
        log_to_wandb=False,
        wandb_project="sae_lens_sst2_gpt2small_lastlayer_tokenized_local",
        wandb_log_frequency=30,
        eval_every_n_wandb_logs=20,
    ),
    device=device,
    seed=42,
    n_checkpoints=1,
    checkpoint_path="checkpoints",
    dtype="float32",
)

# Instantiate and run SAE training using the loaded dataset
runner = SAETrainingRunner(cfg, override_dataset=train_dataset)
sparse_autoencoder = runner.run()
save_path = "/kaggle/working/saved_sae_model"
os.makedirs(save_path, exist_ok=True)

# If the SAE object has a save method
if hasattr(sparse_autoencoder, "save"):
    sparse_autoencoder.save(save_path)
else:
    # Fallback: save model state dict manually
    torch.save(sparse_autoencoder.state_dict(), os.path.join(save_path, "model_state_dict.pt"))

print(f"SAE model saved to {save_path}")

import torch
from torch.utils.data import DataLoader
from datasets import load_from_disk
from transformers import GPT2Model, GPT2Tokenizer
import matplotlib.pyplot as plt
import os
from tqdm import tqdm
from sae_lens import StandardTrainingSAE, LanguageModelSAERunnerConfig, StandardTrainingSAEConfig

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load tokenizer and GPT-2 model (frozen, eval mode)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
gpt2_model = GPT2Model.from_pretrained("gpt2").to(device)
gpt2_model.eval()

# SAE Config
layer_name = "blocks.11.hook_mlp_out"
sae_cfg = StandardTrainingSAEConfig(
    d_in=768,
    d_sae=512,
    apply_b_dec_to_input=False,
    normalize_activations="expected_average_only_in",
    l1_coefficient=0.01,
    l1_warm_up_steps=50,
)
cfg = LanguageModelSAERunnerConfig(
    model_name="gpt2",
    hook_name=layer_name,
    dataset_path=None,
    is_dataset_tokenized=True,
    streaming=False,
    sae=sae_cfg,
    lr=5e-5,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=0,
    lr_decay_steps=200,
    train_batch_size_tokens=4096,
    context_size=128,
    n_batches_in_buffer=64,
    training_tokens=4096 * 1000,
    store_batch_size_prompts=16,
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    device=device,
    seed=42,
    n_checkpoints=1,
    checkpoint_path="checkpoints",
    dtype="float32",
)

# Load SAE model and weights
local_sae_path = "/kaggle/working/saved_sae_model"
weight_file = "model_state_dict.pt"
state_dict = torch.load(f"{local_sae_path}/{weight_file}", map_location=device)
sae_model = StandardTrainingSAE(cfg.sae).to(device)
sae_model.load_state_dict(state_dict)

# Load tokenized SST-2 dataset
train_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/train")
validation_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/validation")

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)

optimizer = torch.optim.Adam(sae_model.parameters(), lr=5e-5)
criterion = torch.nn.MSELoss()

train_losses = []
val_losses = []
num_epochs = 10

for epoch in range(num_epochs):
    sae_model.train()
    running_train_loss = 0.0
    train_bar = tqdm(train_loader, desc=f"Epoch {epoch+1} Training", unit="batch")
    for batch in train_bar:
        input_ids = batch["input_ids"].to(device)

        with torch.no_grad():
            gpt2_activations = gpt2_model(input_ids).last_hidden_state
        batch_size, seq_len, hidden_dim = gpt2_activations.shape
        sae_input = gpt2_activations.view(batch_size * seq_len, hidden_dim)

        optimizer.zero_grad()
        sae_output = sae_model(sae_input)
        loss = criterion(sae_output, sae_input)
        loss.backward()
        optimizer.step()

        running_train_loss += loss.item()
        train_bar.set_postfix(loss=loss.item())

    avg_train_loss = running_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    sae_model.eval()
    running_val_loss = 0.0
    val_bar = tqdm(val_loader, desc=f"Epoch {epoch+1} Validation", unit="batch")
    with torch.no_grad():
        for batch in val_bar:
            input_ids = batch["input_ids"].to(device)
            gpt2_activations = gpt2_model(input_ids).last_hidden_state
            batch_size, seq_len, hidden_dim = gpt2_activations.shape
            sae_input = gpt2_activations.view(batch_size * seq_len, hidden_dim)

            sae_output = sae_model(sae_input)
            loss = criterion(sae_output, sae_input)
            running_val_loss += loss.item()
            val_bar.set_postfix(loss=loss.item())

    avg_val_loss = running_val_loss / len(val_loader)
    val_losses.append(avg_val_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f} Validation Loss: {avg_val_loss:.4f}")

plt.figure(figsize=(8,6))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss Curves')
plt.legend()
plt.show()

save_path = '/kaggle/working/saved_sae_model'
os.makedirs(save_path, exist_ok=True)
torch.save(sae_model.state_dict(), os.path.join(save_path, 'sae_model_final.pt'))

import torch
from torch.utils.data import DataLoader
from datasets import load_from_disk
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch.nn.functional as F
from tqdm import tqdm

# SAE imports (adjust if needed)
from sae_lens import StandardTrainingSAE, StandardTrainingSAEConfig, LanguageModelSAERunnerConfig

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load tokenizer and GPT2LMHeadModel
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2").to(device)
gpt2_model.eval()

# SAE Config (adjust as needed)
sae_cfg = StandardTrainingSAEConfig(
    d_in=768,
    d_sae=512,
    apply_b_dec_to_input=False,
    normalize_activations="expected_average_only_in",
    l1_coefficient=0.01,
    l1_warm_up_steps=50,
)
cfg = LanguageModelSAERunnerConfig(
    model_name="gpt2",
    hook_name="blocks.11.hook_mlp_out",
    is_dataset_tokenized=True,
    streaming=False,
    sae=sae_cfg,
    device=device,
)

# Load SAE model weights
local_sae_path = "/kaggle/working/saved_sae_model"
weight_file = "model_state_dict.pt"
state_dict = torch.load(f"{local_sae_path}/{weight_file}", map_location=device)
sae_model = StandardTrainingSAE(cfg.sae).to(device)
sae_model.load_state_dict(state_dict)
sae_model.eval()

# Load validation dataset and DataLoader
val_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/validation")
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)  # reduced batch size for memory

ce_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
epsilon = 1e-10
replace_mode = None
hooked_activations = {}

def activation_hook(module, input, output):
    global replace_mode
    hooked_activations['acts'] = output.detach()

    if replace_mode == "sae":
        batch_size, seq_len, hidden_dim = output.shape
        flat_acts = output.view(batch_size * seq_len, hidden_dim)
        with torch.no_grad():
            sae_out = sae_model(flat_acts)
        replacement = sae_out.view(batch_size, seq_len, hidden_dim)
        return replacement

    elif replace_mode == "ablation":
        return torch.zeros_like(output)

    else:
        return output

hook_handle = gpt2_model.transformer.h[11].mlp.register_forward_hook(activation_hook)

# Metric accumulators
total_ce_loss_without_sae = 0.0
total_ce_loss_with_sae = 0.0
total_ce_loss_with_ablation = 0.0

total_kl_div_with_sae = 0.0
total_kl_div_with_ablation = 0.0

total_mse = 0.0
total_explained_variance = 0.0
total_cossim = 0.0

total_l2_norm_in = 0.0
total_l2_norm_out = 0.0
total_l2_ratio = 0.0
total_rel_reconstruction_bias = 0.0

total_l0 = 0
total_l1 = 0.0

total_tokens_eval_reconstruction = 0
total_tokens_eval_sparsity_variance = 0

total_elements = 0
total_samples = 0

for batch_idx, batch in enumerate(tqdm(val_loader, desc="Unified Evaluation")):
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)

    batch_size, seq_len = input_ids.size()
    num_tokens = torch.sum(attention_mask).item()

    # Baseline (no replacement)
    replace_mode = None
    with torch.no_grad():
        outputs_baseline = gpt2_model(input_ids, attention_mask=attention_mask)
        logits_baseline = outputs_baseline.logits

    baseline_probs = torch.softmax(logits_baseline[:, :-1, :], dim=-1).clamp(min=epsilon)
    ce_loss_baseline = ce_loss_fn(
        logits_baseline[:, :-1, :].reshape(-1, logits_baseline.size(-1)),
        input_ids[:, 1:].reshape(-1),
    )
    total_ce_loss_without_sae += ce_loss_baseline.item() * batch_size

    # With SAE replacement
    replace_mode = "sae"
    with torch.no_grad():
        outputs_sae = gpt2_model(input_ids, attention_mask=attention_mask)
        logits_sae = outputs_sae.logits

    sae_probs = torch.softmax(logits_sae[:, :-1, :], dim=-1).clamp(min=epsilon)
    ce_loss_sae = ce_loss_fn(
        logits_sae[:, :-1, :].reshape(-1, logits_sae.size(-1)),
        input_ids[:, 1:].reshape(-1),
    )
    total_ce_loss_with_sae += ce_loss_sae.item() * batch_size

    kl_div_sae = F.kl_div(sae_probs.log(), baseline_probs, reduction='batchmean')
    total_kl_div_with_sae += kl_div_sae.item() * batch_size

    # With ablation replacement
    replace_mode = "ablation"
    with torch.no_grad():
        outputs_abl = gpt2_model(input_ids, attention_mask=attention_mask)
        logits_abl = outputs_abl.logits

    abl_probs = torch.softmax(logits_abl[:, :-1, :], dim=-1).clamp(min=epsilon)
    ce_loss_abl = ce_loss_fn(
        logits_abl[:, :-1, :].reshape(-1, logits_abl.size(-1)),
        input_ids[:, 1:].reshape(-1),
    )
    total_ce_loss_with_ablation += ce_loss_abl.item() * batch_size

    kl_div_abl = F.kl_div(abl_probs.log(), baseline_probs, reduction='batchmean')
    total_kl_div_with_ablation += kl_div_abl.item() * batch_size

    # Reconstruction metrics (use no replacement to get original activations)
    replace_mode = None
    with torch.no_grad():
        _ = gpt2_model(input_ids, attention_mask=attention_mask)
    original_activations = hooked_activations['acts']  # [batch, seq_len, d_in]
    batch_size_, seq_len_, d_in = original_activations.shape

    flat_acts = original_activations.view(batch_size_ * seq_len_, d_in)
    with torch.no_grad():
        reconstruction = sae_model(flat_acts)

    # MSE
    mse = F.mse_loss(reconstruction, flat_acts, reduction='mean').item()
    total_mse += mse * batch_size

    # Explained Variance
    var_target = torch.var(flat_acts, dim=0, unbiased=False)
    var_residual = torch.var(flat_acts - reconstruction, dim=0, unbiased=False)
    ev = 1 - (var_residual / (var_target + epsilon))
    explained_variance = torch.mean(ev).item()
    total_explained_variance += explained_variance * batch_size

    # Cosine similarity
    cossim = F.cosine_similarity(reconstruction, flat_acts, dim=1).mean().item()
    total_cossim += cossim * batch_size

    # Shrinkage metrics
    l2_norm_in = torch.norm(flat_acts, dim=1).mean().item()
    l2_norm_out = torch.norm(reconstruction, dim=1).mean().item()
    l2_ratio = l2_norm_out / (l2_norm_in + epsilon)
    rel_bias = reconstruction.abs().sum(dim=1).mean().item() / (flat_acts.abs().sum(dim=1).mean().item() + epsilon)

    total_l2_norm_in += l2_norm_in * batch_size
    total_l2_norm_out += l2_norm_out * batch_size
    total_l2_ratio += l2_ratio * batch_size
    total_rel_reconstruction_bias += rel_bias * batch_size

    # Sparsity metrics
    l0 = torch.count_nonzero(reconstruction).item()
    l1 = reconstruction.abs().sum().item()

    total_l0 += l0
    total_l1 += l1

    total_elements += reconstruction.numel()

    # Token statistics for reconstruction metrics
    total_tokens_eval_reconstruction += num_tokens

    # Token statistics for sparsity and variance metrics
    total_tokens_eval_sparsity_variance += num_tokens

    total_samples += batch_size

# Remove hook
hook_handle.remove()

# Compute averages
ce_loss_without_sae_avg = total_ce_loss_without_sae / total_samples
ce_loss_with_sae_avg = total_ce_loss_with_sae / total_samples
ce_loss_with_ablation_avg = total_ce_loss_with_ablation / total_samples

kl_div_with_sae_avg = total_kl_div_with_sae / total_samples
kl_div_with_ablation_avg = total_kl_div_with_ablation / total_samples

ce_loss_score = (ce_loss_with_ablation_avg - ce_loss_with_sae_avg) / (ce_loss_with_ablation_avg - ce_loss_without_sae_avg + epsilon)
kl_div_score = (kl_div_with_ablation_avg - kl_div_with_sae_avg) / (kl_div_with_ablation_avg + epsilon)

avg_mse = total_mse / total_samples
avg_explained_variance = total_explained_variance / total_samples
avg_cossim = total_cossim / total_samples

avg_l2_norm_in = total_l2_norm_in / total_samples
avg_l2_norm_out = total_l2_norm_out / total_samples
avg_l2_ratio = total_l2_ratio / total_samples
avg_rel_reconstruction_bias = total_rel_reconstruction_bias / total_samples

avg_l0 = total_l0 / total_elements
avg_l1 = total_l1 / total_samples

# Print all metrics
print(f"Cross-Entropy Loss Without SAE: {ce_loss_without_sae_avg:.4f}")
print(f"Cross-Entropy Loss With SAE: {ce_loss_with_sae_avg:.4f}")
print(f"Cross-Entropy Loss With Ablation: {ce_loss_with_ablation_avg:.4f}")
print(f"CE Loss Score: {ce_loss_score:.4f}")

print(f"KL Divergence With SAE: {kl_div_with_sae_avg:.6f}")
print(f"KL Divergence With Ablation: {kl_div_with_ablation_avg:.6f}")
print(f"KL Divergence Score: {kl_div_score:.6f}")

print(f"Reconstruction MSE: {avg_mse:.6f}")
print(f"Reconstruction Explained Variance: {avg_explained_variance:.6f}")
print(f"Reconstruction Cosine Similarity: {avg_cossim:.6f}")

print(f"L2 Norm Input: {avg_l2_norm_in:.6f}")
print(f"L2 Norm Output: {avg_l2_norm_out:.6f}")
print(f"L2 Norm Ratio: {avg_l2_ratio:.6f}")
print(f"Relative Reconstruction Bias: {avg_rel_reconstruction_bias:.6f}")

print(f"Sparsity L0 (fraction non-zero): {avg_l0:.6f}")
print(f"Sparsity L1 (avg L1 norm): {avg_l1:.6f}")

print(f"Total tokens for reconstruction evaluation: {total_tokens_eval_reconstruction}")
print(f"Total tokens for sparsity and variance evaluation: {total_tokens_eval_sparsity_variance}")