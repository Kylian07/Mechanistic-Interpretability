# -*- coding: utf-8 -*-
"""GPT2 last Layer word overlap and semantic matching.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUBv2T_mZvISPj-D_5ooeYgaIGPXyEvN
"""

!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn

import torch
from datasets import load_dataset
from sae_lens import SAE, HookedSAETransformer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import numpy as np
import torch.nn.functional as F

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load model and dataset
model = HookedSAETransformer.from_pretrained("gpt2-small", device=device)
dataset = load_dataset("glue", "sst2", split="validation[:500]")
texts = dataset["sentence"]
labels = dataset["label"]

batch_size = 8
max_len = 64

# Last GPT2 layer hook name
layer_name = "blocks.11.hook_resid_pre"

# Load SAE for last layer
sae = SAE.from_pretrained(
    release="gpt2-small-res-jb",
    sae_id=layer_name,
    device=device,
)

feature_acts_list = []

print(f"Extracting feature activations from last layer: {layer_name} ...")
for i in range(0, len(texts), batch_size):
    batch_texts = texts[i : i + batch_size]
    tokens = model.to_tokens(batch_texts)[:, :max_len].to(device)
    _, cache = model.run_with_cache(tokens, names_filter=[sae.cfg.metadata.hook_name])
    sae_in = cache[sae.cfg.metadata.hook_name]
    feature_acts = sae.encode(sae_in).squeeze()  # (batch, seq, d_sae)
    feature_acts_list.append(feature_acts.cpu())

# After collecting all feature_acts tensors (before concatenation):
max_seq_len = max(t.shape[1] for t in feature_acts_list)

padded_acts = []
for t in feature_acts_list:
    seq_len = t.shape[1]
    if seq_len < max_seq_len:
        # Pad sequence length dimension (dim=1) with zeros
        pad_amount = max_seq_len - seq_len
        # pad = (pad_left, pad_right) for last 2 dims - here (0,0) for dim=2 features, pad for dim=1 seq_len
        padded_t = F.pad(t, (0, 0, 0, pad_amount))  # padding for dim1: (left=0, right=pad_amount)
        padded_acts.append(padded_t)
    else:
        padded_acts.append(t)

feature_activations = torch.cat(padded_acts, dim=0)  # shape: (num_samples, max_seq_len, d_sae)
# Task 3 analysis starts here
activation_threshold = 1e-5

# Pool activations by max over seq dimension
sample_feature_max = feature_activations.max(dim=1).values.detach().cpu().numpy()


# Load sentence transformer model for semantic similarity
sem_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# Get top 10 frequent features for this layer
feature_counts = (feature_activations > activation_threshold).sum(dim=(0, 1)).numpy()
top_features = feature_counts.argsort()[-10:][::-1].tolist()

print("\nTop 10 frequent features indices:", top_features)

for feature_idx in top_features:
    active_samples_idx = np.where(sample_feature_max[:, feature_idx] > activation_threshold)[0]
    active_texts = [texts[i] for i in active_samples_idx]

    print(f"\nFeature {feature_idx} activates in {len(active_texts)} sentences.")

    # Word overlap analysis
    tokenized_texts = [set(t.lower().split()) for t in active_texts]
    word_counts = Counter()
    for tokens in tokenized_texts:
        word_counts.update(tokens)
    most_common_words = word_counts.most_common(10)
    print("Top common words in activating sentences:", most_common_words)

    # Semantic similarity analysis
    if len(active_texts) > 1:
        embeddings = sem_model.encode(active_texts, convert_to_tensor=True)
        sim_matrix = cosine_similarity(embeddings.cpu().numpy())
        n = sim_matrix.shape[0]
        avg_sim = (sim_matrix.sum() - n) / (n * (n - 1))
        print(f"Average semantic similarity between activating sentences: {avg_sim:.4f}")
    else:
        print("Not enough activating sentences for semantic similarity analysis.")