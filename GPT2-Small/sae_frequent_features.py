# -*- coding: utf-8 -*-
"""SAE Frequent Features.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ZZxFW3ozT_F9SsGW5P9a0ESBHOoKIyQ
"""

!pip install sae-lens transformers datasets wandb
!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn

import torch
from tqdm import tqdm
from datasets import load_dataset
from sae_lens import (
    HookedSAETransformer,
    StandardTrainingSAE,
    LanguageModelSAERunnerConfig,
    StandardTrainingSAEConfig,
)
import os

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load GPT-2 small with hooks
model = HookedSAETransformer.from_pretrained("gpt2-small", device=device)

# Load SST-2 validation subset for speed
dataset = load_dataset("glue", "sst2", split="validation[:500]")
texts = dataset["sentence"]

batch_size = 8
max_len = 64

# Layers to analyze (12 transformer blocks)
layers_to_analyze = [f"blocks.{i}.hook_mlp_out" for i in range(12)]

# Path to your locally saved SAE model directory and weights
local_sae_path = "/kaggle/working/saved_sae_model"
weight_file = "model_state_dict.pt"

# SAE config used during your training (modify if needed)
base_sae_cfg = StandardTrainingSAEConfig(
    d_in=768,
    d_sae=512,
    apply_b_dec_to_input=False,
    normalize_activations="expected_average_only_in",
    l1_coefficient=0.01,
    l1_warm_up_steps=50,
)

top_features_per_layer = {}

for layer_name in layers_to_analyze:
    print(f"\nAnalyzing layer: {layer_name}")
    
    # Create SAE config for this layer with correct hook_name
    sae_runner_cfg = LanguageModelSAERunnerConfig(
        model_name="gpt2",
        hook_name=layer_name,
        dataset_path=None,
        is_dataset_tokenized=True,
        streaming=False,
        sae=base_sae_cfg,
        lr=5e-5,
        adam_beta1=0.9,
        adam_beta2=0.999,
        lr_scheduler_name="constant",
        lr_warm_up_steps=0,
        lr_decay_steps=200,
        train_batch_size_tokens=4096,
        context_size=53,
        n_batches_in_buffer=64,
        training_tokens=4096 * 1000,
        store_batch_size_prompts=16,
        feature_sampling_window=1000,
        dead_feature_window=1000,
        dead_feature_threshold=1e-4,
        device=device,
        seed=42,
        n_checkpoints=1,
        checkpoint_path="checkpoints",
        dtype="float32",
    )
    
    # Instantiate SAE model for this layer
    sae = StandardTrainingSAE(sae_runner_cfg.sae)
    
    # Load weights from local saved file
    state_dict_path = os.path.join(local_sae_path, weight_file)
    state_dict = torch.load(state_dict_path, map_location=device)
    sae.load_state_dict(state_dict)
    sae.to(device)
    sae.eval()
    
    feature_counts = torch.zeros(sae.cfg.d_sae, device=device)
    print("Counting feature frequencies...")
    
    for i in tqdm(range(0, len(texts), batch_size)):
        batch_texts = texts[i : i + batch_size]
        tokens = model.to_tokens(batch_texts)[:, :max_len].to(device)
        _, cache = model.run_with_cache(tokens, names_filter=[layer_name])
        sae_in = cache[layer_name]
        feature_acts = sae.encode(sae_in).squeeze()  # (batch, seq, d_sae)
        feature_counts += (feature_acts > 0).sum(dim=(0, 1))
    
    top_features = torch.topk(feature_counts, 10).indices.tolist()
    top_features_per_layer[layer_name] = top_features
    print(f"Top 10 most frequent features at {layer_name}: {top_features}")

# Print summary after all layers
print("\nSummary of top features per layer:")
for layer, features in top_features_per_layer.items():
    print(f"{layer}: {features}")
