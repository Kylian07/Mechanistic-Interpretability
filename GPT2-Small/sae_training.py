# -*- coding: utf-8 -*-
"""SAE training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ZZxFW3ozT_F9SsGW5P9a0ESBHOoKIyQ
"""

!pip install sae-lens transformers datasets wandb
!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn

from datasets import load_dataset
from transformers import AutoTokenizer
import os
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2Model

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Load SST-2 dataset
dataset = load_dataset("glue", "sst2")

# Load SST-2 train and validation data
def tokenize(batch):
    return tokenizer(batch["sentence"],
                     padding='max_length',
                     truncation=True,
                     max_length=53)

tokenized_train = dataset["train"].map(tokenize, batched=True)
tokenized_val= dataset["validation"].map(tokenize, batched=True)
tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask"])
tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])

save_dir = "/kaggle/working/tokenized_sst2_gpt2"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

tokenized_train.save_to_disk(f"{save_dir}/train")
tokenized_val.save_to_disk(f"{save_dir}/validation")

print(f"Tokenized SST-2 saved to {save_dir}")

from datasets import load_from_disk
import torch
import os

from sae_lens import (
    LanguageModelSAERunnerConfig,
    SAETrainingRunner,
    StandardTrainingSAEConfig,
    LoggingConfig,
)
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

train_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/train")
validation_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/validation")

# Check that expected columns exist
print("Train dataset columns:", train_dataset.column_names)
print("Validation dataset columns:", validation_dataset.column_names)

# Define training parameters
total_training_steps = 1000
batch_size = 64
total_training_tokens = total_training_steps * batch_size

lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5
l1_warm_up_steps = total_training_steps // 20

# Define SAE-Lens config
cfg = LanguageModelSAERunnerConfig(
    model_name="gpt2",  # Correct GPT-2 model ID
    hook_name="blocks.11.hook_mlp_out",  # Last layer MLP output hook
    dataset_path=None,                    # Not used when override_dataset is provided
    is_dataset_tokenized=True,
    streaming=False,
    sae=StandardTrainingSAEConfig(
        d_in=768,
        d_sae=512,
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=l1_warm_up_steps,
    ),
    lr=5e-5,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=lr_warm_up_steps,
    lr_decay_steps=lr_decay_steps,
    train_batch_size_tokens=batch_size,
    context_size=53,
    n_batches_in_buffer=64,
    training_tokens=total_training_tokens,
    store_batch_size_prompts=16,
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    logger=LoggingConfig(
        log_to_wandb=False,
        wandb_project="sae_lens_sst2_gpt2small_lastlayer_tokenized_local",
        wandb_log_frequency=30,
        eval_every_n_wandb_logs=20,
    ),
    device=device,
    seed=42,
    n_checkpoints=1,
    checkpoint_path="checkpoints",
    dtype="float32",
)

# Instantiate and run SAE training using the loaded dataset
runner = SAETrainingRunner(cfg, override_dataset=train_dataset)
sparse_autoencoder = runner.run()
save_path = "/kaggle/working/saved_sae_model"
os.makedirs(save_path, exist_ok=True)

# If the SAE object has a save method
if hasattr(sparse_autoencoder, "save"):
    sparse_autoencoder.save(save_path)
else:
    # Fallback: save model state dict manually
    torch.save(sparse_autoencoder.state_dict(), os.path.join(save_path, "model_state_dict.pt"))

print(f"SAE model saved to {save_path}")