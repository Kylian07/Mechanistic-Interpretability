# -*- coding: utf-8 -*-
"""SAE semantic Similarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ZZxFW3ozT_F9SsGW5P9a0ESBHOoKIyQ
"""

!pip install sae-lens transformers datasets wandb
!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn

import torch
from datasets import load_dataset
from sae_lens import (
    LanguageModelSAERunnerConfig,
    StandardTrainingSAEConfig,
    StandardTrainingSAE,
    HookedSAETransformer,
)
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import numpy as np
import torch.nn.functional as F
import os

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load GPT-2 small with hooks
model = HookedSAETransformer.from_pretrained("gpt2-small", device=device)

# Load SST-2 validation subset (first 500 samples)
dataset = load_dataset("glue", "sst2", split="validation[:500]")
texts = dataset["sentence"]
labels = dataset["label"]

batch_size = 8
max_len = 64
layer_name = "blocks.11.hook_mlp_out"  # Use same hook as training

# Path to saved SAE model directory and weights
local_sae_path = "/kaggle/working/saved_sae_model"
weight_file = "model_state_dict.pt"

# SAE-specific configuration (must match training config)
sae_cfg = StandardTrainingSAEConfig(
    d_in=768,
    d_sae=512,
    apply_b_dec_to_input=False,
    normalize_activations="expected_average_only_in",
    l1_coefficient=0.01,
    l1_warm_up_steps=50,
)

# Overall SAE runner config
cfg = LanguageModelSAERunnerConfig(
    model_name="gpt2",
    hook_name=layer_name,
    dataset_path=None,
    is_dataset_tokenized=True,
    streaming=False,
    sae=sae_cfg,
    lr=5e-5,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=0,
    lr_decay_steps=200,
    train_batch_size_tokens=4096,
    context_size=53,
    n_batches_in_buffer=64,
    training_tokens=4096 * 1000,
    store_batch_size_prompts=16,
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    device=device,
    seed=42,
    n_checkpoints=1,
    checkpoint_path="checkpoints",
    dtype="float32",
)

# Instantiate SAE model (concrete subclass)
sae = StandardTrainingSAE(cfg.sae)

# Load saved model weights
state_dict_path = os.path.join(local_sae_path, weight_file)
state_dict = torch.load(state_dict_path, map_location=device)
sae.load_state_dict(state_dict)
sae.to(device)
sae.eval()

feature_acts_list = []
print(f"Extracting feature activations from layer: {layer_name} ...")

for i in range(0, len(texts), batch_size):
    batch_texts = texts[i : i + batch_size]
    tokens = model.to_tokens(batch_texts)[:, :max_len].to(device)
    _, cache = model.run_with_cache(tokens, names_filter=[layer_name])
    sae_in = cache[layer_name]
    feature_acts = sae.encode(sae_in).squeeze()
    feature_acts_list.append(feature_acts.cpu())

max_seq_len = max(t.shape[1] for t in feature_acts_list)
padded_acts = []
for t in feature_acts_list:
    seq_len = t.shape[1]
    if seq_len < max_seq_len:
        pad_amount = max_seq_len - seq_len
        padded_t = F.pad(t, (0, 0, 0, pad_amount))
        padded_acts.append(padded_t)
    else:
        padded_acts.append(t)
feature_activations = torch.cat(padded_acts, dim=0)

activation_threshold = 1e-5
sample_feature_max = feature_activations.max(dim=1).values.detach().cpu().numpy()

sem_model = SentenceTransformer("all-MiniLM-L6-v2", device=device)

feature_counts = (feature_activations > activation_threshold).sum(dim=(0, 1)).numpy()
top_features = feature_counts.argsort()[-10:][::-1].tolist()
print("\nTop 10 frequent features indices:", top_features)

for feature_idx in top_features:
    active_samples_idx = np.where(sample_feature_max[:, feature_idx] > activation_threshold)[0]
    active_texts = [texts[i] for i in active_samples_idx]
    print(f"\nFeature {feature_idx} activates in {len(active_texts)} sentences.")

    tokenized_texts = [set(t.lower().split()) for t in active_texts]
    word_counts = Counter()
    for tokens in tokenized_texts:
        word_counts.update(tokens)
    most_common_words = word_counts.most_common(10)
    print("Top common words in activating sentences:", most_common_words)

    if len(active_texts) > 1:
        embeddings = sem_model.encode(active_texts, convert_to_tensor=True)
        sim_matrix = cosine_similarity(embeddings.cpu().numpy())
        n = sim_matrix.shape[0]
        avg_sim = (sim_matrix.sum() - n) / (n * (n - 1))
        print(f"Average semantic similarity between activating sentences: {avg_sim:.4f}")
    else:
        print("Not enough activating sentences for semantic similarity analysis.")