# -*- coding: utf-8 -*-
"""Interpretibility score

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/interpretibility-score.e96ad7a4-1c50-4dee-a0c8-3edd2624de36.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251010/auto/storage/goog4_request%26X-Goog-Date%3D20251010T143507Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7610570dbf401881038a3878f24d8270f6ab068f1dccb3f9ad07e3d5229511e5bc8b6826a094a8af267a415616376f72a31cf809b260fe6cd9c963bb3135e72b34a5a822a6c8ddbbb4bd4551607ba4a0a454fd9d483d33071c18788842155952ce52607ebc7e884a462a1b080d1d1b7349de6517d65cb6a228123b6edcc6eaffa695563e552a795c50d0ace7ce7986c6701f45c07ffe5992d248e42d5f2d2d16fed097c82adbe0a99965bbec8599a157afb7f99618f34c035d8f6b179429046f0bd44b3919ab06bf1d2204313605c0c5a226328039ed0b08019d51b3e7e09d26d0e6bb08200fd694418aa7be01a336cc11345469690a1356532d2b83734e4a53
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install sae-lens transformers datasets wandb
!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn
# %pip install sae-lens transformer-lens circuitsvis
!pip install openai
!pip install groq

from datasets import load_dataset
from transformers import AutoTokenizer
import os
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2Model

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Load SST-2 dataset
dataset = load_dataset("glue", "sst2")

# Load SST-2 train and validation data
def tokenize(batch):
    return tokenizer(batch["sentence"],
                     padding='max_length',  # Pad all sequences to max_length
                     truncation=True,
                     max_length=128)

tokenized_train = dataset["train"].map(tokenize, batched=True)
tokenized_val= dataset["validation"].map(tokenize, batched=True)
tokenized_train.set_format(type="torch", columns=["input_ids", "attention_mask"])
tokenized_val.set_format(type="torch", columns=["input_ids", "attention_mask"])


# Save tokenized dataset locally
save_dir = "/kaggle/working/tokenized_sst2_gpt2"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

tokenized_train.save_to_disk(f"{save_dir}/train")
tokenized_val.save_to_disk(f"{save_dir}/validation")

print(f"Tokenized SST-2 saved to {save_dir}")


from datasets import load_from_disk
import torch
import os

from sae_lens import (
    LanguageModelSAERunnerConfig,
    SAETrainingRunner,
    StandardTrainingSAEConfig,
    LoggingConfig,
)


import torch

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load local tokenized SST-2 datasets saved with save_to_disk()
train_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/train")
validation_dataset = load_from_disk("/kaggle/working/tokenized_sst2_gpt2/validation")

# Check that expected columns exist
print("Train dataset columns:", train_dataset.column_names)
print("Validation dataset columns:", validation_dataset.column_names)

# Define training parameters
total_training_steps = 80000
batch_size = 64
total_training_tokens = total_training_steps * batch_size

lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5
l1_warm_up_steps = total_training_steps // 20

# Define SAE-Lens config
cfg = LanguageModelSAERunnerConfig(
    model_name="gpt2",  # Correct GPT-2 model ID
    hook_name="blocks.11.hook_mlp_out",  # Last layer MLP output hook
    dataset_path=None,                    # Not used when override_dataset is provided
    is_dataset_tokenized=True,
    streaming=False,
    sae=StandardTrainingSAEConfig(
        d_in=768,
        d_sae=1536,
        apply_b_dec_to_input=False,
        normalize_activations="expected_average_only_in",
        l1_coefficient=0.01,
        l1_warm_up_steps=l1_warm_up_steps,
    ),
    lr=5e-5,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=lr_warm_up_steps,
    lr_decay_steps=lr_decay_steps,
    train_batch_size_tokens=batch_size,
    context_size=128,
    n_batches_in_buffer=64,
    training_tokens=total_training_tokens,
    store_batch_size_prompts=16,
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    logger=LoggingConfig(
        log_to_wandb=False,
        wandb_project="sae_lens_sst2_gpt2small_lastlayer_tokenized_local",
        wandb_log_frequency=30,
        eval_every_n_wandb_logs=20,
    ),
    device=device,
    seed=42,
    n_checkpoints=1,
    checkpoint_path="checkpoints",
    dtype="float32",
)

# Instantiate and run SAE training using the loaded dataset
runner = SAETrainingRunner(cfg, override_dataset=train_dataset)
sparse_autoencoder = runner.run()
save_path = "/kaggle/working/saved_sae_model"
os.makedirs(save_path, exist_ok=True)

# If the SAE object has a save method
if hasattr(sparse_autoencoder, "save"):
    sparse_autoencoder.save(save_path)
else:
    # Fallback: save model state dict manually
    torch.save(sparse_autoencoder.state_dict(), os.path.join(save_path, "model_state_dict.pt"))

print(f"SAE model saved to {save_path}")

import os
import torch
import numpy as np
from torch.utils.data import DataLoader
from datasets import load_from_disk, load_dataset
from scipy.stats import pearsonr
import random
from kaggle_secrets import UserSecretsClient
from groq import Groq
from tqdm import tqdm
import re
from transformers import GPT2Tokenizer, GPT2Model

# === Initialize Groq Llama 3 client ===
user_secrets = UserSecretsClient()
groq_api_key = user_secrets.get_secret("GROQ_API_KEY")
os.environ["GROQ_API_KEY"] = groq_api_key
client = Groq()

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    response = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

# --- SAE wrapper class ---
class SAEWithLastLayerActivations(torch.nn.Module):
    def __init__(self, sae_model):
        super().__init__()
        self.sae = sae_model
        self.activations = None

        hook_module = getattr(self.sae, "hook_sae_acts_post", None)
        if hook_module is None:
            raise ValueError("hook_sae_acts_post not found in SAE model")
        hook_module.register_forward_hook(self._hook_fn)

    def _hook_fn(self, module, input, output):
        self.activations = output.detach()

    def forward(self, sae_input):
        _ = self.sae(sae_input)
        return self.activations

    def get_activations(self, sae_input):
        self.eval()
        with torch.no_grad():
            _ = self.forward(sae_input)
            return self.activations.cpu()

# --- Setup device and tokenizer ---
device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

gpt2_model = GPT2Model.from_pretrained("gpt2").to(device)
gpt2_model.eval()

# --- Function to extract activations with sentences ---
def extract_activations_and_texts(model_wrapper, tokenized_dataset, sentences, feature_idx, device, max_batches=None):
    model_wrapper.eval()
    all_activations = []
    all_texts = []
    all_tokens = []

    dataloader = DataLoader(tokenized_dataset, batch_size=32)
    for i, batch in enumerate(tqdm(dataloader, desc="Extracting activations")):
        if max_batches is not None and i >= max_batches:
            break
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        with torch.no_grad():
            outputs = gpt2_model(input_ids=input_ids, attention_mask=attention_mask)
            hidden_states = outputs.last_hidden_state  # (B, seq_len, 768)

            activations = model_wrapper.get_activations(hidden_states)  # (B, seq_len, d_sae)

        batch_acts = activations[:, :, feature_idx].cpu().numpy()
        batch_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids.cpu().tolist()]
        batch_attn = attention_mask.cpu().numpy()

        start_idx = i * dataloader.batch_size
        batch_texts = sentences[start_idx : start_idx + len(batch["input_ids"])]

        for tokens, acts, attn_mask, text in zip(batch_tokens, batch_acts, batch_attn, batch_texts):
            filtered_tokens = []
            filtered_acts = []
            for t, a, m in zip(tokens, acts, attn_mask):
                if m == 1:
                    filtered_tokens.append(t)
                    filtered_acts.append(a)
            all_tokens.append(filtered_tokens)
            all_activations.append(np.array(filtered_acts))
            all_texts.append(text)

    return all_activations, all_texts, all_tokens

# --- Select top sentences ---
def select_top_sentences(acts, texts, num_top=20):
    scores = [np.sum(a) for a in acts]  # Sum of activations per sentence
    idxs = np.argsort(scores)[::-1][:num_top]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]


# --- Generate interpretation ---
def get_llama3_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a specific feature from a neural network that activates on certain input sentences. \n\n"
        "Given the following sentences along with the per-token activation strengths of this feature "
        "(higher numbers indicate stronger activation), provide a concise and clear single-sentence explanation "
        "describing what causes this feature to activate strongly. Focus on semantic or syntactic patterns.\n\n"
    )

    for i, (text, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{text}\"\nActivations: {acts.tolist()}\n\n"

    prompt += "Provide your interpretation below:"
    return chat_with_llama(prompt)


# --- Select eval sentences ---
def select_eval_sentences(acts, texts, n=5):
    scores = [np.max(a) for a in acts]
    idxs = np.argsort(scores)[::-1]
    high_idxs = idxs[:n]
    remaining = [i for i in range(len(acts)) if i not in high_idxs]
    nonzero = [i for i in remaining if np.std(acts[i]) > 0]
    rand_idxs = random.sample(nonzero, n)
    eval_idxs = list(high_idxs) + rand_idxs
    return [acts[i] for i in eval_idxs], [texts[i] for i in eval_idxs]

# --- Predict activation ---
def get_llama3_activation(sentence, interpretation):
    prompt = (
        f'You are given an interpretation of a neural network feature as:\n\n"{interpretation}"\n\n'
        'Given the above interpretation, analyze the following sentence and estimate how strongly this '
        'feature activates over the entire sentence on a scale from 0 (not active) to 10 (very active).\n\n'
        f'Sentence: "{sentence}"\n\nPlease provide only a single numeric score between 0 and 10 as your activation estimate.\nActivation:'
    )
    print(f"Predicting activation for sentence:\n{sentence}")
    response = chat_with_llama(prompt)
    match = re.search(r"\d+(\.\d+)?", response)
    if match:
        return float(match.group())
    else:
        print(f"Warning: Could not parse activation, got response: {response}")
        return 0.0

# --- Compute correlation ---
def compute_correlation(actual, predicted):
    # Convert lists to numpy arrays and rescale actual activations from 0-1 to 0-10
    actual_scalar = np.array([np.mean(a) for a in actual])
    print(actual_scalar)
    actual_scalar = actual_scalar * 10.0
    print(actual_scalar)
    predicted_scalar = np.array(predicted)
    print(predicted_scalar)
    corr, _ = pearsonr(actual_scalar, predicted_scalar)
    return corr


def save_top_activations(filename, sentences, actual_acts, predicted_acts):
    with open(filename, "w", encoding="utf-8") as f:
        f.write("Sentence\tActual Activation\tPredicted Activation\n")
        for sent, actual, pred in zip(sentences, actual_acts, predicted_acts):
            # Rescale actual activations here before writing
            actual_score = np.mean(actual) * 10 if isinstance(actual, np.ndarray) else actual * 10
            f.write(f"{sent}\t{actual_score:.4f}\t{pred:.4f}\n")
    print(f"Top activations saved to {filename}")

def save_interpretability_report(
    filename,
    feature_name,
    interpretation_sentences,
    interpretation_acts,
    interpretation_text,
    eval_sentences,
    eval_actual_acts,
    eval_predicted_acts,
    interpretability_score
):
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"Feature: {feature_name}\n\n")

        f.write("Top 5 sentences sent for interpretation:\n")
        for i, (sent, acts) in enumerate(zip(interpretation_sentences, interpretation_acts), 1):
            avg_act = np.mean(acts) * 10 if isinstance(acts, np.ndarray) else acts * 10
            f.write(f"{i}. {sent}\n   Average Activation: {avg_act:.4f}\n")
        f.write("\n")

        f.write("Llama 3 Interpretation:\n")
        f.write(interpretation_text.strip() + "\n\n")

        f.write("Other 5 high activating sentences sent for prediction:\n")
        for i, (sent, actual, pred) in enumerate(zip(eval_sentences, eval_actual_acts, eval_predicted_acts), 1):
            actual_score = np.mean(actual) * 10 if isinstance(actual, np.ndarray) else actual * 10
            f.write(f"{i}. {sent}\n   Actual Activation: {actual_score:.4f}\n   Predicted Activation: {pred:.4f}\n")
        f.write("\n")

        f.write(f"Overall Interpretability Pearson Correlation Score: {interpretability_score:.4f}\n")

    print(f"Interpretability report saved to {filename}")



# In your interpretability_score_pipeline, after prediction and before correlation calculation add:
def interpretability_score_pipeline(sparse_autoencoder, validation_tokenized, validation_sentences, feature_index, device):
    print("Extracting per-token activations and sentences...")
    all_acts, all_texts, all_tokens = extract_activations_and_texts(
        sae_with_acts, validation_tokenized, validation_sentences, feature_index, device)

    print("Selecting top 20 sentences for interpretation prompt...")
    top_acts, top_texts = select_top_sentences(all_acts, all_texts, 20)

    # Use only top 5 for interpretation prompt
    interpretation_acts = top_acts[:5]
    interpretation_texts = top_texts[:5]

    print("\nTop 5 sentences used for interpretation prompt:")
    for i, sent in enumerate(interpretation_texts, 1):
        print(f"{i}. {sent}")

    print("Getting interpretation from Llama 3...")
    interpretation = get_llama3_interpretation(interpretation_texts, interpretation_acts)
    print("\n--- Feature Interpretation ---")
    print(interpretation)
    print("--- End of Interpretation ---\n")

    # Exclude the interpretation sentences for evaluation:
    interpretation_set = set(interpretation_texts)
    rest_acts_texts = [(act, txt) for act, txt in zip(all_acts, all_texts) if txt not in interpretation_set]

    # Sort the remaining sentences by sum or max activation as preferred
    rest_acts, rest_texts = zip(*rest_acts_texts)
    rest_acts = list(rest_acts)
    rest_texts = list(rest_texts)

    print("Selecting top 5 activating sentences (excluding interpretation ones) for evaluation...")
    scores = [np.sum(a) for a in rest_acts]  # or use max(a) if preferred
    idxs = np.argsort(scores)[::-1][:5]
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    print("\nTop 5 evaluation sentences (excluded from interpretation):")
    for i, sent in enumerate(eval_texts, 1):
        print(f"{i}. {sent}")

    print("Getting predicted activations from Llama 3 for evaluation sentences...")
    predicted_acts = [get_llama3_activation(sent, interpretation) for sent in eval_texts]

    # Save top evaluation sentences & activations if desired
    save_top_activations("eval_top_activations.txt", eval_texts, eval_acts, predicted_acts)

    print("Calculating interpretability score on evaluation sentences...")
    score = compute_correlation(eval_acts, predicted_acts)
    print(f"\nInterpretability Pearson Correlation Score on held-out top 5 sentences: {score:.4f}")

    save_interpretability_report(
    filename="/kaggle/working/interpretability_report.txt",
    feature_name=f"Feature {feature_index}",
    interpretation_sentences=interpretation_texts,
    interpretation_acts=interpretation_acts,
    interpretation_text=interpretation,
    eval_sentences=eval_texts,
    eval_actual_acts=eval_acts,
    eval_predicted_acts=predicted_acts,
    interpretability_score=score
    )


# === Main execution ===
if __name__ == "__main__":
    validation_dataset_path = "/kaggle/working/tokenized_sst2_gpt2/validation"
    validation_tokenized = load_from_disk(validation_dataset_path)
    validation_original = load_dataset("glue", "sst2", split="validation")

    # Load your trained SAE model here (replace appropriately)
    # sparse_autoencoder = load_your_sae_model()
    sparse_autoencoder.to(device)

    sae_with_acts = SAEWithLastLayerActivations(sparse_autoencoder)
    sae_with_acts.to(device)
    sae_with_acts.eval()

    interpretability_score_pipeline(
        sparse_autoencoder,
        validation_tokenized,
        validation_original["sentence"],
        feature_index=1311,
        device=device
    )

