# -*- coding: utf-8 -*-
"""GPT2 analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jRhHy41g9-XYB8K3RgPhnFK6CmZEDYR7
"""

import torch
from datasets import load_dataset
from transformer_lens import HookedTransformer
from sae_lens import SAE
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import numpy as np
from collections import Counter
import os
import json
import joblib

# Kaggle directories
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.makedirs('/kaggle/working/results', exist_ok=True)

os.environ['HF_HOME'] = '/kaggle/working/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load GPT-2 Small
print("Loading GPT-2 Small model...")
model = HookedTransformer.from_pretrained("gpt2-small", device=device)
model.eval()

print(f"Model has {model.cfg.n_layers} layers (0-{model.cfg.n_layers-1})")

# Load SST-2 dataset
print("Loading SST-2 dataset...")
dataset = load_dataset("glue", "sst2")
train_dataset = dataset["train"]
val_dataset = dataset["validation"]

print(f"Train samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")

# ============================================================================
# TASK 1: Record Baseline Performance (Zero-Shot Prompting)
# ============================================================================
print("\n" + "="*70)
print("TASK 1: BASELINE PERFORMANCE - ZERO-SHOT PROMPTING")
print("="*70)

baseline_predictions = []
baseline_labels = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Zero-Shot Evaluation"):
        sentence = val_dataset[idx]["sentence"]
        true_label = val_dataset[idx]["label"]

        prompt = f"""Classify the sentiment as positive or negative.

Sentence: {sentence}
Sentiment:"""

        tokens = model.to_tokens(prompt)
        logits = model(tokens)

        next_token_logits = logits[0, -1, :]
        next_token_id = torch.argmax(next_token_logits).item()
        generated_text = model.tokenizer.decode([next_token_id]).strip().lower()

        if "positive" in generated_text or generated_text.startswith("pos"):
            prediction = 1
        elif "negative" in generated_text or generated_text.startswith("neg"):
            prediction = 0
        else:
            pos_token_id = model.tokenizer.encode(" positive")[0]
            neg_token_id = model.tokenizer.encode(" negative")[0]

            if next_token_logits[pos_token_id] > next_token_logits[neg_token_id]:
                prediction = 1
            else:
                prediction = 0

        baseline_predictions.append(prediction)
        baseline_labels.append(true_label)

baseline_predictions = np.array(baseline_predictions)
baseline_labels = np.array(baseline_labels)

baseline_acc = accuracy_score(baseline_labels, baseline_predictions)
baseline_p, baseline_r, baseline_f1, _ = precision_recall_fscore_support(
    baseline_labels, baseline_predictions, average='binary'
)
baseline_cm = confusion_matrix(baseline_labels, baseline_predictions)

print("\n### BASELINE RESULTS ###")
print(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"Precision: {baseline_p:.4f}")
print(f"Recall: {baseline_r:.4f}")
print(f"F1-Score: {baseline_f1:.4f}")
print(f"\nConfusion Matrix:")
print(f"              Neg    Pos")
print(f"Actual Neg  [{baseline_cm[0,0]:5d}  {baseline_cm[0,1]:5d}]")
print(f"       Pos  [{baseline_cm[1,0]:5d}  {baseline_cm[1,1]:5d}]")

baseline_results = {
    'accuracy': float(baseline_acc),
    'precision': float(baseline_p),
    'recall': float(baseline_r),
    'f1_score': float(baseline_f1),
    'confusion_matrix': baseline_cm.tolist()
}

with open('/kaggle/working/results/baseline_results.json', 'w') as f:
    json.dump(baseline_results, f, indent=2)

print("\n✓ Baseline results saved")

# ============================================================================
# TASK 2: Extract SAE Feature Activations
# ============================================================================
print("\n" + "="*70)
print("TASK 2: SAE FEATURE EXTRACTION")
print("="*70)

# First, check hook dimensions
print("\nChecking hook dimensions...")
test_tokens = model.to_tokens("test")
_, test_cache = model.run_with_cache(test_tokens)

print(f"blocks.11.hook_mlp_out shape: {test_cache['blocks.11.hook_mlp_out'].shape}")
print(f"blocks.11.attn.hook_z shape: {test_cache['blocks.11.attn.hook_z'].shape}")

# Use the correct hook based on SAE expectations
# The SAE from "gpt2-small-hook-z-kk" expects attention outputs (768 dim)
print("\nLoading pretrained SAE for attention outputs...")
release = "gpt2-small-hook-z-kk"
sae_id = "blocks.11.hook_z"

try:
    sae = SAE.from_pretrained(release, sae_id)
except:
    sae = SAE.from_pretrained(release, sae_id)[0]

sae.to(device)
sae.eval()

print(f"SAE loaded: {sae_id}")
print(f"SAE feature dimension: {sae.cfg.d_sae}")
print(f"SAE expects input dimension: {sae.cfg.d_in}")

# Use the correct hook name that matches SAE input dimension
hook_name = "blocks.11.attn.hook_z"
print(f"Using hook: {hook_name}")

# Verify dimensions match
test_hook_acts = test_cache[hook_name]
print(f"Hook output shape: {test_hook_acts.shape}")
print(f"Expected by SAE: {sae.cfg.d_in}")

# Extract features from training set
print(f"\nExtracting features from training set...")
train_features = []
train_labels = []
train_feature_details = []

with torch.no_grad():
    for idx in tqdm(range(len(train_dataset)), desc="Training Set"):
        sentence = train_dataset[idx]["sentence"]
        label = train_dataset[idx]["label"]

        tokens = model.to_tokens(sentence)
        _, cache = model.run_with_cache(tokens)

        # Use attention output hook
        hook_acts = cache[hook_name]
        sae_feature_acts = sae.encode(hook_acts)

        pooled_features = sae_feature_acts.mean(dim=1).squeeze()
        binary_features = (pooled_features > 0).float()

        train_features.append(binary_features.cpu().numpy())
        train_labels.append(label)

        active_features = torch.where(pooled_features > 0)[0]
        active_values = pooled_features[active_features]

        train_feature_details.append({
            'label': label,
            'sentence': sentence,
            'active_features': active_features.cpu().numpy(),
            'active_values': active_values.cpu().numpy(),
            'num_active': len(active_features)
        })

# Extract features from validation set
print("Extracting features from validation set...")
val_features = []
val_labels = []
val_feature_details = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Validation Set"):
        sentence = val_dataset[idx]["sentence"]
        label = val_dataset[idx]["label"]

        tokens = model.to_tokens(sentence)
        _, cache = model.run_with_cache(tokens)

        hook_acts = cache[hook_name]
        sae_feature_acts = sae.encode(hook_acts)

        pooled_features = sae_feature_acts.mean(dim=1).squeeze()
        binary_features = (pooled_features > 0).float()

        val_features.append(binary_features.cpu().numpy())
        val_labels.append(label)

        active_features = torch.where(pooled_features > 0)[0]
        active_values = pooled_features[active_features]

        val_feature_details.append({
            'label': label,
            'sentence': sentence,
            'active_features': active_features.cpu().numpy(),
            'active_values': active_values.cpu().numpy(),
            'num_active': len(active_features)
        })

X_train = np.array(train_features)
y_train = np.array(train_labels)
X_val = np.array(val_features)
y_val = np.array(val_labels)

print(f"\nFeature extraction complete!")
print(f"Training features shape: {X_train.shape}")
print(f"Validation features shape: {X_val.shape}")

# Save features
np.save('/kaggle/working/results/X_train.npy', X_train)
np.save('/kaggle/working/results/y_train.npy', y_train)
np.save('/kaggle/working/results/X_val.npy', X_val)
np.save('/kaggle/working/results/y_val.npy', y_val)

print("✓ Features saved")

# Train classifier
print("\nTraining classifier on SAE features...")
clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
clf.fit(X_train, y_train)

joblib.dump(clf, '/kaggle/working/results/sentiment_classifier.pkl')
print("✓ Classifier saved")

sae_predictions = clf.predict(X_val)

sae_acc = accuracy_score(y_val, sae_predictions)
sae_p, sae_r, sae_f1, _ = precision_recall_fscore_support(
    y_val, sae_predictions, average='binary'
)
sae_cm = confusion_matrix(y_val, sae_predictions)

print("\n### SAE-BASED RESULTS ###")
print(f"Accuracy: {sae_acc:.4f} ({sae_acc*100:.2f}%)")
print(f"Precision: {sae_p:.4f}")
print(f"Recall: {sae_r:.4f}")
print(f"F1-Score: {sae_f1:.4f}")
print(f"\nConfusion Matrix:")
print(f"              Neg    Pos")
print(f"Actual Neg  [{sae_cm[0,0]:5d}  {sae_cm[0,1]:5d}]")
print(f"       Pos  [{sae_cm[1,0]:5d}  {sae_cm[1,1]:5d}]")

sae_results = {
    'accuracy': float(sae_acc),
    'precision': float(sae_p),
    'recall': float(sae_r),
    'f1_score': float(sae_f1),
    'confusion_matrix': sae_cm.tolist()
}

with open('/kaggle/working/results/sae_results.json', 'w') as f:
    json.dump(sae_results, f, indent=2)

print("✓ SAE results saved")

# ============================================================================
# TASK 3: Comprehensive Feature Analysis
# ============================================================================
print("\n" + "="*70)
print("TASK 3: COMPREHENSIVE FEATURE ANALYSIS")
print("="*70)

pos_all_features = []
neg_all_features = []

for detail in train_feature_details:
    if detail['label'] == 1:
        pos_all_features.extend(detail['active_features'])
    else:
        neg_all_features.extend(detail['active_features'])

pos_feature_counts = Counter(pos_all_features)
neg_feature_counts = Counter(neg_all_features)

pos_unique_features = set(pos_all_features)
neg_unique_features = set(neg_all_features)

common_features = pos_unique_features & neg_unique_features
pos_only_features = pos_unique_features - neg_unique_features
neg_only_features = neg_unique_features - pos_unique_features

jaccard_similarity = len(common_features) / len(pos_unique_features | neg_unique_features)

total_pos_samples = sum(y_train == 1)
total_neg_samples = sum(y_train == 0)

pos_uncommon = {feat: count for feat, count in pos_feature_counts.items()
                if count / total_pos_samples < 0.05}
neg_uncommon = {feat: count for feat, count in neg_feature_counts.items()
                if count / total_neg_samples < 0.05}

pos_very_rare = {feat: count for feat, count in pos_feature_counts.items()
                 if count / total_pos_samples < 0.01}
neg_very_rare = {feat: count for feat, count in neg_feature_counts.items()
                 if count / total_neg_samples < 0.01}

print("\n" + "="*70)
print("A. OVERALL FEATURE STATISTICS")
print("="*70)
print(f"Total samples - POSITIVE: {total_pos_samples}, NEGATIVE: {total_neg_samples}")
print(f"\nUnique features:")
print(f"  POSITIVE: {len(pos_unique_features)}")
print(f"  NEGATIVE: {len(neg_unique_features)}")
print(f"  Common: {len(common_features)}")
print(f"  POSITIVE-only: {len(pos_only_features)}")
print(f"  NEGATIVE-only: {len(neg_only_features)}")
print(f"\nJaccard Similarity: {jaccard_similarity:.4f}")

if jaccard_similarity < 0.5:
    print("→ DIFFERENT feature sets (strong separation)")
else:
    print("→ SIMILAR feature sets (overlapping representations)")

print("\n" + "="*70)
print("B. UNCOMMON FEATURES (<5%)")
print("="*70)
print(f"Uncommon POSITIVE: {len(pos_uncommon)} ({len(pos_uncommon)/len(pos_unique_features)*100:.1f}%)")
print(f"Uncommon NEGATIVE: {len(neg_uncommon)} ({len(neg_uncommon)/len(neg_unique_features)*100:.1f}%)")
print(f"Very rare (<1%) - POS: {len(pos_very_rare)}, NEG: {len(neg_very_rare)}")

print("\n### Top 10 Uncommon Important POSITIVE Features ###")
if pos_uncommon:
    for feat_idx, count in sorted(pos_uncommon.items(),
                                  key=lambda x: abs(clf.coef_[0][x[0]]),
                                  reverse=True)[:10]:
        print(f"  Feature {feat_idx:5d}: {count:4d} times ({count/total_pos_samples*100:.2f}%) | "
              f"Weight: {clf.coef_[0][feat_idx]:+.4f}")

print("\n### Top 10 Uncommon Important NEGATIVE Features ###")
if neg_uncommon:
    for feat_idx, count in sorted(neg_uncommon.items(),
                                  key=lambda x: abs(clf.coef_[0][x[0]]),
                                  reverse=True)[:10]:
        print(f"  Feature {feat_idx:5d}: {count:4d} times ({count/total_neg_samples*100:.2f}%) | "
              f"Weight: {clf.coef_[0][feat_idx]:+.4f}")

# Save analysis
feature_analysis = {
    'overall_statistics': {
        'jaccard_similarity': float(jaccard_similarity),
        'common_features': len(common_features),
        'pos_exclusive': len(pos_only_features),
        'neg_exclusive': len(neg_only_features)
    },
    'uncommon_features': {
        'pos_uncommon': len(pos_uncommon),
        'neg_uncommon': len(neg_uncommon)
    }
}

with open('/kaggle/working/results/feature_analysis.json', 'w') as f:
    json.dump(feature_analysis, f, indent=2)

# Final summary
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)
print(f"Baseline: Accuracy={baseline_acc*100:.2f}%, F1={baseline_f1:.4f}")
print(f"SAE-Based: Accuracy={sae_acc*100:.2f}%, F1={sae_f1:.4f}")
print(f"Improvement: {(sae_acc-baseline_acc)*100:+.2f}%")
print(f"Feature overlap (Jaccard): {jaccard_similarity:.4f}")
print("="*70)
print("✓ Complete!")