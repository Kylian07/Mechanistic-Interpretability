# -*- coding: utf-8 -*-
"""Pretrained GPT2_SAE_SAElens

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/pretrained-gpt2-sae-saelens.23486761-8142-456d-b73b-2f042327f5bb.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251204/auto/storage/goog4_request%26X-Goog-Date%3D20251204T214846Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5e9b89da980671b45242c9e4de2d8e460967ef17f06cde1dab166bb1c98e7d921a9875e9ccd237602afbaf52aa26835bac232c688ed9931b5f728cfb073ef05439b26acbabc5a792018356b849a0e0f0fc063c95f27bc7821cfaa9c75b4e602681560e5cab83c069f740ab36861965933f99716c0944a3307503b9d2b8f1f43496c7759aadd0a5154c9bddcd31b605e358cd4b72dbe235957f78a8ba361965d73ccb9ebd01af475e03ac4d81a04a9b9624134a72e3c202899d1960769a25053b0cfe7d707a7219e0ccbc70a10659f39ba71dc7fad96ac0d591aa37f66f1668bb2fba4051077c8c5f5e84c3819046a7355f2b16a1f9f0e73baf487cd6c43c4883
"""

!pip install sae-lens

import torch
from datasets import load_dataset
from transformer_lens import HookedTransformer
from sae_lens import SAE
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import numpy as np
from collections import Counter
import os

# Kaggle directories
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.makedirs('/kaggle/working/results', exist_ok=True)

os.environ['HF_HOME'] = '/kaggle/working/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load GPT-2 Small
print("Loading GPT-2 Small model...")
model = HookedTransformer.from_pretrained("gpt2-small", device=device)
model.eval()

print(f"Model has {model.cfg.n_layers} layers (0-{model.cfg.n_layers-1})")

# Load SST-2 dataset
print("Loading SST-2 dataset...")
dataset = load_dataset("glue", "sst2")
train_dataset = dataset["train"]
val_dataset = dataset["validation"]

print(f"Train samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")

# ============================================================================
# STEP 1: Baseline - Zero-Shot Prompting
# ============================================================================
print("\n" + "="*70)
print("STEP 1: BASELINE - ZERO-SHOT PROMPTING")
print("="*70)

baseline_predictions = []
baseline_labels = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Zero-Shot Evaluation"):
        sentence = val_dataset[idx]["sentence"]
        true_label = val_dataset[idx]["label"]
        
        prompt = f"""Classify the sentiment as positive or negative.

Sentence: {sentence}
Sentiment:"""
        
        tokens = model.to_tokens(prompt)
        logits = model(tokens)
        
        next_token_logits = logits[0, -1, :]
        next_token_id = torch.argmax(next_token_logits).item()
        generated_text = model.tokenizer.decode([next_token_id]).strip().lower()
        
        if "positive" in generated_text or generated_text.startswith("pos"):
            prediction = 1
        elif "negative" in generated_text or generated_text.startswith("neg"):
            prediction = 0
        else:
            pos_token_id = model.tokenizer.encode(" positive")[0]
            neg_token_id = model.tokenizer.encode(" negative")[0]
            
            if next_token_logits[pos_token_id] > next_token_logits[neg_token_id]:
                prediction = 1
            else:
                prediction = 0
        
        baseline_predictions.append(prediction)
        baseline_labels.append(true_label)

baseline_predictions = np.array(baseline_predictions)
baseline_labels = np.array(baseline_labels)

baseline_acc = accuracy_score(baseline_labels, baseline_predictions)
baseline_p, baseline_r, baseline_f1, _ = precision_recall_fscore_support(
    baseline_labels, baseline_predictions, average='binary'
)
baseline_cm = confusion_matrix(baseline_labels, baseline_predictions)

print(f"\nBaseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"F1-Score: {baseline_f1:.4f}")

# Save baseline
with open('/kaggle/working/results/baseline_results.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("BASELINE - ZERO-SHOT PROMPTING\n")
    f.write("="*70 + "\n\n")
    f.write(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\n")
    f.write(f"Precision: {baseline_p:.4f}\n")
    f.write(f"Recall: {baseline_r:.4f}\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n")

print("✓ Baseline saved")

# ============================================================================
# STEP 2: Layer-wise Performance Using RAW Representations
# ============================================================================
print("\n" + "="*70)
print("STEP 2: LAYER-WISE PERFORMANCE (RAW REPRESENTATIONS)")
print("="*70)

layer_performance = {}

for layer_num in range(model.cfg.n_layers):
    print(f"\n{'='*70}")
    print(f"LAYER {layer_num} - RAW REPRESENTATION CLASSIFIER")
    print(f"{'='*70}")
    
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    
    # Extract RAW representations from training set
    print("Extracting training representations...")
    train_reps = []
    train_labels = []
    
    with torch.no_grad():
        for idx in tqdm(range(len(train_dataset)), desc=f"Train L{layer_num}"):
            sentence = train_dataset[idx]["sentence"]
            label = train_dataset[idx]["label"]
            
            tokens = model.to_tokens(sentence)
            _, cache = model.run_with_cache(tokens)
            
            # Get RAW layer representation
            layer_acts = cache[hook_name]
            
            # Debug: print shape for first iteration
            if idx == 0:
                print(f"DEBUG: Raw layer_acts shape: {layer_acts.shape}")
            
            # Pool across sequence dimension (mean pooling)
            # layer_acts shape should be [batch, seq_len, d_model]
            pooled = layer_acts.mean(dim=1)  # Shape: [batch, d_model]
            
            if idx == 0:
                print(f"DEBUG: After mean(dim=1) shape: {pooled.shape}")
            
            # Convert to 1D numpy array
            pooled_np = pooled.cpu().numpy()
            
            # Flatten to ensure 1D
            pooled_flat = pooled_np.flatten()
            
            if idx == 0:
                print(f"DEBUG: Final flattened shape: {pooled_flat.shape}")
            
            train_reps.append(pooled_flat)
            train_labels.append(label)
    
    X_train = np.array(train_reps)
    y_train = np.array(train_labels)
    
    print(f"Training representations shape: {X_train.shape}")
    print(f"Training labels shape: {y_train.shape}")
    
    # Extract RAW representations from validation set
    print("Extracting validation representations...")
    val_reps = []
    val_labels = []
    
    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"Val L{layer_num}"):
            sentence = val_dataset[idx]["sentence"]
            label = val_dataset[idx]["label"]
            
            tokens = model.to_tokens(sentence)
            _, cache = model.run_with_cache(tokens)
            
            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_np = pooled.cpu().numpy()
            pooled_flat = pooled_np.flatten()
            
            val_reps.append(pooled_flat)
            val_labels.append(label)
    
    X_val = np.array(val_reps)
    y_val = np.array(val_labels)
    
    print(f"Validation representations shape: {X_val.shape}")
    print(f"Validation labels shape: {y_val.shape}")
    
    # Verify shapes
    print(f"X_train dimensions: {X_train.ndim}")
    print(f"X_val dimensions: {X_val.ndim}")
    
    if X_train.ndim != 2:
        print(f"ERROR: X_train has {X_train.ndim} dimensions, reshaping...")
        X_train = X_train.reshape(X_train.shape[0], -1)
        print(f"New X_train shape: {X_train.shape}")
    
    if X_val.ndim != 2:
        print(f"ERROR: X_val has {X_val.ndim} dimensions, reshaping...")
        X_val = X_val.reshape(X_val.shape[0], -1)
        print(f"New X_val shape: {X_val.shape}")
    
    # Train classifier on RAW representations
    print("Training classifier...")
    clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
    clf.fit(X_train, y_train)
    
    # Evaluate
    predictions = clf.predict(X_val)
    
    acc = accuracy_score(y_val, predictions)
    p, r, f1, _ = precision_recall_fscore_support(y_val, predictions, average='binary')
    cm = confusion_matrix(y_val, predictions)
    
    print(f"\nLayer {layer_num} Performance:")
    print(f"Accuracy: {acc:.4f} ({acc*100:.2f}%)")
    print(f"F1-Score: {f1:.4f}")
    print(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%")
    
    layer_performance[layer_num] = {
        'accuracy': acc,
        'precision': p,
        'recall': r,
        'f1_score': f1,
        'improvement': acc - baseline_acc,
        'confusion_matrix': cm
    }
    
    # Save layer performance
    with open(f'/kaggle/working/results/layer_{layer_num}_performance.txt', 'w') as f:
        f.write("="*70 + "\n")
        f.write(f"LAYER {layer_num} - PERFORMANCE (RAW REPRESENTATIONS)\n")
        f.write("="*70 + "\n\n")
        f.write(f"Method: Logistic Regression on raw layer activations\n")
        f.write(f"Representation dimension: {X_train.shape[1]}\n\n")
        f.write(f"Accuracy: {acc:.4f} ({acc*100:.2f}%)\n")
        f.write(f"Precision: {p:.4f}\n")
        f.write(f"Recall: {r:.4f}\n")
        f.write(f"F1-Score: {f1:.4f}\n\n")
        f.write(f"Baseline Accuracy: {baseline_acc:.4f}\n")
        f.write(f"Improvement: {(acc - baseline_acc)*100:+.2f}%\n\n")
        f.write("Confusion Matrix:\n")
        f.write("              Predicted\n")
        f.write("              Neg    Pos\n")
        f.write(f"Actual Neg  [{cm[0,0]:5d}  {cm[0,1]:5d}]\n")
        f.write(f"       Pos  [{cm[1,0]:5d}  {cm[1,1]:5d}]\n")
    
    print(f"✓ Layer {layer_num} performance saved")

# ============================================================================
# STEP 3: SAE Feature Analysis (Interpretability Only)
# ============================================================================
print("\n" + "="*70)
print("STEP 3: SAE FEATURE ANALYSIS (INTERPRETABILITY)")
print("="*70)

sae_analysis = {}

for layer_num in range(model.cfg.n_layers):
    print(f"\n{'='*70}")
    print(f"ANALYZING SAE FEATURES - LAYER {layer_num}")
    print(f"{'='*70}")
    
    # Load pretrained SAE
    try:
        release = "gpt2-small-hook-z-kk"
        sae_id = f"blocks.{layer_num}.hook_z"
        
        try:
            sae = SAE.from_pretrained(release, sae_id)
        except:
            sae = SAE.from_pretrained(release, sae_id)[0]
        
        sae.to(device)
        sae.eval()
        
        hook_name = f"blocks.{layer_num}.attn.hook_z"
        print(f"Using SAE: {sae_id}")
        
    except Exception as e:
        print(f"Could not load SAE for layer {layer_num}: {e}")
        continue
    
    # Extract SAE features from validation set
    print("Extracting SAE features for analysis...")
    val_feature_details = []
    
    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"SAE L{layer_num}"):
            sentence = val_dataset[idx]["sentence"]
            label = val_dataset[idx]["label"]
            
            tokens = model.to_tokens(sentence)
            _, cache = model.run_with_cache(tokens)
            
            hook_acts = cache[hook_name]
            sae_feature_acts = sae.encode(hook_acts)
            
            # Pool across sequence
            pooled_features = sae_feature_acts.mean(dim=1)
            pooled_features = pooled_features.cpu().numpy().flatten()
            
            active_indices = np.where(pooled_features > 0)[0]
            active_values = pooled_features[active_indices]
            
            val_feature_details.append({
                'true_label': label,
                'active_features': active_indices,
                'active_values': active_values,
                'num_active': len(active_indices)
            })
    
    # Analyze features
    pos_features = []
    neg_features = []
    
    for detail in val_feature_details:
        if detail['true_label'] == 1:
            pos_features.extend(detail['active_features'])
        else:
            neg_features.extend(detail['active_features'])
    
    pos_counts = Counter(pos_features)
    neg_counts = Counter(neg_features)
    
    pos_unique = set(pos_features)
    neg_unique = set(neg_features)
    common = pos_unique & neg_unique
    pos_only = pos_unique - neg_unique
    neg_only = neg_unique - pos_unique
    
    jaccard = len(common) / len(pos_unique | neg_unique) if (pos_unique | neg_unique) else 0
    
    total_pos = sum(d['true_label'] == 1 for d in val_feature_details)
    total_neg = sum(d['true_label'] == 0 for d in val_feature_details)
    
    # Uncommon features
    pos_uncommon = {f: c for f, c in pos_counts.items() if c / total_pos < 0.05}
    neg_uncommon = {f: c for f, c in neg_counts.items() if c / total_neg < 0.05}
    
    # Average active
    pos_avg = np.mean([d['num_active'] for d in val_feature_details if d['true_label'] == 1])
    neg_avg = np.mean([d['num_active'] for d in val_feature_details if d['true_label'] == 0])
    
    # Determine similarity
    if jaccard < 0.3:
        similarity = "VERY DIFFERENT - Strong separation"
    elif jaccard < 0.5:
        similarity = "MODERATELY DIFFERENT"
    elif jaccard < 0.7:
        similarity = "MODERATELY SIMILAR"
    else:
        similarity = "VERY SIMILAR"
    
    # Common discriminative
    common_discriminative = []
    for feat_idx in common:
        pos_freq = pos_counts[feat_idx] / total_pos
        neg_freq = neg_counts[feat_idx] / total_neg
        diff = abs(pos_freq - neg_freq)
        common_discriminative.append((feat_idx, diff, pos_freq, neg_freq))
    
    common_discriminative.sort(key=lambda x: x[1], reverse=True)
    
    # Save SAE analysis
    with open(f'/kaggle/working/results/layer_{layer_num}_sae_features.txt', 'w') as f:
        f.write("="*70 + "\n")
        f.write(f"LAYER {layer_num} - SAE FEATURE ANALYSIS\n")
        f.write("="*70 + "\n\n")
        
        f.write("A. FEATURE STATISTICS\n")
        f.write("-"*70 + "\n")
        f.write(f"Total samples - POSITIVE: {total_pos}, NEGATIVE: {total_neg}\n")
        f.write(f"Unique POSITIVE features: {len(pos_unique)}\n")
        f.write(f"Unique NEGATIVE features: {len(neg_unique)}\n")
        f.write(f"Common features: {len(common)}\n")
        f.write(f"POSITIVE-only: {len(pos_only)}\n")
        f.write(f"NEGATIVE-only: {len(neg_only)}\n")
        f.write(f"Jaccard similarity: {jaccard:.4f}\n")
        f.write(f"→ {similarity}\n\n")
        
        f.write("B. UNCOMMON FEATURES\n")
        f.write("-"*70 + "\n")
        f.write(f"Uncommon POSITIVE (<5%): {len(pos_uncommon)}\n")
        f.write(f"Uncommon NEGATIVE (<5%): {len(neg_uncommon)}\n\n")
        
        f.write("C. ACTIVATION PATTERNS\n")
        f.write("-"*70 + "\n")
        f.write(f"Average active - POSITIVE: {pos_avg:.2f}, NEGATIVE: {neg_avg:.2f}\n\n")
        
        f.write("D. TOP 5 POSITIVE FEATURES\n")
        f.write("-"*70 + "\n")
        for rank, (feat, count) in enumerate(pos_counts.most_common(5), 1):
            freq = count / total_pos * 100
            status = "COMMON" if feat in common else "EXCLUSIVE"
            f.write(f"{rank}. Feature {feat}: {count} times ({freq:.2f}%) | {status}\n")
        
        f.write("\nE. TOP 5 NEGATIVE FEATURES\n")
        f.write("-"*70 + "\n")
        for rank, (feat, count) in enumerate(neg_counts.most_common(5), 1):
            freq = count / total_neg * 100
            status = "COMMON" if feat in common else "EXCLUSIVE"
            f.write(f"{rank}. Feature {feat}: {count} times ({freq:.2f}%) | {status}\n")
        
        f.write("\nF. TOP 5 DISCRIMINATIVE COMMON FEATURES\n")
        f.write("-"*70 + "\n")
        for rank, (feat, diff, pos_freq, neg_freq) in enumerate(common_discriminative[:5], 1):
            bias = "POSITIVE" if pos_freq > neg_freq else "NEGATIVE"
            f.write(f"{rank}. Feature {feat}: {bias}-biased | "
                   f"POS:{pos_freq*100:.2f}%, NEG:{neg_freq*100:.2f}%\n")
    
    sae_analysis[layer_num] = {
        'jaccard': jaccard,
        'common': len(common),
        'pos_unique': len(pos_unique),
        'neg_unique': len(neg_unique),
        'similarity': similarity
    }
    
    print(f"✓ Layer {layer_num} SAE analysis saved")

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

best_layer = max(layer_performance.items(), key=lambda x: x[1]['accuracy'])
worst_layer = min(layer_performance.items(), key=lambda x: x[1]['accuracy'])

with open('/kaggle/working/results/final_summary.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("FINAL SUMMARY - LAYER-WISE PERFORMANCE\n")
    f.write("="*70 + "\n\n")
    
    f.write("1. BASELINE (Zero-Shot Prompting)\n")
    f.write("-"*70 + "\n")
    f.write(f"Accuracy: {baseline_acc*100:.2f}%\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n\n")
    
    f.write("2. LAYER-WISE PERFORMANCE (Raw Representations)\n")
    f.write("-"*70 + "\n")
    f.write(f"{'Layer':<8} {'Accuracy':<12} {'F1-Score':<12} {'Improvement':<15} {'Jaccard':<12}\n")
    f.write("-"*70 + "\n")
    
    for layer_num in sorted(layer_performance.keys()):
        perf = layer_performance[layer_num]
        sae_info = sae_analysis.get(layer_num, {})
        jaccard_val = sae_info.get('jaccard', 0)
        f.write(f"Layer {layer_num:<2}  {perf['accuracy']*100:5.2f}%      "
               f"{perf['f1_score']:.4f}      {perf['improvement']*100:+6.2f}%        "
               f"{jaccard_val:.4f}\n")
    
    f.write("\n3. KEY FINDINGS\n")
    f.write("-"*70 + "\n")
    f.write(f"Best Layer: {best_layer[0]}\n")
    f.write(f"  Accuracy: {best_layer[1]['accuracy']*100:.2f}%\n")
    f.write(f"  → This layer encodes the most sentiment information\n\n")
    
    f.write(f"Worst Layer: {worst_layer[0]}\n")
    f.write(f"  Accuracy: {worst_layer[1]['accuracy']*100:.2f}%\n\n")
    
    f.write("INTERPRETATION:\n")
    f.write("- Higher accuracy = more sentiment information in layer\n")
    f.write("- Lower Jaccard = more distinct positive/negative features\n")

print(f"\n✓ Best layer: Layer {best_layer[0]} ({best_layer[1]['accuracy']*100:.2f}% accuracy)")
print("✓ Complete!")
print("="*70)

