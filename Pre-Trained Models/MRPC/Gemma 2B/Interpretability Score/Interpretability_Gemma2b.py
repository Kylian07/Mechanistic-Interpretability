# -*- coding: utf-8 -*-
"""notebook4e4074e925

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/notebook4e4074e925.d5c0d19d-cc3c-4c39-aab0-a4f5204a0fee.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251231/auto/storage/goog4_request%26X-Goog-Date%3D20251231T052833Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5473020dc8d98a8ec984632ccd3e215b9ef16bc8096b9b40acbf21165638e184b796846ce9b79c9b99684ea969250251ed0573214bf434c833e58d335848d3b106e1d8371d4710125a28046b4232579d420bc4d7891300e05fa6d567d1d3d47dccb97e99c858b6ca0c8d7b75f62110e54f7f0aa5490d7b42bf107993fb84f7fb892b74630a76e67edf9d154a7f32939409f81578ff030347ea7f7e4a0901a235c371d24cf26a5049d167d41a3b731bb249bb808838b2739fb33b32a5fd458781809f385e1561b23d299a9e6b248d84e61015dc4672298c90de54e91938c21e0300b4ad5513eddc03179527273de7f9cf456053d61732e86e4e7b7d46d722eb7c
"""

# -*- coding: utf-8 -*-

# ======================================================================
# 0. CLEAR EXISTING MEMORY FIRST
# ======================================================================
import torch
import gc

# Clear any existing CUDA memory
torch.cuda.empty_cache()
gc.collect()

# Check current memory usage
if torch.cuda.is_available():
    print(f"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
    print(f"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB")

    # Force reset if memory already in use
    if torch.cuda.memory_allocated(0) > 1e9:  # More than 1GB
        print("⚠️ WARNING: Significant GPU memory already in use!")
        print("Please RESTART THE KERNEL before running this notebook.")
        print("Go to: Kernel → Restart & Clear Output")
        raise RuntimeError("Kernel restart required to free GPU memory")

!pip install -q sae-lens groq transformer-lens

# ======================================================================
# 1. HF AUTHENTICATION
# ======================================================================
from huggingface_hub import login

try:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    hf_token = user_secrets.get_secret("HF_TOKEN")
    login(token=hf_token)
    print("✓ Logged in to Hugging Face")
except Exception as e:
    print(f"Secrets error: {e}")
    login()

# ======================================================================
# 2. IMPORTS
# ======================================================================
import re
import numpy as np
from datasets import load_dataset
from transformer_lens import HookedTransformer
from sae_lens import SAE
from groq import Groq
from scipy.stats import pearsonr
from tqdm import tqdm
from collections import Counter
import os

os.makedirs('/kaggle/working/results_interpretability', exist_ok=True)
device = "cuda" if torch.cuda.is_available() else "cpu"

# ======================================================================
# 3. GROQ SETUP
# ======================================================================
user_secrets = UserSecretsClient()
os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")
client = Groq(api_key=os.environ["GROQ_API_KEY"])

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful interpretability assistant."},
            {"role": "user", "content": prompt},
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# ======================================================================
# 4. LOAD MODEL WITH MINIMAL MEMORY
# ======================================================================
print("Loading Gemma-2B (FP16, CPU offload)...")

# Load with aggressive memory settings
model = HookedTransformer.from_pretrained(
    "google/gemma-2b",
    device="cpu",  # Load to CPU first
    dtype=torch.float16,
    fold_ln=False,
    center_writing_weights=False,
    center_unembed=False,
)

# Move to GPU after loading
print("Moving model to GPU...")
model.to(device)
model.eval()

# Clear any leftover memory
gc.collect()
torch.cuda.empty_cache()

print(f"✓ Model loaded. GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")

# ======================================================================
# 5. LOAD DATASET
# ======================================================================
print("Loading MRPC validation dataset...")
dataset = load_dataset("glue", "mrpc", split="validation")
print(f"Validation samples: {len(dataset)}")

def combine_sentences(example):
    return f"{example['sentence1']} <|endoftext|> {example['sentence2']}"

val_combined = [combine_sentences(ex) for ex in dataset]

# ======================================================================
# 6. INTERPRETABILITY FUNCTIONS (ONE AT A TIME)
# ======================================================================
def top_k_global_features(layer_num, sae_model, k=5, max_examples=None):
    """Find top-k most frequent features across ALL validation examples."""
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    counter = Counter()

    num_examples = len(val_combined) if max_examples is None else min(max_examples, len(val_combined))

    for i in tqdm(range(num_examples), desc=f"Counting features L{layer_num}"):
        with torch.no_grad():
            tokens = model.to_tokens(val_combined[i], prepend_bos=True)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]
            sae_acts = sae_model.encode(hook_acts)

            # Find active features (any token > 0)
            active = (sae_acts > 0).any(dim=1).squeeze()
            active_ids = active.nonzero(as_tuple=True)[0].cpu().tolist()
            counter.update(active_ids)

            del tokens, cache, hook_acts, sae_acts, active
            torch.cuda.empty_cache()

    topk = counter.most_common(k)
    print(f"Layer {layer_num} top-{k} features (idx, count): {topk}")
    return [idx for idx, _ in topk]

def extract_acts(layer_num, sae_model, feat_idx, max_examples=None):
    """Extract per-token activations for ONE feature across ALL validation examples."""
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    acts_all, texts_all = [], []

    num_examples = len(val_combined) if max_examples is None else min(max_examples, len(val_combined))

    for i in tqdm(range(num_examples), desc=f"Extracting L{layer_num} F{feat_idx}"):
        with torch.no_grad():
            tokens = model.to_tokens(val_combined[i], prepend_bos=True)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]
            sae_acts = sae_model.encode(hook_acts)

            # Get this feature's per-token activations
            feat_act = sae_acts[0, :, feat_idx].cpu().numpy()  # (seq_len,)
            acts_all.append(feat_act)
            texts_all.append(val_combined[i])

            del tokens, cache, hook_acts, sae_acts, feat_act
            torch.cuda.empty_cache()

    return acts_all, texts_all

def select_top(acts, texts, n=20):
    """Select top-n examples by total activation."""
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:n]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

def llama_interpretation(top_texts, top_acts):
    """Generate interpretation using LLaMA-3.1 via Groq."""
    prompt = (
        "You are analyzing a sparse autoencoder feature from Gemma-2B on MRPC paraphrase detection.\n"
        "Each text below is a concatenated sentence pair (separated by <|endoftext|>). "
        "Activations show per-token strength for this feature.\n\n"
        "From this data, give one concise sentence describing what pattern or concept "
        "this feature responds to (e.g., semantic similarity, negation, specific word types).\n\n"
    )
    for i, (txt, acts) in enumerate(zip(top_texts, top_acts), 1):
        # Show mean activation to keep prompt concise
        mean_act = np.mean(acts)
        prompt += f"{i}. \"{txt[:80]}...\"\n   Mean activation: {mean_act:.3f}\n\n"
    prompt += "Your one-sentence explanation:"
    return chat_with_llama(prompt, max_tokens=100)

def llama_activation_score(sentence, interpretation):
    """Predict activation score 0-10 using LLaMA-3.1."""
    prompt = (
        f'Feature interpretation:\n"{interpretation}"\n\n'
        "On a scale from 0 (not active) to 10 (very active), estimate how strongly "
        "this feature activates on the following sentence pair. Respond with only a single number.\n\n"
        f'Sentence pair: \"{sentence[:80]}...\"\nActivation score:'
    )
    resp = chat_with_llama(prompt, max_tokens=16)
    m = re.search(r"\d+(\.\d+)?", resp)
    return float(m.group()) if m else 0.0

def pearson_score(actual_acts, pred_scores):
    """Calculate Pearson correlation."""
    actual = np.array([np.mean(a) for a in actual_acts]) * 10.0  # Scale to 0-10
    pred = np.array(pred_scores)
    if len(actual) < 2:
        return 0.0
    corr, _ = pearsonr(actual, pred)
    return corr

def save_interp_report(path, layer_num, feat_idx, interpretation,
                       interp_texts, interp_acts, eval_texts, eval_acts, pred_scores, corr):
    """Save detailed interpretation results to file."""
    with open(path, "a", encoding="utf-8") as f:
        f.write(f"Layer {layer_num}, Feature {feat_idx}\n")
        f.write("-"*70 + "\n")
        f.write("Interpretation:\n" + interpretation.strip() + "\n\n")

        f.write("Top examples used for interpretation:\n")
        for i, (txt, acts) in enumerate(zip(interp_texts, interp_acts), 1):
            mean_act = np.mean(acts) * 10.0
            f.write(f"{i}. {txt[:80]}...\n   Activation: {mean_act:.3f}\n")

        f.write("\nEvaluation examples:\n")
        for i, (txt, acts, pred) in enumerate(zip(eval_texts, eval_acts, pred_scores), 1):
            actual = np.mean(acts) * 10.0
            f.write(f"{i}. {txt[:80]}...\n   Actual={actual:.3f}, Pred={pred:.3f}\n")

        f.write(f"\nPearson correlation: {corr:.4f}\n")
        f.write("="*70 + "\n\n")

def interpret_feature(layer_num, sae_model, feat_idx, report_path, max_examples=None):
    """Full interpretability pipeline for one feature."""

    # Extract activations for all examples
    acts_all, texts_all = extract_acts(layer_num, sae_model, feat_idx, max_examples)

    # Select top-20 by activation
    top_acts, top_texts = select_top(acts_all, texts_all, n=20)

    # Use top-5 for interpretation
    interp_acts, interp_texts = top_acts[:5], top_texts[:5]

    print(f"\nTop 5 examples for L{layer_num} Feature {feat_idx}:")
    for i, s in enumerate(interp_texts, 1):
        print(f"{i}. {s[:70]}...")

    # Generate interpretation
    interpretation = llama_interpretation(interp_texts, interp_acts)
    print(f"\nInterpretation: {interpretation}")

    # Evaluate on held-out examples (next 5 from top-20)
    used = set(interp_texts)
    rest = [(a, t) for a, t in zip(acts_all, texts_all) if t not in used]

    if len(rest) < 5:
        print("Not enough held-out examples for evaluation; skipping.")
        del acts_all, texts_all
        gc.collect()
        return None

    rest_acts, rest_texts = zip(*rest)
    rest_acts, rest_texts = list(rest_acts), list(rest_texts)
    scores = [np.sum(a) for a in rest_acts]
    idxs = np.argsort(scores)[::-1][:5]  # Next top-5
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    print("\nEvaluation examples:")
    for i, s in enumerate(eval_texts, 1):
        print(f"{i}. {s[:70]}...")

    # Predict and evaluate
    pred_scores = [llama_activation_score(s, interpretation) for s in eval_texts]
    corr = pearson_score(eval_acts, pred_scores)
    print(f"Pearson correlation: {corr:.4f}")

    # Save results
    save_interp_report(report_path, layer_num, feat_idx, interpretation,
                       interp_texts, interp_acts, eval_texts, eval_acts, pred_scores, corr)

    # Clean up
    del acts_all, texts_all, rest_acts, rest_texts
    gc.collect()
    torch.cuda.empty_cache()

    return corr

# ======================================================================
# 7. MAIN: LAYERS 10 & 11
# ======================================================================
print("\n" + "="*70)
print("GROQ INTERPRETABILITY FOR LAYERS 10 & 11")
print("="*70)

release = "gemma-scope-2b-pt-att-canonical"
LAYERS = [10, 11]
TOP_K = 5  # Top 5 features per layer
MAX_EXAMPLES = None  # Use ALL validation examples (408)

for layer_idx in LAYERS:
    print(f"\n{'='*70}")
    print(f"LAYER {layer_idx}: Top-{TOP_K} Global Features")
    print(f"{'='*70}")

    # Load SAE for this layer
    sae_id = f"layer_{layer_idx}/width_16k/canonical"
    print(f"Loading SAE: {release}, {sae_id}")

    try:
        sae_model = SAE.from_pretrained(release, sae_id)
    except:
        sae_model = SAE.from_pretrained(release, sae_id)[0]

    sae_model.to(device).eval()
    print(f"✓ Loaded SAE for layer {layer_idx}")

    # Find top-k globally frequent features
    top_feats = top_k_global_features(layer_idx, sae_model, TOP_K, MAX_EXAMPLES)

    # Create report file
    report_file = f"/kaggle/working/results_interpretability/layer{layer_idx}_top{TOP_K}_interpretability.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"INTERPRETABILITY REPORT – Gemma-2B MRPC Layer {layer_idx}\n")
        f.write(f"Top-{TOP_K} Global Features (by frequency)\n")
        f.write(f"Analyzed on {len(val_combined)} validation examples\n")
        f.write("="*70 + "\n\n")

    # Interpret each top feature
    correlations = []
    for feat_idx in top_feats:
        print(f"\n{'='*70}")
        print(f"Layer {layer_idx}, Feature {feat_idx}")
        print(f"{'='*70}")

        try:
            corr = interpret_feature(
                layer_num=layer_idx,
                sae_model=sae_model,
                feat_idx=feat_idx,  # FIXED: was feature_idx
                report_path=report_file,
                max_examples=MAX_EXAMPLES,
            )

            if corr is not None:
                correlations.append((feat_idx, corr))

        except Exception as e:
            print(f"Error processing feature {feat_idx}: {e}")
            import traceback
            traceback.print_exc()
            with open(report_file, "a", encoding="utf-8") as f:
                f.write(f"Layer {layer_idx}, Feature {feat_idx}\n")
                f.write(f"ERROR: {e}\n")
                f.write("="*70 + "\n\n")

        # Clear between features
        gc.collect()
        torch.cuda.empty_cache()

    # Summary for this layer
    print(f"\n{'='*70}")
    print(f"LAYER {layer_idx} SUMMARY")
    print(f"{'='*70}")
    print(f"Analyzed {len(correlations)} features")
    if correlations:
        avg_corr = np.mean([c for _, c in correlations])
        best_feat, best_corr = max(correlations, key=lambda x: x[1])
        print(f"Average Pearson correlation: {avg_corr:.4f}")
        print(f"Best feature: {best_feat} (corr={best_corr:.4f})")

        # Save summary to report
        with open(report_file, "a", encoding="utf-8") as f:
            f.write(f"\nSUMMARY FOR LAYER {layer_idx}\n")
            f.write("="*70 + "\n")
            f.write(f"Features analyzed: {len(correlations)}\n")
            f.write(f"Average Pearson correlation: {avg_corr:.4f}\n")
            f.write(f"Best feature: {best_feat} (correlation: {best_corr:.4f})\n")
            f.write("\nAll correlations:\n")
            for feat, corr in sorted(correlations, key=lambda x: x[1], reverse=True):
                f.write(f"  Feature {feat}: {corr:.4f}\n")

    print(f"\n✓ Layer {layer_idx} complete. Report saved to {report_file}")

    # Clean up SAE model
    del sae_model
    gc.collect()
    torch.cuda.empty_cache()

print("\n" + "="*70)
print("✓ INTERPRETABILITY ANALYSIS COMPLETE FOR LAYERS 10 & 11")
print(f"✓ Results saved to /kaggle/working/results_interpretability/")
print("="*70)

