# -*- coding: utf-8 -*-
"""Gemma 2b MRPC

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/gemma-2b-mrpc.91fd70b3-898d-46c2-b004-e0bf0358a339.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251231/auto/storage/goog4_request%26X-Goog-Date%3D20251231T052709Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6384da3c595c832f3c4f3ceb82045c5a1a3460b7b2a60d3bb31407cc4d5f9000b7ef57be43d9afc4a7f9c7064714e69626937d6c4bcb9a97127f4d9cbf64b7583243019a81fa57057ca1c891a3cb534b84b8064fd39aa811c87395d741892e7c19b9c43b7e9b9a4a705427563d1140aed3dec6a321e01c98c10d8bf3cb73858929fa2e668150ee8a436243cba505bbded7313ba762e828550062fd19d96ad881196011a3653ee819e21674118d1901e09ddc4864c4c10aff40b18ba09d7f44db2d5b7b20037a049558cc2126ccd0edb7bc8b6898e616b14d2934a6e2895e514613acc0a66ca6a64e4549828b1c60ec232d0ea96d0c9f7d722240e5ead26f46ef
"""

# -*- coding: utf-8 -*-

!pip install sae-lens groq

# ======================================================================
# 0. HF AUTHENTICATION (REQUIRED FOR GEMMA)
# ======================================================================
from huggingface_hub import login

# OPTION A: Kaggle Secrets (recommended)
try:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    hf_token = user_secrets.get_secret("HF_TOKEN")
    login(token=hf_token)
    print("✓ Logged in to Hugging Face via Kaggle Secrets")
except Exception as e:
    print("Could not use Kaggle Secrets, falling back to manual login:", e)
    login()
    print("✓ Logged in to Hugging Face manually")

# ======================================================================
# 1. Imports
# ======================================================================
import torch
import re
import numpy as np
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformer_lens import HookedTransformer
from sae_lens import SAE
from groq import Groq
from scipy.stats import pearsonr
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from collections import Counter
import os

# ======================================================================
# 2. Setup
# ======================================================================
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.makedirs('/kaggle/working/results_mrpc_gemma', exist_ok=True)

os.environ['HF_HOME'] = '/kaggle/working/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Groq setup
user_secrets = UserSecretsClient()
os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
if GROQ_API_KEY is None:
    raise ValueError("GROQ_API_KEY not found in Kaggle secrets.")

client = Groq(api_key=GROQ_API_KEY)

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful interpretability assistant."},
            {"role": "user", "content": prompt},
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# ======================================================================
# 3. Load Gemma-2B (BASE MODEL - for Gemma Scope SAEs)
# ======================================================================
print("Loading Gemma-2B base model (for Gemma Scope SAEs)...")
model = HookedTransformer.from_pretrained("google/gemma-2b", device=device)
model.eval()

print(f"Model has {model.cfg.n_layers} layers (0-{model.cfg.n_layers-1})")

print("Loading GLUE MRPC dataset...")
dataset = load_dataset("glue", "mrpc")
train_dataset = dataset["train"]
val_dataset = dataset["validation"]

print(f"Train samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Val class distribution: {Counter(val_dataset['label'])}")

# ======================================================================
# 4. Helper: Combine Sentence Pairs
# ======================================================================
def combine_sentences(example):
    """Concatenate sentence1 and sentence2 with separator."""
    return f"{example['sentence1']} <|endoftext|> {example['sentence2']}"

val_combined = [combine_sentences(ex) for ex in val_dataset]

# ======================================================================
# 5. STEP 1: Baseline - Zero-Shot Prompting
# ======================================================================
print("\n" + "="*70)
print("STEP 1: BASELINE - ZERO-SHOT PROMPTING (Gemma-2B on MRPC)")
print("="*70)

baseline_predictions = []
baseline_labels = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Zero-Shot Evaluation"):
        s1 = val_dataset[idx]["sentence1"]
        s2 = val_dataset[idx]["sentence2"]
        true_label = val_dataset[idx]["label"]

        prompt = f"""Are these two sentences paraphrases?

Sentence 1: {s1}
Sentence 2: {s2}
Answer (yes/no):"""

        tokens = model.to_tokens(prompt, prepend_bos=True)
        logits = model(tokens)

        next_token_logits = logits[0, -1, :]

        yes_token_id = model.to_tokens(" yes", prepend_bos=False)[0, 0]
        no_token_id = model.to_tokens(" no", prepend_bos=False)[0, 0]

        if next_token_logits[yes_token_id] > next_token_logits[no_token_id]:
            prediction = 1
        else:
            prediction = 0

        baseline_predictions.append(prediction)
        baseline_labels.append(true_label)

baseline_predictions = np.array(baseline_predictions)
baseline_labels = np.array(baseline_labels)

baseline_acc = accuracy_score(baseline_labels, baseline_predictions)
baseline_p, baseline_r, baseline_f1, _ = precision_recall_fscore_support(
    baseline_labels, baseline_predictions, average='binary'
)

print(f"\nBaseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"Precision: {baseline_p:.4f}, Recall: {baseline_r:.4f}, F1: {baseline_f1:.4f}")

with open('/kaggle/working/results_mrpc_gemma/baseline_results.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("BASELINE - ZERO-SHOT PROMPTING (Gemma-2B on MRPC)\n")
    f.write("="*70 + "\n\n")
    f.write(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\n")
    f.write(f"Precision: {baseline_p:.4f}\n")
    f.write(f"Recall: {baseline_r:.4f}\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n")

print("✓ Baseline saved")

# ======================================================================
# 6. STEP 2: Layer-wise Analysis (Probes + Gemma Scope SAEs)
# ======================================================================
print("\n" + "="*70)
print("STEP 2: LAYER-WISE ANALYSIS (Gemma Scope SAEs on MRPC)")
print("="*70)

layer_performance = {}
sae_feature_stats = {}

TOP_K = 10
release = "gemma-scope-2b-pt-att-canonical"

for layer_num in range(model.cfg.n_layers):
    print(f"\n{'='*70}")
    print(f"LAYER {layer_num} ANALYSIS")
    print(f"{'='*70}")

    hook_name = f"blocks.{layer_num}.attn.hook_z"

    # --------------------------------------------------------------
    # Part A: Raw representation classifier
    # --------------------------------------------------------------
    print("\n--- Part A: Raw Representation Classifier ---")

    print("Extracting training representations...")
    train_reps = []
    train_labels = []

    with torch.no_grad():
        for idx in tqdm(range(len(train_dataset)), desc=f"Train L{layer_num}"):
            combined = combine_sentences(train_dataset[idx])
            label = train_dataset[idx]["label"]

            tokens = model.to_tokens(combined, prepend_bos=True)
            _, cache = model.run_with_cache(tokens)

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            train_reps.append(pooled_flat)
            train_labels.append(label)

    X_train = np.array(train_reps)
    y_train = np.array(train_labels)

    print("Extracting validation representations...")
    val_reps = []
    val_labels_raw = []

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"Val L{layer_num}"):
            combined = combine_sentences(val_dataset[idx])
            label = val_dataset[idx]["label"]

            tokens = model.to_tokens(combined, prepend_bos=True)
            _, cache = model.run_with_cache(tokens)

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            val_reps.append(pooled_flat)
            val_labels_raw.append(label)

    X_val = np.array(val_reps)
    y_val = np.array(val_labels_raw)

    print("Training classifier...")
    clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
    clf.fit(X_train, y_train)

    predictions = clf.predict(X_val)

    acc = accuracy_score(y_val, predictions)
    p, r, f1, _ = precision_recall_fscore_support(y_val, predictions, average='binary')
    cm = confusion_matrix(y_val, predictions)

    layer_performance[layer_num] = {
        "accuracy": acc,
        "precision": p,
        "recall": r,
        "f1_score": f1,
        "improvement": acc - baseline_acc,
        "confusion_matrix": cm,
    }

    print(f"Layer {layer_num} Accuracy: {acc:.4f} ({acc*100:.2f}%)")
    print(f"F1-Score: {f1:.4f}")
    print(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%")

    # --------------------------------------------------------------
    # Part B: Gemma Scope SAE feature analysis
    # --------------------------------------------------------------
    print("\n--- Part B: Gemma Scope SAE Feature Analysis ---")

    sae_id = f"layer_{layer_num}/width_16k/canonical"

    try:
        print(f"Loading Gemma Scope SAE: {release}, {sae_id}")
        try:
            sae = SAE.from_pretrained(release, sae_id)
        except:
            sae = SAE.from_pretrained(release, sae_id)[0]

        sae.to(device)
        sae.eval()
        print(f"✓ Loaded SAE: {sae_id}")

    except Exception as e:
        print(f"✗ Could not load SAE for layer {layer_num}: {e}")
        sae_feature_stats[layer_num] = {"error": str(e)}
        continue

    print("Extracting SAE features...")

    pos_active_features = set()
    neg_active_features = set()

    pos_feature_activations = {}
    neg_feature_activations = {}

    pos_feature_counts = Counter()
    neg_feature_counts = Counter()

    total_pos = 0
    total_neg = 0

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"SAE L{layer_num}"):
            combined = combine_sentences(val_dataset[idx])
            label = val_dataset[idx]["label"]

            tokens = model.to_tokens(combined, prepend_bos=True)
            _, cache = model.run_with_cache(tokens)

            hook_acts = cache[hook_name]
            sae_feature_acts = sae.encode(hook_acts)

            pooled_features = sae_feature_acts.mean(dim=1)
            pooled_features = pooled_features.cpu().numpy().flatten()

            active_indices = np.where(pooled_features > 0)[0]
            active_values = pooled_features[active_indices]

            if label == 1:  # Paraphrase
                total_pos += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    pos_active_features.add(feat_idx)
                    pos_feature_counts[feat_idx] += 1
                    pos_feature_activations.setdefault(feat_idx, []).append(act_val)
            else:  # Not paraphrase
                total_neg += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    neg_active_features.add(feat_idx)
                    neg_feature_counts[feat_idx] += 1
                    neg_feature_activations.setdefault(feat_idx, []).append(act_val)

    common_features = pos_active_features & neg_active_features
    pos_only_features = pos_active_features - neg_active_features
    neg_only_features = neg_active_features - pos_active_features

    pos_avg_activations = {f: np.mean(acts) for f, acts in pos_feature_activations.items()}
    neg_avg_activations = {f: np.mean(acts) for f, acts in neg_feature_activations.items()}

    top5_pos_by_activation = sorted(
        pos_avg_activations.items(), key=lambda x: x[1], reverse=True
    )[:5]
    top5_neg_by_activation = sorted(
        neg_avg_activations.items(), key=lambda x: x[1], reverse=True
    )[:5]

    topk_pos_by_frequency = pos_feature_counts.most_common(TOP_K)
    topk_neg_by_frequency = neg_feature_counts.most_common(TOP_K)

    top5_pos_by_frequency = pos_feature_counts.most_common(5)
    top5_neg_by_frequency = neg_feature_counts.most_common(5)

    topk_pos_ids = {feat for feat, _ in topk_pos_by_frequency}
    topk_neg_ids = {feat for feat, _ in topk_neg_by_frequency}
    topk_common_ids = topk_pos_ids & topk_neg_ids

    sae_feature_stats[layer_num] = {
        "total_pos_features": len(pos_active_features),
        "total_neg_features": len(neg_active_features),
        "common_features": len(common_features),
        "pos_only_features": len(pos_only_features),
        "neg_only_features": len(neg_only_features),
        "top5_pos_by_activation": top5_pos_by_activation,
        "top5_neg_by_activation": top5_neg_by_activation,
        "top5_pos_by_frequency": top5_pos_by_frequency,
        "top5_neg_by_frequency": top5_neg_by_frequency,
        "topk_pos_by_frequency": topk_pos_by_frequency,
        "topk_neg_by_frequency": topk_neg_by_frequency,
        "topk_common_ids": topk_common_ids,
        "total_pos_samples": total_pos,
        "total_neg_samples": total_neg,
    }

    # Save per-layer file
    with open(f"/kaggle/working/results_mrpc_gemma/layer_{layer_num}_complete_analysis.txt", "w") as f:
        f.write("="*70 + "\n")
        f.write(f"LAYER {layer_num} - COMPLETE ANALYSIS (Gemma-2B on MRPC)\n")
        f.write("="*70 + "\n\n")

        f.write("A. LAYER PERFORMANCE (Raw Representations)\n")
        f.write("-"*70 + "\n")
        f.write(f"Accuracy: {acc:.4f} ({acc*100:.2f}%)\n")
        f.write(f"Precision: {p:.4f}\n")
        f.write(f"Recall: {r:.4f}\n")
        f.write(f"F1-Score: {f1:.4f}\n")
        f.write(f"Baseline Accuracy: {baseline_acc:.4f}\n")
        f.write(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%\n\n")

        f.write("Confusion Matrix:\n")
        f.write("              Predicted\n")
        f.write("         Not-Para  Para\n")
        f.write(f"Actual 0  [{cm[0,0]:5d}  {cm[0,1]:5d}]\n")
        f.write(f"       1  [{cm[1,0]:5d}  {cm[1,1]:5d}]\n\n")

        f.write("B. SAE FEATURE COUNTS\n")
        f.write("-"*70 + "\n")
        f.write(f"Total PARAPHRASE samples: {total_pos}\n")
        f.write(f"Total NON-PARAPHRASE samples: {total_neg}\n\n")
        f.write(f"Total features activated for PARAPHRASE: {len(pos_active_features)}\n")
        f.write(f"Total features activated for NON-PARAPHRASE: {len(neg_active_features)}\n")
        f.write(f"COMMON features: {len(common_features)}\n")
        f.write(f"UNIQUE to PARAPHRASE: {len(pos_only_features)}\n")
        f.write(f"UNIQUE to NON-PARAPHRASE: {len(neg_only_features)}\n\n")

        # Sections C-I (same structure as SST-2)
        f.write("C. TOP 5 MOST ACTIVATING FEATURES (PARAPHRASE)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, avg_act) in enumerate(top5_pos_by_activation, 1):
            freq = pos_feature_counts[feat_idx]
            pct = freq / total_pos * 100 if total_pos > 0 else 0.0
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(
                f"{rank}. Feature {feat_idx}: avg_activation={avg_act:.4f}, "
                f"frequency={freq}/{total_pos} ({pct:.1f}%) | {status}\n"
            )

        # Continue with sections D-I (copy from your SST-2 script)

    print(f"✓ Layer {layer_num} complete analysis saved")

# ======================================================================
# 7. FINAL SUMMARY
# ======================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

best_layer = max(layer_performance.items(), key=lambda x: x[1]["accuracy"])
worst_layer = min(layer_performance.items(), key=lambda x: x[1]["accuracy"])

print(
    f"\n{'Layer':<6} {'Acc%':<8} {'F1':<8} {'Improv':<10} "
    f"{'Para Feat':<10} {'Non-Para':<10} {'Common':<10}"
)
print("-"*90)
for layer_num in sorted(layer_performance.keys()):
    perf = layer_performance[layer_num]
    stats = sae_feature_stats.get(layer_num, {})
    print(
        f"L{layer_num:<5} {perf['accuracy']*100:5.2f}%   {perf['f1_score']:.4f}  "
        f"{perf['improvement']*100:+6.2f}%    "
        f"{stats.get('total_pos_features', 'N/A'):<10} "
        f"{stats.get('total_neg_features', 'N/A'):<10} "
        f"{stats.get('common_features', 'N/A'):<10}"
    )

print(f"\n✓ Best layer: Layer {best_layer[0]} ({best_layer[1]['accuracy']*100:.2f}% accuracy, F1={best_layer[1]['f1_score']:.4f})")

# Get top 2 layers
top_2_layers = sorted(layer_performance.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:2]
top_2_layer_nums = [layer_num for layer_num, _ in top_2_layers]
print(f"Top 2 layers for interpretability: {top_2_layer_nums}")

with open("/kaggle/working/results_mrpc_gemma/final_summary.txt", "w") as f:
    f.write("="*70 + "\n")
    f.write("FINAL SUMMARY - GEMMA-2B ON MRPC\n")
    f.write("="*70 + "\n\n")

    f.write("1. BASELINE (Zero-Shot Prompting)\n")
    f.write("-"*70 + "\n")
    f.write(f"Accuracy: {baseline_acc*100:.2f}%\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n\n")

    f.write(f"2. Top 2 Layers: {top_2_layer_nums}\n\n")
    f.write("INTERPRETATION:\n")
    f.write("- MRPC is paraphrase detection (sentence pairs)\n")
    f.write("- SAE features capture semantic similarity patterns\n")

print("\n✓ All results saved to /kaggle/working/results_mrpc_gemma/")
print("="*70)
print("✓ GEMMA-2B MRPC ANALYSIS COMPLETE (with Groq interpretability for top 2 layers)")
print("="*70)
