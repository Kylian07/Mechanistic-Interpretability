# -*- coding: utf-8 -*-
"""Gemma 2b MRPC

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/gemma-2b-mrpc.91fd70b3-898d-46c2-b004-e0bf0358a339.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251231/auto/storage/goog4_request%26X-Goog-Date%3D20251231T052709Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6384da3c595c832f3c4f3ceb82045c5a1a3460b7b2a60d3bb31407cc4d5f9000b7ef57be43d9afc4a7f9c7064714e69626937d6c4bcb9a97127f4d9cbf64b7583243019a81fa57057ca1c891a3cb534b84b8064fd39aa811c87395d741892e7c19b9c43b7e9b9a4a705427563d1140aed3dec6a321e01c98c10d8bf3cb73858929fa2e668150ee8a436243cba505bbded7313ba762e828550062fd19d96ad881196011a3653ee819e21674118d1901e09ddc4864c4c10aff40b18ba09d7f44db2d5b7b20037a049558cc2126ccd0edb7bc8b6898e616b14d2934a6e2895e514613acc0a66ca6a64e4549828b1c60ec232d0ea96d0c9f7d722240e5ead26f46ef
"""

# -*- coding: utf-8 -*-

!pip install sae-lens groq

# ======================================================================
# 0. HF AUTHENTICATION (REQUIRED FOR GEMMA)
# ======================================================================
from huggingface_hub import login

# OPTION A: Kaggle Secrets (recommended)
try:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    hf_token = user_secrets.get_secret("HF_TOKEN")
    login(token=hf_token)
    print("✓ Logged in to Hugging Face via Kaggle Secrets")
except Exception as e:
    print("Could not use Kaggle Secrets, falling back to manual login:", e)
    login()
    print("✓ Logged in to Hugging Face manually")

# ======================================================================
# 1. Imports
# ======================================================================
import torch
import re
import numpy as np
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformer_lens import HookedTransformer
from sae_lens import SAE
from groq import Groq
from scipy.stats import pearsonr
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from collections import Counter
import os

# ======================================================================
# 2. Setup
# ======================================================================
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.makedirs('/kaggle/working/results_mrpc_gemma', exist_ok=True)

os.environ['HF_HOME'] = '/kaggle/working/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Groq setup
user_secrets = UserSecretsClient()
os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
if GROQ_API_KEY is None:
    raise ValueError("GROQ_API_KEY not found in Kaggle secrets.")

client = Groq(api_key=GROQ_API_KEY)

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful interpretability assistant."},
            {"role": "user", "content": prompt},
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# ======================================================================
# 3. Load Gemma-2B (BASE MODEL - for Gemma Scope SAEs)
# ======================================================================
print("Loading Gemma-2B base model (for Gemma Scope SAEs)...")
model = HookedTransformer.from_pretrained("google/gemma-2b", device=device)
model.eval()

print(f"Model has {model.cfg.n_layers} layers (0-{model.cfg.n_layers-1})")

print("Loading GLUE MRPC dataset...")
dataset = load_dataset("glue", "mrpc")
train_dataset = dataset["train"]
val_dataset = dataset["validation"]

print(f"Train samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Val class distribution: {Counter(val_dataset['label'])}")

# ======================================================================
# 4. Helper: Combine Sentence Pairs
# ======================================================================
def combine_sentences(example):
    """Concatenate sentence1 and sentence2 with separator."""
    return f"{example['sentence1']} <|endoftext|> {example['sentence2']}"

val_combined = [combine_sentences(ex) for ex in val_dataset]

# ======================================================================
# 5. STEP 1: Baseline - Zero-Shot Prompting
# ======================================================================
print("\n" + "="*70)
print("STEP 1: BASELINE - ZERO-SHOT PROMPTING (Gemma-2B on MRPC)")
print("="*70)

baseline_predictions = []
baseline_labels = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Zero-Shot Evaluation"):
        s1 = val_dataset[idx]["sentence1"]
        s2 = val_dataset[idx]["sentence2"]
        true_label = val_dataset[idx]["label"]

        prompt = f"""Are these two sentences paraphrases?

Sentence 1: {s1}
Sentence 2: {s2}
Answer (yes/no):"""

        tokens = model.to_tokens(prompt, prepend_bos=True)
        logits = model(tokens)

        next_token_logits = logits[0, -1, :]

        yes_token_id = model.to_tokens(" yes", prepend_bos=False)[0, 0]
        no_token_id = model.to_tokens(" no", prepend_bos=False)[0, 0]

        if next_token_logits[yes_token_id] > next_token_logits[no_token_id]:
            prediction = 1
        else:
            prediction = 0

        baseline_predictions.append(prediction)
        baseline_labels.append(true_label)

baseline_predictions = np.array(baseline_predictions)
baseline_labels = np.array(baseline_labels)

baseline_acc = accuracy_score(baseline_labels, baseline_predictions)
baseline_p, baseline_r, baseline_f1, _ = precision_recall_fscore_support(
    baseline_labels, baseline_predictions, average='binary'
)

print(f"\nBaseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"Precision: {baseline_p:.4f}, Recall: {baseline_r:.4f}, F1: {baseline_f1:.4f}")

with open('/kaggle/working/results_mrpc_gemma/baseline_results.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("BASELINE - ZERO-SHOT PROMPTING (Gemma-2B on MRPC)\n")
    f.write("="*70 + "\n\n")
    f.write(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\n")
    f.write(f"Precision: {baseline_p:.4f}\n")
    f.write(f"Recall: {baseline_r:.4f}\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n")

print("✓ Baseline saved")

# ======================================================================
# 6. STEP 2: Layer-wise Analysis (Probes + Gemma Scope SAEs)
# ======================================================================
print("\n" + "="*70)
print("STEP 2: LAYER-WISE ANALYSIS (Gemma Scope SAEs on MRPC)")
print("="*70)

layer_performance = {}
sae_feature_stats = {}

TOP_K = 10
release = "gemma-scope-2b-pt-att-canonical"

for layer_num in range(model.cfg.n_layers):
    print(f"\n{'='*70}")
    print(f"LAYER {layer_num} ANALYSIS")
    print(f"{'='*70}")

    hook_name = f"blocks.{layer_num}.attn.hook_z"

    # --------------------------------------------------------------
    # Part A: Raw representation classifier
    # --------------------------------------------------------------
    print("\n--- Part A: Raw Representation Classifier ---")

    print("Extracting training representations...")
    train_reps = []
    train_labels = []

    with torch.no_grad():
        for idx in tqdm(range(len(train_dataset)), desc=f"Train L{layer_num}"):
            combined = combine_sentences(train_dataset[idx])
            label = train_dataset[idx]["label"]

            tokens = model.to_tokens(combined, prepend_bos=True)
            _, cache = model.run_with_cache(tokens)

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            train_reps.append(pooled_flat)
            train_labels.append(label)

    X_train = np.array(train_reps)
    y_train = np.array(train_labels)

    print("Extracting validation representations...")
    val_reps = []
    val_labels_raw = []

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"Val L{layer_num}"):
            combined = combine_sentences(val_dataset[idx])
            label = val_dataset[idx]["label"]

            tokens = model.to_tokens(combined, prepend_bos=True)
            _, cache = model.run_with_cache(tokens)

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            val_reps.append(pooled_flat)
            val_labels_raw.append(label)

    X_val = np.array(val_reps)
    y_val = np.array(val_labels_raw)

    print("Training classifier...")
    clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
    clf.fit(X_train, y_train)

    predictions = clf.predict(X_val)

    acc = accuracy_score(y_val, predictions)
    p, r, f1, _ = precision_recall_fscore_support(y_val, predictions, average='binary')
    cm = confusion_matrix(y_val, predictions)

    layer_performance[layer_num] = {
        "accuracy": acc,
        "precision": p,
        "recall": r,
        "f1_score": f1,
        "improvement": acc - baseline_acc,
        "confusion_matrix": cm,
    }

    print(f"Layer {layer_num} Accuracy: {acc:.4f} ({acc*100:.2f}%)")
    print(f"F1-Score: {f1:.4f}")
    print(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%")

    # --------------------------------------------------------------
    # Part B: Gemma Scope SAE feature analysis
    # --------------------------------------------------------------
    print("\n--- Part B: Gemma Scope SAE Feature Analysis ---")

    sae_id = f"layer_{layer_num}/width_16k/canonical"

    try:
        print(f"Loading Gemma Scope SAE: {release}, {sae_id}")
        try:
            sae = SAE.from_pretrained(release, sae_id)
        except:
            sae = SAE.from_pretrained(release, sae_id)[0]

        sae.to(device)
        sae.eval()
        print(f"✓ Loaded SAE: {sae_id}")

    except Exception as e:
        print(f"✗ Could not load SAE for layer {layer_num}: {e}")
        sae_feature_stats[layer_num] = {"error": str(e)}
        continue

    print("Extracting SAE features...")

    pos_active_features = set()
    neg_active_features = set()

    pos_feature_activations = {}
    neg_feature_activations = {}

    pos_feature_counts = Counter()
    neg_feature_counts = Counter()

    total_pos = 0
    total_neg = 0

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"SAE L{layer_num}"):
            combined = combine_sentences(val_dataset[idx])
            label = val_dataset[idx]["label"]

            tokens = model.to_tokens(combined, prepend_bos=True)
            _, cache = model.run_with_cache(tokens)

            hook_acts = cache[hook_name]
            sae_feature_acts = sae.encode(hook_acts)

            pooled_features = sae_feature_acts.mean(dim=1)
            pooled_features = pooled_features.cpu().numpy().flatten()

            active_indices = np.where(pooled_features > 0)[0]
            active_values = pooled_features[active_indices]

            if label == 1:  # Paraphrase
                total_pos += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    pos_active_features.add(feat_idx)
                    pos_feature_counts[feat_idx] += 1
                    pos_feature_activations.setdefault(feat_idx, []).append(act_val)
            else:  # Not paraphrase
                total_neg += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    neg_active_features.add(feat_idx)
                    neg_feature_counts[feat_idx] += 1
                    neg_feature_activations.setdefault(feat_idx, []).append(act_val)

    common_features = pos_active_features & neg_active_features
    pos_only_features = pos_active_features - neg_active_features
    neg_only_features = neg_active_features - pos_active_features

    pos_avg_activations = {f: np.mean(acts) for f, acts in pos_feature_activations.items()}
    neg_avg_activations = {f: np.mean(acts) for f, acts in neg_feature_activations.items()}

    top5_pos_by_activation = sorted(
        pos_avg_activations.items(), key=lambda x: x[1], reverse=True
    )[:5]
    top5_neg_by_activation = sorted(
        neg_avg_activations.items(), key=lambda x: x[1], reverse=True
    )[:5]

    topk_pos_by_frequency = pos_feature_counts.most_common(TOP_K)
    topk_neg_by_frequency = neg_feature_counts.most_common(TOP_K)

    top5_pos_by_frequency = pos_feature_counts.most_common(5)
    top5_neg_by_frequency = neg_feature_counts.most_common(5)

    topk_pos_ids = {feat for feat, _ in topk_pos_by_frequency}
    topk_neg_ids = {feat for feat, _ in topk_neg_by_frequency}
    topk_common_ids = topk_pos_ids & topk_neg_ids

    sae_feature_stats[layer_num] = {
        "total_pos_features": len(pos_active_features),
        "total_neg_features": len(neg_active_features),
        "common_features": len(common_features),
        "pos_only_features": len(pos_only_features),
        "neg_only_features": len(neg_only_features),
        "top5_pos_by_activation": top5_pos_by_activation,
        "top5_neg_by_activation": top5_neg_by_activation,
        "top5_pos_by_frequency": top5_pos_by_frequency,
        "top5_neg_by_frequency": top5_neg_by_frequency,
        "topk_pos_by_frequency": topk_pos_by_frequency,
        "topk_neg_by_frequency": topk_neg_by_frequency,
        "topk_common_ids": topk_common_ids,
        "total_pos_samples": total_pos,
        "total_neg_samples": total_neg,
    }

    # Save per-layer file
    with open(f"/kaggle/working/results_mrpc_gemma/layer_{layer_num}_complete_analysis.txt", "w") as f:
        f.write("="*70 + "\n")
        f.write(f"LAYER {layer_num} - COMPLETE ANALYSIS (Gemma-2B on MRPC)\n")
        f.write("="*70 + "\n\n")

        f.write("A. LAYER PERFORMANCE (Raw Representations)\n")
        f.write("-"*70 + "\n")
        f.write(f"Accuracy: {acc:.4f} ({acc*100:.2f}%)\n")
        f.write(f"Precision: {p:.4f}\n")
        f.write(f"Recall: {r:.4f}\n")
        f.write(f"F1-Score: {f1:.4f}\n")
        f.write(f"Baseline Accuracy: {baseline_acc:.4f}\n")
        f.write(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%\n\n")

        f.write("Confusion Matrix:\n")
        f.write("              Predicted\n")
        f.write("         Not-Para  Para\n")
        f.write(f"Actual 0  [{cm[0,0]:5d}  {cm[0,1]:5d}]\n")
        f.write(f"       1  [{cm[1,0]:5d}  {cm[1,1]:5d}]\n\n")

        f.write("B. SAE FEATURE COUNTS\n")
        f.write("-"*70 + "\n")
        f.write(f"Total PARAPHRASE samples: {total_pos}\n")
        f.write(f"Total NON-PARAPHRASE samples: {total_neg}\n\n")
        f.write(f"Total features activated for PARAPHRASE: {len(pos_active_features)}\n")
        f.write(f"Total features activated for NON-PARAPHRASE: {len(neg_active_features)}\n")
        f.write(f"COMMON features: {len(common_features)}\n")
        f.write(f"UNIQUE to PARAPHRASE: {len(pos_only_features)}\n")
        f.write(f"UNIQUE to NON-PARAPHRASE: {len(neg_only_features)}\n\n")

        # Sections C-I (same structure as SST-2)
        f.write("C. TOP 5 MOST ACTIVATING FEATURES (PARAPHRASE)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, avg_act) in enumerate(top5_pos_by_activation, 1):
            freq = pos_feature_counts[feat_idx]
            pct = freq / total_pos * 100 if total_pos > 0 else 0.0
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(
                f"{rank}. Feature {feat_idx}: avg_activation={avg_act:.4f}, "
                f"frequency={freq}/{total_pos} ({pct:.1f}%) | {status}\n"
            )

        # Continue with sections D-I (copy from your SST-2 script)

    print(f"✓ Layer {layer_num} complete analysis saved")

# ======================================================================
# 7. FINAL SUMMARY
# ======================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

best_layer = max(layer_performance.items(), key=lambda x: x[1]["accuracy"])
worst_layer = min(layer_performance.items(), key=lambda x: x[1]["accuracy"])

print(
    f"\n{'Layer':<6} {'Acc%':<8} {'F1':<8} {'Improv':<10} "
    f"{'Para Feat':<10} {'Non-Para':<10} {'Common':<10}"
)
print("-"*90)
for layer_num in sorted(layer_performance.keys()):
    perf = layer_performance[layer_num]
    stats = sae_feature_stats.get(layer_num, {})
    print(
        f"L{layer_num:<5} {perf['accuracy']*100:5.2f}%   {perf['f1_score']:.4f}  "
        f"{perf['improvement']*100:+6.2f}%    "
        f"{stats.get('total_pos_features', 'N/A'):<10} "
        f"{stats.get('total_neg_features', 'N/A'):<10} "
        f"{stats.get('common_features', 'N/A'):<10}"
    )

print(f"\n✓ Best layer: Layer {best_layer[0]} ({best_layer[1]['accuracy']*100:.2f}% accuracy, F1={best_layer[1]['f1_score']:.4f})")

# Get top 2 layers
top_2_layers = sorted(layer_performance.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:2]
top_2_layer_nums = [layer_num for layer_num, _ in top_2_layers]
print(f"Top 2 layers for interpretability: {top_2_layer_nums}")

with open("/kaggle/working/results_mrpc_gemma/final_summary.txt", "w") as f:
    f.write("="*70 + "\n")
    f.write("FINAL SUMMARY - GEMMA-2B ON MRPC\n")
    f.write("="*70 + "\n\n")

    f.write("1. BASELINE (Zero-Shot Prompting)\n")
    f.write("-"*70 + "\n")
    f.write(f"Accuracy: {baseline_acc*100:.2f}%\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n\n")

    f.write(f"2. Top 2 Layers: {top_2_layer_nums}\n\n")
    f.write("INTERPRETATION:\n")
    f.write("- MRPC is paraphrase detection (sentence pairs)\n")
    f.write("- SAE features capture semantic similarity patterns\n")

print("\n✓ All results saved to /kaggle/working/results_mrpc_gemma/")
print("="*70)

# ======================================================================
# 8. GROQ INTERPRETABILITY FOR TOP 2 LAYERS
# ======================================================================
print("\n" + "="*70)
print("STEP 3: GROQ INTERPRETABILITY FOR TOP 2 LAYERS")
print("="*70)

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, texts):
        self.texts = texts
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        return self.texts[idx]

val_text_ds = TextDataset(val_combined)

class SAEWithActs(torch.nn.Module):
    def __init__(self, sae_model):
        super().__init__()
        self.sae = sae_model
        self.activations = None
        hook_module = getattr(self.sae, "hook_sae_acts_post", None)
        if hook_module is None:
            raise ValueError("hook_sae_acts_post not found on SAE model.")
        hook_module.register_forward_hook(self._hook)

    def _hook(self, module, inp, out):
        self.activations = out.detach()

    def forward(self, sae_input):
        _ = self.sae(sae_input)
        return self.activations

    def get_acts(self, sae_input):
        self.eval()
        with torch.no_grad():
            _ = self.forward(sae_input)
            return self.activations

def top_k_global_features_for_layer(layer_num, sae_model, k=10, max_batches=None):
    sae_wrap = SAEWithActs(sae_model).to(device)
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    counter = Counter()
    dataloader = DataLoader(val_text_ds, batch_size=32)

    for i, batch_texts in enumerate(tqdm(dataloader, desc=f"Counting features L{layer_num}")):
        if max_batches is not None and i >= max_batches:
            break

        batch_tokens = model.to_tokens(batch_texts, prepend_bos=True)
        with torch.no_grad():
            _, cache = model.run_with_cache(batch_tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]
            latents = sae_wrap.get_acts(hook_acts)

        active = (latents > 0).any(dim=1)
        for row in active:
            idxs = row.nonzero(as_tuple=True)[0].tolist()
            counter.update(idxs)

    topk = counter.most_common(k)
    print(f"Layer {layer_num} top-{k} global features (idx, count): {topk}")
    return [idx for idx, _ in topk]

def extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=None):
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    acts_all, texts_all = [], []
    dataloader = DataLoader(val_text_ds, batch_size=32)

    for i, batch_texts in enumerate(tqdm(dataloader, desc=f"Extracting L{layer_num} F{feature_idx}")):
        if max_batches is not None and i >= max_batches:
            break

        batch_tokens = model.to_tokens(batch_texts, prepend_bos=True)
        with torch.no_grad():
            _, cache = model.run_with_cache(batch_tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]
            latents = sae_wrap.get_acts(hook_acts)

        feat_acts = latents[:, :, feature_idx].detach().cpu().numpy()
        for a_sent, txt in zip(feat_acts, batch_texts):
            acts_all.append(a_sent)
            texts_all.append(txt)

    return acts_all, texts_all

def select_top(acts, texts, n=20):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:n]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

def llama_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a sparse autoencoder feature from Gemma-2B on MRPC paraphrase detection.\n"
        "Each text below is a concatenated sentence pair; activations show per-token strength.\n\n"
        "From this data, give one concise sentence describing what pattern or concept "
        "this feature responds to.\n\n"
    )
    for i, (txt, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{txt}\"\nActivations: {acts.tolist()}\n\n"
    prompt += "Your explanation:"
    return chat_with_llama(prompt)

def llama_activation_score(sentence, interpretation):
    prompt = (
        f'Feature interpretation:\n"{interpretation}"\n\n'
        "On a scale from 0 (not active) to 10 (very active), estimate how strongly "
        "this feature activates on the following sentence pair. Respond with only a single number.\n\n"
        f'Text: \"{sentence}\"\nActivation:'
    )
    resp = chat_with_llama(prompt, max_tokens=16)
    m = re.search(r"\d+(\.\d+)?", resp)
    return float(m.group()) if m else 0.0

def pearson_score(actual_acts, pred_scores):
    actual = np.array([np.mean(a) for a in actual_acts]) * 10.0
    pred = np.array(pred_scores)
    corr, _ = pearsonr(actual, pred)
    return corr

def save_interp_report(path, layer_num, feature_idx, interpretation,
                       eval_texts, eval_acts, pred_scores, corr):
    with open(path, "a", encoding="utf-8") as f:
        f.write(f"Layer {layer_num}, Feature {feature_idx}\n")
        f.write("-"*70 + "\n")
        f.write("Interpretation:\n" + interpretation.strip() + "\n\n")
        f.write("Evaluation sentence pairs:\n")
        for i, (txt, acts, pred) in enumerate(zip(eval_texts, eval_acts, pred_scores), 1):
            actual = np.mean(acts) * 10.0
            f.write(f"{i}. {txt}\n   Actual={actual:.3f}, Pred={pred:.3f}\n")
        f.write(f"\nPearson correlation: {corr:.4f}\n")
        f.write("="*70 + "\n\n")

def interpretability_for_feature(layer_num, sae_model, feature_idx, report_path, max_batches=None):
    sae_wrap = SAEWithActs(sae_model).to(device)

    acts_all, texts_all = extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=max_batches)

    top_acts, top_texts = select_top(acts_all, texts_all, n=20)
    interp_acts, interp_texts = top_acts[:5], top_texts[:5]

    print(f"\nTop 5 sentence pairs for L{layer_num} feature {feature_idx}:")
    for i, s in enumerate(interp_texts, 1):
        print(f"{i}. {s}")

    interpretation = llama_interpretation(interp_texts, interp_acts)
    print("\nInterpretation:", interpretation)

    used = set(interp_texts)
    rest = [(a, t) for a, t in zip(acts_all, texts_all) if t not in used]
    if not rest:
        print("No remaining sentences for evaluation; skipping feature.")
        return None

    rest_acts, rest_texts = zip(*rest)
    rest_acts, rest_texts = list(rest_acts), list(rest_texts)
    scores = [np.sum(a) for a in rest_acts]
    idxs = np.argsort(scores)[::-1][:5]
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    print("\nEvaluation sentence pairs:")
    for i, s in enumerate(eval_texts, 1):
        print(f"{i}. {s}")

    pred_scores = [llama_activation_score(s, interpretation) for s in eval_texts]
    corr = pearson_score(eval_acts, pred_scores)
    print(f"Pearson correlation for L{layer_num} feature {feature_idx}: {corr:.4f}")

    save_interp_report(report_path, layer_num, feature_idx, interpretation,
                       eval_texts, eval_acts, pred_scores, corr)

    return corr

# Run interpretability for top 2 layers
INTERP_TOP_K = 10

for layer_idx in top_2_layer_nums:
    print("\n" + "="*70)
    print(f"LAYER {layer_idx}: GROQ INTERPRETABILITY (Top-{INTERP_TOP_K} global features)")
    print("="*70)

    sae_id = f"layer_{layer_idx}/width_16k/canonical"
    try:
        sae_model = SAE.from_pretrained(release, sae_id)
    except:
        sae_model = SAE.from_pretrained(release, sae_id)[0]
    sae_model.to(device).eval()

    topk_feats = top_k_global_features_for_layer(
        layer_num=layer_idx,
        sae_model=sae_model,
        k=INTERP_TOP_K,
        max_batches=None,
    )

    report_file = f"/kaggle/working/results_mrpc_gemma/layer{layer_idx}_top{INTERP_TOP_K}_interpretability.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"INTERPRETABILITY REPORT – Gemma-2B MRPC Layer {layer_idx}, top-{INTERP_TOP_K} global features\n")
        f.write("="*70 + "\n\n")

    for feat in topk_feats:
        print(f"\n--- Layer {layer_idx}, global feature {feat} ---")
        interpretability_for_feature(
            layer_num=layer_idx,
            sae_model=sae_model,
            feature_idx=feat,
            report_path=report_file,
            max_batches=None,
        )

    print(f"\nLayer {layer_idx} interpretability: finished. Report saved to {report_file}")

print("\n" + "="*70)
print("✓ GEMMA-2B MRPC ANALYSIS COMPLETE (with Groq interpretability for top 2 layers)")
print("="*70)