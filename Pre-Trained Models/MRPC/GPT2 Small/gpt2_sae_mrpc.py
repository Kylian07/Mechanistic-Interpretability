# -*- coding: utf-8 -*-
"""GPT2 SAE MRPC

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/gpt2-sae-mrpc.55108106-3c18-4263-9685-e95545dacfd3.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251228/auto/storage/goog4_request%26X-Goog-Date%3D20251228T133419Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D33c5a3da58069eb9a9c1222f71d715ed553a33eb10007c10aac11882497458f7b8ac1801c3f9730a9583f76f66478308263a2ec2a3f3491bcfccb2c47c7319c4b7411658b11304d6cf595baa5d9f7a2a98a77bcbfaff703d4bed36b60242aaae8f3d5d0f6caa8d297e920fd0d32751fefd15ee015fe0b7d7c7d3f3f8a454c05543d594a7046cde9831c39566ebf70b42a17975819c29b7f07f41d3f75101041c7bb9c4c61453c2871422630fad6c8fb19e1a064f7fe983c28d6e7d9dc1ae84121ec21ae10a73d07f07312b6e50e06b407fefb9ae1d4e53762f9532ca992872c8a4b7bd32c04beff8847f49817cfe5abebfa596e24aa713dbb608c39994e6b749
"""

# ================================================================
# 0. INSTALLS & IMPORTS
# ================================================================
!pip install -q sae-lens transformer-lens datasets scikit-learn groq

import os
import gc
import re
import torch
import numpy as np
from collections import Counter
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformer_lens import HookedTransformer
from sae_lens import SAE
from groq import Groq
from scipy.stats import pearsonr
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ================================================================
# 1. SETUP
# ================================================================
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.makedirs('/kaggle/working/results_mrpc', exist_ok=True)

os.environ['HF_HOME'] = '/kaggle/working/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'

# ================================================================
# 2. GROQ SETUP
# ================================================================
from kaggle_secrets import UserSecretsClient
user_secrets = UserSecretsClient()
os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")

GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
if GROQ_API_KEY is None:
    raise ValueError("Set GROQ_API_KEY in Kaggle secrets.")

client = Groq(api_key=GROQ_API_KEY)

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful interpretability assistant."},
            {"role": "user", "content": prompt},
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# ================================================================
# 3. LOAD GPT-2 SMALL & MRPC
# ================================================================
print("Loading HookedTransformer (GPT-2 Small)...")
model = HookedTransformer.from_pretrained("gpt2-small", device=device)
model.eval()

n_layers = model.cfg.n_layers
print(f"Model has {n_layers} layers (0-{n_layers-1})")

print("Loading GLUE MRPC dataset...")
dataset = load_dataset("glue", "mrpc")
train_dataset = dataset["train"]
val_dataset = dataset["validation"]

print(f"Train: {len(train_dataset)}, Val: {len(val_dataset)}")
print(f"Val class distribution: {Counter(val_dataset['label'])}")

# ================================================================
# 4. HELPER: COMBINE SENTENCE PAIRS
# ================================================================
def combine_sentences(example):
    """Concatenate sentence1 and sentence2 with GPT-2's EOS token as separator."""
    return f"{example['sentence1']} <|endoftext|> {example['sentence2']}"

# Create combined text list for interpretability later
val_combined = [combine_sentences(ex) for ex in val_dataset]

# ================================================================
# 5. BASELINE: ZERO-SHOT PROMPTING
# ================================================================
print("\n" + "="*70)
print("STEP 1: BASELINE - ZERO-SHOT PROMPTING (GPT-2 Small on MRPC)")
print("="*70)

baseline_predictions = []
baseline_labels = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Zero-Shot Evaluation"):
        s1 = val_dataset[idx]["sentence1"]
        s2 = val_dataset[idx]["sentence2"]
        true_label = val_dataset[idx]["label"]

        prompt = f"""Are these two sentences paraphrases?

Sentence 1: {s1}
Sentence 2: {s2}
Answer (yes/no):"""

        tokens = model.to_tokens(prompt)
        logits = model(tokens)
        next_token_logits = logits[0, -1, :]

        yes_id = model.to_single_token(" yes")
        no_id = model.to_single_token(" no")

        if next_token_logits[yes_id] > next_token_logits[no_id]:
            prediction = 1
        else:
            prediction = 0

        baseline_predictions.append(prediction)
        baseline_labels.append(true_label)

baseline_predictions = np.array(baseline_predictions)
baseline_labels = np.array(baseline_labels)

baseline_acc = accuracy_score(baseline_labels, baseline_predictions)
baseline_p, baseline_r, baseline_f1, _ = precision_recall_fscore_support(
    baseline_labels, baseline_predictions, average='binary'
)

print(f"\nBaseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"Precision: {baseline_p:.4f}, Recall: {baseline_r:.4f}, F1: {baseline_f1:.4f}")

with open('/kaggle/working/results_mrpc/baseline_results.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("BASELINE - ZERO-SHOT PROMPTING (GPT-2 Small on MRPC)\n")
    f.write("="*70 + "\n\n")
    f.write(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\n")
    f.write(f"Precision: {baseline_p:.4f}\n")
    f.write(f"Recall: {baseline_r:.4f}\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n")

print("✓ Baseline saved")

# ================================================================
# 6. LAYER-WISE ANALYSIS (PROBES + SAEs)
# ================================================================
print("\n" + "="*70)
print("STEP 2: LAYER-WISE ANALYSIS (GPT-2 Small + SAE on MRPC)")
print("="*70)

layer_performance = {}
sae_feature_stats = {}

TOP_K = 10
release = "gpt2-small-hook-z-kk"

for layer_num in range(n_layers):
    print(f"\n{'='*70}")
    print(f"LAYER {layer_num} ANALYSIS")
    print(f"{'='*70}")

    hook_name = f"blocks.{layer_num}.attn.hook_z"

    # ----------------------------------------------------------
    # Part A: Raw representation classifier
    # ----------------------------------------------------------
    print("\n--- Part A: Raw Representation Classifier ---")

    print("Extracting training representations...")
    train_reps = []
    train_labels = []

    with torch.no_grad():
        for idx in tqdm(range(len(train_dataset)), desc=f"Train L{layer_num}"):
            combined = combine_sentences(train_dataset[idx])
            label = train_dataset[idx]["label"]

            tokens = model.to_tokens(combined)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            train_reps.append(pooled_flat)
            train_labels.append(label)

    X_train = np.array(train_reps)
    y_train = np.array(train_labels)

    print("Extracting validation representations...")
    val_reps = []
    val_labels_raw = []

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"Val L{layer_num}"):
            combined = combine_sentences(val_dataset[idx])
            label = val_dataset[idx]["label"]

            tokens = model.to_tokens(combined)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            val_reps.append(pooled_flat)
            val_labels_raw.append(label)

    X_val = np.array(val_reps)
    y_val = np.array(val_labels_raw)

    print("Training classifier...")
    clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
    clf.fit(X_train, y_train)

    predictions = clf.predict(X_val)

    acc = accuracy_score(y_val, predictions)
    p, r, f1, _ = precision_recall_fscore_support(y_val, predictions, average='binary')
    cm = confusion_matrix(y_val, predictions)

    layer_performance[layer_num] = {
        "accuracy": acc,
        "precision": p,
        "recall": r,
        "f1_score": f1,
        "improvement": acc - baseline_acc,
        "confusion_matrix": cm,
    }

    print(f"Layer {layer_num} Accuracy: {acc:.4f} ({acc*100:.2f}%)")
    print(f"F1-Score: {f1:.4f}")
    print(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%")

    # ----------------------------------------------------------
    # Part B: SAE feature analysis
    # ----------------------------------------------------------
    print("\n--- Part B: SAE Feature Analysis ---")

    sae_id = f"blocks.{layer_num}.hook_z"
    try:
        try:
            sae = SAE.from_pretrained(release, sae_id)
        except:
            sae = SAE.from_pretrained(release, sae_id)[0]
        sae.to(device).eval()
        print(f"Loaded SAE: {sae_id}")
    except Exception as e:
        print(f"Could not load SAE for layer {layer_num}: {e}")
        sae_feature_stats[layer_num] = {}
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        continue

    print("Extracting SAE features...")

    pos_active_features = set()
    neg_active_features = set()

    pos_feature_activations = {}
    neg_feature_activations = {}

    pos_feature_counts = Counter()
    neg_feature_counts = Counter()

    total_pos = 0
    total_neg = 0

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"SAE L{layer_num}"):
            combined = combine_sentences(val_dataset[idx])
            label = val_dataset[idx]["label"]

            tokens = model.to_tokens(combined)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])

            layer_acts = cache[hook_name]
            sae_acts = sae.encode(layer_acts)

            pooled_features = sae_acts.mean(dim=1)[0].cpu().numpy().flatten()

            active_indices = np.where(pooled_features > 0)[0]
            active_values = pooled_features[active_indices]

            if label == 1:
                total_pos += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    pos_active_features.add(feat_idx)
                    pos_feature_counts[feat_idx] += 1
                    pos_feature_activations.setdefault(feat_idx, []).append(act_val)
            else:
                total_neg += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    neg_active_features.add(feat_idx)
                    neg_feature_counts[feat_idx] += 1
                    neg_feature_activations.setdefault(feat_idx, []).append(act_val)

    common_features = pos_active_features & neg_active_features
    pos_only_features = pos_active_features - neg_active_features
    neg_only_features = neg_active_features - pos_active_features

    pos_avg_activations = {f: np.mean(acts) for f, acts in pos_feature_activations.items()}
    neg_avg_activations = {f: np.mean(acts) for f, acts in neg_feature_activations.items()}

    top5_pos_by_activation = sorted(
        pos_avg_activations.items(), key=lambda x: x[1], reverse=True
    )[:5]
    top5_neg_by_activation = sorted(
        neg_avg_activations.items(), key=lambda x: x[1], reverse=True
    )[:5]

    topk_pos_by_frequency = pos_feature_counts.most_common(TOP_K)
    topk_neg_by_frequency = neg_feature_counts.most_common(TOP_K)

    top5_pos_by_frequency = pos_feature_counts.most_common(5)
    top5_neg_by_frequency = neg_feature_counts.most_common(5)

    topk_pos_ids = {feat for feat, _ in topk_pos_by_frequency}
    topk_neg_ids = {feat for feat, _ in topk_neg_by_frequency}
    topk_common_ids = topk_pos_ids & topk_neg_ids

    sae_feature_stats[layer_num] = {
        "total_pos_features": len(pos_active_features),
        "total_neg_features": len(neg_active_features),
        "common_features": len(common_features),
        "pos_only_features": len(pos_only_features),
        "neg_only_features": len(neg_only_features),
        "top5_pos_by_activation": top5_pos_by_activation,
        "top5_neg_by_activation": top5_neg_by_activation,
        "top5_pos_by_frequency": top5_pos_by_frequency,
        "top5_neg_by_frequency": top5_neg_by_frequency,
        "topk_pos_by_frequency": topk_pos_by_frequency,
        "topk_neg_by_frequency": topk_neg_by_frequency,
        "topk_common_ids": topk_common_ids,
        "total_pos_samples": total_pos,
        "total_neg_samples": total_neg,
    }

    # ----------------------------------------------------------
    # Save per-layer analysis
    # ----------------------------------------------------------
    with open(f"/kaggle/working/results_mrpc/layer_{layer_num}_complete_analysis.txt", "w") as f:
        f.write("="*70 + "\n")
        f.write(f"LAYER {layer_num} - COMPLETE ANALYSIS (GPT-2 Small on MRPC)\n")
        f.write("="*70 + "\n\n")

        f.write("A. LAYER PERFORMANCE (Raw Representations)\n")
        f.write("-"*70 + "\n")
        f.write(f"Accuracy: {acc:.4f} ({acc*100:.2f}%)\n")
        f.write(f"Precision: {p:.4f}\n")
        f.write(f"Recall: {r:.4f}\n")
        f.write(f"F1-Score: {f1:.4f}\n")
        f.write(f"Baseline Accuracy: {baseline_acc:.4f}\n")
        f.write(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%\n\n")

        f.write("Confusion Matrix:\n")
        f.write("              Predicted\n")
        f.write("         Not-Para  Para\n")
        f.write(f"Actual 0  [{cm[0,0]:5d}  {cm[0,1]:5d}]\n")
        f.write(f"       1  [{cm[1,0]:5d}  {cm[1,1]:5d}]\n\n")

        f.write("B. SAE FEATURE COUNTS\n")
        f.write("-"*70 + "\n")
        f.write(f"Total PARAPHRASE samples: {total_pos}\n")
        f.write(f"Total NON-PARAPHRASE samples: {total_neg}\n\n")
        f.write(f"Total features activated for PARAPHRASE: {len(pos_active_features)}\n")
        f.write(f"Total features activated for NON-PARAPHRASE: {len(neg_active_features)}\n")
        f.write(f"COMMON features: {len(common_features)}\n")
        f.write(f"UNIQUE to PARAPHRASE: {len(pos_only_features)}\n")
        f.write(f"UNIQUE to NON-PARAPHRASE: {len(neg_only_features)}\n\n")

        f.write("C. TOP 5 MOST ACTIVATING FEATURES (PARAPHRASE)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, avg_act) in enumerate(top5_pos_by_activation, 1):
            freq = pos_feature_counts[feat_idx]
            pct = freq / total_pos * 100 if total_pos > 0 else 0.0
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(
                f"{rank}. Feature {feat_idx}: avg_activation={avg_act:.4f}, "
                f"frequency={freq}/{total_pos} ({pct:.1f}%) | {status}\n"
            )

        f.write("\nD. TOP 5 MOST ACTIVATING FEATURES (NON-PARAPHRASE)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, avg_act) in enumerate(top5_neg_by_activation, 1):
            freq = neg_feature_counts[feat_idx]
            pct = freq / total_neg * 100 if total_neg > 0 else 0.0
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(
                f"{rank}. Feature {feat_idx}: avg_activation={avg_act:.4f}, "
                f"frequency={freq}/{total_neg} ({pct:.1f}%) | {status}\n"
            )

        f.write("\nE. TOP 5 MOST FREQUENT FEATURES (PARAPHRASE)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, count) in enumerate(top5_pos_by_frequency, 1):
            pct = count / total_pos * 100 if total_pos > 0 else 0.0
            avg_act = pos_avg_activations.get(feat_idx, 0.0)
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(
                f"{rank}. Feature {feat_idx}: count={count} ({pct:.1f}%), "
                f"avg_activation={avg_act:.4f} | {status}\n"
            )

        f.write("\nF. TOP 5 MOST FREQUENT FEATURES (NON-PARAPHRASE)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, count) in enumerate(top5_neg_by_frequency, 1):
            pct = count / total_neg * 100 if total_neg > 0 else 0.0
            avg_act = neg_avg_activations.get(feat_idx, 0.0)
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(
                f"{rank}. Feature {feat_idx}: count={count} ({pct:.1f}%), "
                f"avg_activation={avg_act:.4f} | {status}\n"
            )

        f.write(f"\nG. TOP {TOP_K} MOST FREQUENT FEATURES (PARAPHRASE)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, count) in enumerate(topk_pos_by_frequency, 1):
            pct = count / total_pos * 100 if total_pos > 0 else 0.0
            f.write(f"{rank}. Feature {feat_idx}: count={count} ({pct:.1f}%)\n")

        f.write(f"\nH. TOP {TOP_K} MOST FREQUENT FEATURES (NON-PARAPHRASE)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, count) in enumerate(topk_neg_by_frequency, 1):
            pct = count / total_neg * 100 if total_neg > 0 else 0.0
            f.write(f"{rank}. Feature {feat_idx}: count={count} ({pct:.1f}%)\n")

        f.write(f"\nI. COMMON FEATURES AMONG TOP-{TOP_K}\n")
        f.write("-"*70 + "\n")
        f.write(f"Count: {len(topk_common_ids)}\n")
        if len(topk_common_ids) > 0:
            f.write("IDs: " + ", ".join(str(x) for x in sorted(list(topk_common_ids))[:20]) + "\n")

    print(f"✓ Layer {layer_num} complete analysis saved")

    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ================================================================
# 7. FINAL SUMMARY
# ================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

best_layer = max(layer_performance.items(), key=lambda x: x[1]["accuracy"])
worst_layer = min(layer_performance.items(), key=lambda x: x[1]["accuracy"])

print(
    f"\n{'Layer':<6} {'Acc%':<8} {'F1':<8} {'Improv':<10} "
    f"{'Para Feat':<10} {'Non-Para Feat':<15} {'Common':<10}"
)
print("-"*100)
for layer_num in sorted(layer_performance.keys()):
    perf = layer_performance[layer_num]
    stats = sae_feature_stats.get(layer_num, {})
    print(
        f"L{layer_num:<5} {perf['accuracy']*100:5.2f}%   {perf['f1_score']:.4f}  "
        f"{perf['improvement']*100:+6.2f}%    "
        f"{stats.get('total_pos_features', 'N/A'):<10} "
        f"{stats.get('total_neg_features', 'N/A'):<15} "
        f"{stats.get('common_features', 'N/A'):<10}"
    )

print(f"\n✓ Best layer: Layer {best_layer[0]} ({best_layer[1]['accuracy']*100:.2f}% accuracy, F1={best_layer[1]['f1_score']:.4f})")
print(f"✓ Worst layer: Layer {worst_layer[0]} ({worst_layer[1]['accuracy']*100:.2f}% accuracy)")
# Get top 2 layers by accuracy
top_2_layers = sorted(layer_performance.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:2]
top_2_layer_nums = [layer_num for layer_num, _ in top_2_layers]

print(f"\nTop 2 layers for interpretability: {top_2_layer_nums}")

with open("/kaggle/working/results_mrpc/final_summary.txt", "w") as f:
    f.write("="*70 + "\n")
    f.write("FINAL SUMMARY - LAYER-WISE ANALYSIS (GPT-2 Small on MRPC)\n")
    f.write("="*70 + "\n\n")

    f.write("1. BASELINE (Zero-Shot Prompting)\n")
    f.write("-"*70 + "\n")
    f.write(f"Accuracy: {baseline_acc*100:.2f}%\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n\n")

    f.write("2. LAYER-WISE PERFORMANCE & SAE FEATURE STATISTICS\n")
    f.write("-"*70 + "\n")
    f.write(
        f"{'Layer':<6} {'Acc%':<8} {'F1':<8} {'Improv':<10} "
        f"{'Para':<10} {'Non-Para':<10} {'Common':<10}\n"
    )
    f.write("-"*90 + "\n")
    for layer_num in sorted(layer_performance.keys()):
        perf = layer_performance[layer_num]
        stats = sae_feature_stats.get(layer_num, {})
        f.write(
            f"L{layer_num:<5} {perf['accuracy']*100:5.2f}%   {perf['f1_score']:.4f}  "
            f"{perf['improvement']*100:+6.2f}%    "
            f"{stats.get('total_pos_features', 'N/A'):<10} "
            f"{stats.get('total_neg_features', 'N/A'):<10} "
            f"{stats.get('common_features', 'N/A'):<10}\n"
        )

    f.write("\n3. KEY FINDINGS\n")
    f.write("-"*70 + "\n")
    f.write(f"Best Performing Layer: Layer {best_layer[0]}\n")
    f.write(f"  Accuracy: {best_layer[1]['accuracy']*100:.2f}%\n")
    f.write(f"  F1-Score: {best_layer[1]['f1_score']:.4f}\n")
    f.write(f"  Improvement: {best_layer[1]['improvement']*100:+.2f}%\n\n")
    f.write(f"Worst Performing Layer: Layer {worst_layer[0]}\n")
    f.write(f"  Accuracy: {worst_layer[1]['accuracy']*100:.2f}%\n\n")
    f.write(f"Top 2 Layers: {top_2_layer_nums}\n\n")
    f.write("INTERPRETATION:\n")
    f.write("- MRPC is paraphrase detection (sentence pairs)\n")
    f.write("- SAE features capture semantic similarity patterns\n")
    f.write("- Para-only features = semantic equivalence detectors\n")
    f.write("- Non-para features = contradiction/divergence signals\n")

print("\n✓ All results saved to /kaggle/working/results_mrpc/")
print("="*70)

# ================================================================
# 8. GROQ INTERPRETABILITY FOR TOP 2 LAYERS
# ================================================================
print("\n" + "="*70)
print("STEP 3: GROQ INTERPRETABILITY FOR TOP 2 LAYERS")
print("="*70)

# Dataset wrapper for val_combined
class TextDataset(torch.utils.data.Dataset):
    def __init__(self, texts):
        self.texts = texts
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        return self.texts[idx]

val_text_ds = TextDataset(val_combined)

# SAE wrapper
class SAEWithActs(torch.nn.Module):
    def __init__(self, sae_model):
        super().__init__()
        self.sae = sae_model
        self.activations = None
        hook_module = getattr(self.sae, "hook_sae_acts_post", None)
        if hook_module is None:
            raise ValueError("hook_sae_acts_post not found on SAE model.")
        hook_module.register_forward_hook(self._hook)

    def _hook(self, module, inp, out):
        self.activations = out.detach()

    def forward(self, sae_input):
        _ = self.sae(sae_input)
        return self.activations

    def get_acts(self, sae_input):
        self.eval()
        with torch.no_grad():
            _ = self.forward(sae_input)
            return self.activations

# Global feature counting
def top_k_global_features_for_layer(layer_num, sae_model, k=10, max_batches=None):
    sae_wrap = SAEWithActs(sae_model).to(device)
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    counter = Counter()
    dataloader = DataLoader(val_text_ds, batch_size=32)

    for i, batch_texts in enumerate(tqdm(dataloader, desc=f"Counting features L{layer_num}")):
        if max_batches is not None and i >= max_batches:
            break

        batch_tokens = model.to_tokens(batch_texts)
        with torch.no_grad():
            _, cache = model.run_with_cache(batch_tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]
            latents = sae_wrap.get_acts(hook_acts)

        active = (latents > 0).any(dim=1)
        for row in active:
            idxs = row.nonzero(as_tuple=True)[0].tolist()
            counter.update(idxs)

    topk = counter.most_common(k)
    print(f"Layer {layer_num} top-{k} global features (idx, count): {topk}")
    return [idx for idx, _ in topk]

# Feature extraction
def extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=None):
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    acts_all, texts_all = [], []
    dataloader = DataLoader(val_text_ds, batch_size=32)

    for i, batch_texts in enumerate(tqdm(dataloader, desc=f"Extracting L{layer_num} F{feature_idx}")):
        if max_batches is not None and i >= max_batches:
            break

        batch_tokens = model.to_tokens(batch_texts)
        with torch.no_grad():
            _, cache = model.run_with_cache(batch_tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]
            latents = sae_wrap.get_acts(hook_acts)

        feat_acts = latents[:, :, feature_idx].detach().cpu().numpy()
        for a_sent, txt in zip(feat_acts, batch_texts):
            acts_all.append(a_sent)
            texts_all.append(txt)

    return acts_all, texts_all

def select_top(acts, texts, n=20):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:n]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

def llama_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a sparse autoencoder feature from GPT-2 on MRPC paraphrase detection.\n"
        "Each text below is a concatenated sentence pair; activations show per-token strength.\n\n"
        "From this data, give one concise sentence describing what pattern or concept "
        "this feature responds to.\n\n"
    )
    for i, (txt, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{txt}\"\nActivations: {acts.tolist()}\n\n"
    prompt += "Your explanation:"
    return chat_with_llama(prompt)

def llama_activation_score(sentence, interpretation):
    prompt = (
        f'Feature interpretation:\n"{interpretation}"\n\n'
        "On a scale from 0 (not active) to 10 (very active), estimate how strongly "
        "this feature activates on the following sentence pair. Respond with only a single number.\n\n"
        f'Text: \"{sentence}\"\nActivation:'
    )
    resp = chat_with_llama(prompt, max_tokens=16)
    m = re.search(r"\d+(\.\d+)?", resp)
    return float(m.group()) if m else 0.0

def pearson_score(actual_acts, pred_scores):
    actual = np.array([np.mean(a) for a in actual_acts]) * 10.0
    pred = np.array(pred_scores)
    corr, _ = pearsonr(actual, pred)
    return corr

def save_interp_report(path, layer_num, feature_idx, interpretation,
                       eval_texts, eval_acts, pred_scores, corr):
    with open(path, "a", encoding="utf-8") as f:
        f.write(f"Layer {layer_num}, Feature {feature_idx}\n")
        f.write("-"*70 + "\n")
        f.write("Interpretation:\n" + interpretation.strip() + "\n\n")
        f.write("Evaluation sentence pairs:\n")
        for i, (txt, acts, pred) in enumerate(zip(eval_texts, eval_acts, pred_scores), 1):
            actual = np.mean(acts) * 10.0
            f.write(f"{i}. {txt}\n   Actual={actual:.3f}, Pred={pred:.3f}\n")
        f.write(f"\nPearson correlation: {corr:.4f}\n")
        f.write("="*70 + "\n\n")

# Pipeline for one feature
def interpretability_for_feature(layer_num, sae_model, feature_idx, report_path, max_batches=None):
    sae_wrap = SAEWithActs(sae_model).to(device)

    acts_all, texts_all = extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=max_batches)

    top_acts, top_texts = select_top(acts_all, texts_all, n=20)
    interp_acts, interp_texts = top_acts[:5], top_texts[:5]

    print(f"\nTop 5 sentence pairs for L{layer_num} feature {feature_idx}:")
    for i, s in enumerate(interp_texts, 1):
        print(f"{i}. {s}")

    interpretation = llama_interpretation(interp_texts, interp_acts)
    print("\nInterpretation:", interpretation)

    used = set(interp_texts)
    rest = [(a, t) for a, t in zip(acts_all, texts_all) if t not in used]
    if not rest:
        print("No remaining sentences for evaluation; skipping feature.")
        return None

    rest_acts, rest_texts = zip(*rest)
    rest_acts, rest_texts = list(rest_acts), list(rest_texts)
    scores = [np.sum(a) for a in rest_acts]
    idxs = np.argsort(scores)[::-1][:5]
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    print("\nEvaluation sentence pairs:")
    for i, s in enumerate(eval_texts, 1):
        print(f"{i}. {s}")

    pred_scores = [llama_activation_score(s, interpretation) for s in eval_texts]
    corr = pearson_score(eval_acts, pred_scores)
    print(f"Pearson correlation for L{layer_num} feature {feature_idx}: {corr:.4f}")

    save_interp_report(report_path, layer_num, feature_idx, interpretation,
                       eval_texts, eval_acts, pred_scores, corr)

    return corr

# Run interpretability for top 2 layers
INTERP_TOP_K = 10

for layer_idx in top_2_layer_nums:
    print("\n" + "="*70)
    print(f"LAYER {layer_idx}: GROQ INTERPRETABILITY (Top-{INTERP_TOP_K} global features)")
    print("="*70)

    sae_id = f"blocks.{layer_idx}.hook_z"
    try:
        sae_model = SAE.from_pretrained(release, sae_id)
    except:
        sae_model = SAE.from_pretrained(release, sae_id)[0]
    sae_model.to(device).eval()

    topk_feats = top_k_global_features_for_layer(
        layer_num=layer_idx,
        sae_model=sae_model,
        k=INTERP_TOP_K,
        max_batches=None,
    )

    report_file = f"/kaggle/working/results_mrpc/layer{layer_idx}_top{INTERP_TOP_K}_interpretability.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"INTERPRETABILITY REPORT – MRPC Layer {layer_idx}, top-{INTERP_TOP_K} global features\n")
        f.write("="*70 + "\n\n")

    for feat in topk_feats:
        print(f"\n--- Layer {layer_idx}, global feature {feat} ---")
        interpretability_for_feature(
            layer_num=layer_idx,
            sae_model=sae_model,
            feature_idx=feat,
            report_path=report_file,
            max_batches=None,
        )

    print(f"\nLayer {layer_idx} interpretability: finished. Report saved to {report_file}")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

print("\n" + "="*70)
print("✓ MRPC ANALYSIS COMPLETE (with Groq interpretability for top 2 layers)")
print("="*70)