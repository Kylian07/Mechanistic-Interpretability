# -*- coding: utf-8 -*-
"""Qwen3 0.6B_Pretrained

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/qwen3-0-6b-pretrained.5cbbea20-c0fb-46f4-8207-3bdb619db95c.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251222/auto/storage/goog4_request%26X-Goog-Date%3D20251222T151647Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D30f930cf5ba811e1a5017ddba9816d5d03a5428f025fe3df628c658a263a70be07c7e8a79e9cad55ccbe49281425312d9666ce736b84dcdbe287654867602cc96f68c0f881544f608323db90775027a052718481fb94b81a42f905ddfa8b822b5d1abb780668318fd84ad08ff46df32aea4925bc8c4ab3559cfeb0ac96059fc43580e769c5410078f6a59ab8c9cf3277dbb9915740462d6f5ed304d3482d16f4a3d12cb6fcce8aa06145677bb9486a910f34ab80fac2049a006a9f9f9d9a8e4615503c484792c95dd9c5c1d9fe7b501876f7e8bfc1dbcc34d0d11fe3db4e94e087b7e1ad2972efd8a55462fc0665dc76ec8c41031cdb53192a3f08bd43a880b7
"""

# ================================================================
# 0. INSTALLS (only once per notebook)
# ================================================================
!pip install -q sae-lens transformers datasets torch groq

# ================================================================
# 1. IMPORTS & BASIC SETUP
# ================================================================
import os, re, numpy as np, torch, gc
from collections import Counter
from datasets import load_dataset
from torch.utils.data import DataLoader
from sae_lens import SAE
from transformers import AutoModelForCausalLM, AutoTokenizer
from groq import Groq
from scipy.stats import pearsonr
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# ================================================================
# 2. GROQ / LLAMA-3 CLIENT
# ================================================================
from kaggle_secrets import UserSecretsClient
user_secrets = UserSecretsClient()
os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")

GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
if GROQ_API_KEY is None:
    raise ValueError("Set GROQ_API_KEY in Kaggle secrets as GROQ_API_KEY.")

client = Groq(api_key=GROQ_API_KEY)

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful interpretability assistant."},
            {"role": "user", "content": prompt},
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# ================================================================
# 3. LOAD QWEN3-0.6B-BASE & SST-2 VAL
# ================================================================
base_model_id = "Qwen/Qwen3-0.6B-Base"
print(f"Loading model: {base_model_id} ...")

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.bfloat16,
    device_map=device,
)
model.eval()

n_layers = model.config.num_hidden_layers
print(f"Model has {n_layers} layers (0-{n_layers-1})")

print("Loading SST-2 validation split...")
val_ds = load_dataset("glue", "sst2", split="validation")
val_sentences = val_ds["sentence"]
print("Val size:", len(val_sentences))

# ================================================================
# 4. DATASET: TEXT DATASET
# ================================================================
class TextDataset(torch.utils.data.Dataset):
    def __init__(self, sentences):
        self.sentences = sentences
    def __len__(self):
        return len(self.sentences)
    def __getitem__(self, idx):
        return self.sentences[idx]

val_text_ds = TextDataset(val_sentences)

# ================================================================
# 5. GET LAYER REPRESENTATIONS (HIDDEN STATES)
# ================================================================
def get_layer_reps_batch(text_batch, layer_idx, max_length=128):
    enc = tokenizer(
        list(text_batch),
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_length,
    ).to(device)

    with torch.no_grad():
        out = model(
            **enc,
            output_hidden_states=True,
            use_cache=False,
        )

    hidden_states = out.hidden_states  # tuple len n_layers+1
    layer_h = hidden_states[layer_idx + 1]  # (B, seq, d_model)
    return layer_h, enc["attention_mask"]  # mask: (B, seq)

# ================================================================
# 6. SAE WRAPPER
# ================================================================
class SAEWithActs(torch.nn.Module):
    def __init__(self, sae_model):
        super().__init__()
        self.sae = sae_model
        self.activations = None
        hook_module = getattr(self.sae, "hook_sae_acts_post", None)
        if hook_module is None:
            raise ValueError("hook_sae_acts_post not found on SAE model.")
        hook_module.register_forward_hook(self._hook)

    def _hook(self, module, inp, out):
        self.activations = out.detach()

    def forward(self, sae_input):
        _ = self.sae(sae_input)
        return self.activations

    def get_acts(self, sae_input):
        self.eval()
        with torch.no_grad():
            _ = self.forward(sae_input)
            return self.activations

# ================================================================
# 7. TOP-K GLOBAL FEATURES FOR QWEN3 LAYER
# ================================================================
def top_k_global_features_for_layer_qwen(layer_num, sae_model, k=10, max_batches=None):
    sae_wrap = SAEWithActs(sae_model).to(device)
    counter = Counter()
    dataloader = DataLoader(val_text_ds, batch_size=32, shuffle=False)

    for i, batch_texts in enumerate(tqdm(dataloader, desc=f"Counting features L{layer_num}")):
        if max_batches is not None and i >= max_batches:
            break

        layer_h, attn_mask = get_layer_reps_batch(batch_texts, layer_num)
        latents = sae_wrap.get_acts(layer_h)

        if attn_mask is not None:
            mask = attn_mask.unsqueeze(-1)
            latents = latents * mask

        active = (latents > 0).any(dim=1)  # (B, d_sae)
        for row in active:
            idxs = row.nonzero(as_tuple=True)[0].tolist()
            counter.update(idxs)

    topk = counter.most_common(k)
    print(f"Layer {layer_num} top-{k} global features (idx, count): {topk}")
    return [idx for idx, _ in topk]

# ================================================================
# 8. INTERPRETABILITY HELPERS
# ================================================================
def extract_feature_acts_qwen(layer_num, sae_wrap, feature_idx, max_batches=None):
    acts_all, texts_all = [], []
    dataloader = DataLoader(val_text_ds, batch_size=32, shuffle=False)

    for i, batch_texts in enumerate(tqdm(dataloader, desc=f"Extracting L{layer_num} F{feature_idx}")):
        if max_batches is not None and i >= max_batches:
            break

        layer_h, attn_mask = get_layer_reps_batch(batch_texts, layer_num)
        latents = sae_wrap.get_acts(layer_h)

        if attn_mask is not None:
            mask = attn_mask.unsqueeze(-1)
            latents = latents * mask

        feat_acts = latents[:, :, feature_idx].detach().cpu().numpy()
        for a_sent, txt in zip(feat_acts, batch_texts):
            acts_all.append(a_sent)
            texts_all.append(txt)

    return acts_all, texts_all

def select_top(acts, texts, n=20):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:n]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

def llama_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a sparse autoencoder feature from Qwen3-0.6B.\n"
        "Each sentence below is accompanied by per-token activation strengths for this feature; "
        "larger numbers indicate stronger activation.\n\n"
        "From this data, give one concise sentence describing what pattern or concept "
        "this feature responds to.\n\n"
    )
    for i, (txt, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{txt}\"\nActivations: {acts.tolist()}\n\n"
    prompt += "Your explanation:"
    return chat_with_llama(prompt)

def llama_activation_score(sentence, interpretation):
    prompt = (
        f'Feature interpretation:\n"{interpretation}"\n\n'
        "On a scale from 0 (not active) to 10 (very active), estimate how strongly "
        "this feature activates on the following sentence. Respond with only a single number.\n\n"
        f'Sentence: \"{sentence}\"\nActivation:'
    )
    resp = chat_with_llama(prompt, max_tokens=16)
    m = re.search(r"\d+(\.\d+)?", resp)
    return float(m.group()) if m else 0.0

def pearson_score(actual_acts, pred_scores):
    actual = np.array([np.mean(a) for a in actual_acts]) * 10.0
    pred = np.array(pred_scores)
    corr, _ = pearsonr(actual, pred)
    return corr

def save_report(path, layer_num, feature_idx, interpretation,
                eval_texts, eval_acts, pred_scores, corr):
    with open(path, "a", encoding="utf-8") as f:
        f.write(f"Layer {layer_num}, Feature {feature_idx}\n")
        f.write("-"*70 + "\n")
        f.write("Interpretation:\n" + interpretation.strip() + "\n\n")
        f.write("Evaluation sentences:\n")
        for i, (txt, acts, pred) in enumerate(zip(eval_texts, eval_acts, pred_scores), 1):
            actual = np.mean(acts) * 10.0
            f.write(f"{i}. {txt}\n   Actual={actual:.3f}, Pred={pred:.3f}\n")
        f.write(f"\nPearson correlation: {corr:.4f}\n")
        f.write("="*70 + "\n\n")

# ================================================================
# 9. PIPELINE FOR ONE FEATURE (QWEN3)
# ================================================================
def interpretability_for_feature_qwen(layer_num, sae_model, feature_idx, report_path, max_batches=None):
    sae_wrap = SAEWithActs(sae_model).to(device)

    acts_all, texts_all = extract_feature_acts_qwen(layer_num, sae_wrap, feature_idx, max_batches=max_batches)

    top_acts, top_texts = select_top(acts_all, texts_all, n=20)
    interp_acts, interp_texts = top_acts[:5], top_texts[:5]

    print(f"\nTop 5 sentences for L{layer_num} feature {feature_idx}:")
    for i, s in enumerate(interp_texts, 1):
        print(f"{i}. {s}")

    interpretation = llama_interpretation(interp_texts, interp_acts)
    print("\nInterpretation:", interpretation)

    used = set(interp_texts)
    rest = [(a, t) for a, t in zip(acts_all, texts_all) if t not in used]
    if not rest:
        print("No remaining sentences for evaluation; skipping feature.")
        return None

    rest_acts, rest_texts = zip(*rest)
    rest_acts, rest_texts = list(rest_acts), list(rest_texts)
    scores = [np.sum(a) for a in rest_acts]
    idxs = np.argsort(scores)[::-1][:5]
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    print("\nEvaluation sentences:")
    for i, s in enumerate(eval_texts, 1):
        print(f"{i}. {s}")

    pred_scores = [llama_activation_score(s, interpretation) for s in eval_texts]
    corr = pearson_score(eval_acts, pred_scores)
    print(f"Pearson correlation for L{layer_num} feature {feature_idx}: {corr:.4f}")

    save_report(report_path, layer_num, feature_idx, interpretation,
                eval_texts, eval_acts, pred_scores, corr)

    return corr

# ================================================================
# 10. MAIN: QWEN3 LAYERS 12 & 14, TOP-10 GLOBAL FEATURES
# ================================================================
release = "mwhanna-qwen3-0.6b-transcoders-lowl0"
layers_to_analyze = [12, 14]
TOP_K = 10

for layer in layers_to_analyze:
    print("\n" + "="*70)
    print(f"LAYER {layer}: loading pretrained SAE and finding top-{TOP_K} global features")
    print("="*70)

    sae_id = f"layer_{layer}"
    try:
        sae_model = SAE.from_pretrained(release, sae_id)
    except:
        sae_model = SAE.from_pretrained(release, sae_id)[0]
    sae_model.to(device).eval()

    topk_feats = top_k_global_features_for_layer_qwen(
        layer_num=layer,
        sae_model=sae_model,
        k=TOP_K,
        max_batches=None,
    )

    report_file = f"qwen3_layer{layer}_top{TOP_K}_global_features_interpretability.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"INTERPRETABILITY REPORT â€“ Qwen3-0.6B, Layer {layer}, top-{TOP_K} global frequent features\n")
        f.write("="*70 + "\n\n")

    for feat in topk_feats:
        print(f"\n--- Layer {layer}, global feature {feat} ---")
        interpretability_for_feature_qwen(
            layer_num=layer,
            sae_model=sae_model,
            feature_idx=feat,
            report_path=report_file,
            max_batches=None,
        )

    print(f"\nLayer {layer}: finished. Report saved to {report_file}")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

