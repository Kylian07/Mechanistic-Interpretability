# -*- coding: utf-8 -*-
"""Pretrained GPT2_SAE_SAElens

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/pretrained-gpt2-sae-saelens.23486761-8142-456d-b73b-2f042327f5bb.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251204/auto/storage/goog4_request%26X-Goog-Date%3D20251204T214846Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5e9b89da980671b45242c9e4de2d8e460967ef17f06cde1dab166bb1c98e7d921a9875e9ccd237602afbaf52aa26835bac232c688ed9931b5f728cfb073ef05439b26acbabc5a792018356b849a0e0f0fc063c95f27bc7821cfaa9c75b4e602681560e5cab83c069f740ab36861965933f99716c0944a3307503b9d2b8f1f43496c7759aadd0a5154c9bddcd31b605e358cd4b72dbe235957f78a8ba361965d73ccb9ebd01af475e03ac4d81a04a9b9624134a72e3c202899d1960769a25053b0cfe7d707a7219e0ccbc70a10659f39ba71dc7fad96ac0d591aa37f66f1668bb2fba4051077c8c5f5e84c3819046a7355f2b16a1f9f0e73baf487cd6c43c4883
"""

!pip install sae-lens
!pip install groq

# ================== IMPORTS & BASIC SETUP ==================
import os, re, numpy as np, torch, gc
from collections import Counter
from datasets import load_dataset
from torch.utils.data import DataLoader
from sae_lens import SAE
from transformer_lens import HookedTransformer
from groq import Groq
from scipy.stats import pearsonr
from tqdm import tqdm
from huggingface_hub import login

# Memory optimization
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# ================== HUGGING FACE + GROQ AUTH ==================
from kaggle_secrets import UserSecretsClient
user_secrets = UserSecretsClient()

# HF login for Gemma-2B (gated model)
hf_token = user_secrets.get_secret("HF_TOKEN")
login(token=hf_token)
print("✓ Logged in to Hugging Face")

# Groq API for Llama
os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
if GROQ_API_KEY is None:
    raise ValueError("Set GROQ_API_KEY in your Kaggle secrets.")

client = Groq(api_key=GROQ_API_KEY)

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful interpretability assistant."},
            {"role": "user", "content": prompt},
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# ================== LOAD GEMMA-2B & SST-2 VAL ==================
print("Loading HookedTransformer (Gemma-2B base)...")
model = HookedTransformer.from_pretrained("google/gemma-2b", device=device)
model.eval()

print("Loading SST-2 validation split...")
val_ds = load_dataset("glue", "sst2", split="validation")

# Convert Dataset column to Python list
val_sentences = list(val_ds["sentence"])
print(f"Loaded {len(val_sentences)} validation sentences")

# Tokenize with Gemma (prepend_bos=True)
tokens_all = model.to_tokens(val_sentences, prepend_bos=True, truncate=True)
print(f"Tokenized shape: {tokens_all.shape}")

class TokenDataset(torch.utils.data.Dataset):
    def __init__(self, tokens):
        self.tokens = tokens
    def __len__(self):
        return self.tokens.shape[0]
    def __getitem__(self, idx):
        return self.tokens[idx]

val_tok_ds = TokenDataset(tokens_all)

# Clear memory after tokenization
torch.cuda.empty_cache()
gc.collect()

# ================== SAE WRAPPER ==================
class SAEWithActs(torch.nn.Module):
    """Wrap an SAE to expose latent activations via hook_sae_acts_post."""
    def __init__(self, sae_model):
        super().__init__()
        self.sae = sae_model
        self.activations = None
        hook_module = getattr(self.sae, "hook_sae_acts_post", None)
        if hook_module is None:
            raise ValueError("hook_sae_acts_post not found on SAE model.")
        hook_module.register_forward_hook(self._hook)

    def _hook(self, module, inp, out):
        self.activations = out.detach()

    def forward(self, sae_input):
        _ = self.sae(sae_input)
        return self.activations

    def get_acts(self, sae_input):
        self.eval()
        with torch.no_grad():
            _ = self.forward(sae_input)
            return self.activations

# ================== INTERPRETABILITY HELPERS (MEMORY OPTIMIZED) ==================
def extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=None):
    """
    Memory-optimized: smaller batches + aggressive cache clearing.
    Uses blocks.{layer}.attn.hook_z for Gemma Scope SAEs.
    """
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    acts_all, texts_all = [], []
    
    # REDUCED batch size from 32 to 8 for memory
    dataloader = DataLoader(val_tok_ds, batch_size=8)

    for i, batch_tokens in enumerate(tqdm(dataloader, desc=f"Extracting L{layer_num} F{feature_idx}")):
        if max_batches is not None and i >= max_batches:
            break

        batch_tokens = batch_tokens.to(device)
        with torch.no_grad():
            _, cache = model.run_with_cache(batch_tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]                 # (B, seq, d_model)
            latents = sae_wrap.get_acts(hook_acts)       # (B, seq, d_sae)

            # Move to CPU immediately
            feat_acts = latents[:, :, feature_idx].cpu().numpy()  # (B, seq)
            
            # Aggressive cleanup
            del cache, hook_acts, latents
            torch.cuda.empty_cache()

        start = i * dataloader.batch_size
        batch_texts = val_sentences[start : start + feat_acts.shape[0]]

        for a_sent, txt in zip(feat_acts, batch_texts):
            acts_all.append(a_sent)
            texts_all.append(txt)

    return acts_all, texts_all

def select_top(acts, texts, n=20):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:n]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

def llama_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a sparse autoencoder feature from Gemma-2B trained on sentiment data (SST-2).\n"
        "Each sentence below is accompanied by per-token activation strengths for this feature; "
        "larger numbers indicate stronger activation.\n\n"
        "From this data, give ONE concise sentence describing what sentiment pattern or linguistic concept "
        "this feature responds to (e.g., 'Detects positive emotion words', 'Activates on negation with positive adjectives').\n\n"
    )
    for i, (txt, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{txt}\"\nActivations: {acts.tolist()}\n\n"
    prompt += "Your explanation:"
    return chat_with_llama(prompt)

def llama_activation_score(sentence, interpretation):
    prompt = (
        f'Feature interpretation:\n"{interpretation}"\n\n'
        "On a scale from 0 (not active) to 10 (very active), estimate how strongly "
        "this feature activates on the following sentence. Respond with only a single number.\n\n"
        f'Sentence: \"{sentence}\"\nActivation:'
    )
    resp = chat_with_llama(prompt, max_tokens=16)
    m = re.search(r"\d+(\.\d+)?", resp)
    return float(m.group()) if m else 0.0

def pearson_score(actual_acts, pred_scores):
    actual = np.array([np.mean(a) for a in actual_acts]) * 10.0
    pred = np.array(pred_scores)
    corr, _ = pearsonr(actual, pred)
    return corr

def save_report(path, layer_num, feature_idx, interpretation,
                eval_texts, eval_acts, pred_scores, corr):
    with open(path, "a", encoding="utf-8") as f:
        f.write(f"Layer {layer_num}, Feature {feature_idx}\n")
        f.write("-"*70 + "\n")
        f.write("Interpretation:\n" + interpretation.strip() + "\n\n")
        f.write("Evaluation sentences:\n")
        for i, (txt, acts, pred) in enumerate(zip(eval_texts, eval_acts, pred_scores), 1):
            actual = np.mean(acts) * 10.0
            f.write(f"{i}. {txt}\n   Actual={actual:.3f}, Pred={pred:.3f}\n")
        f.write(f"\nPearson correlation: {corr:.4f}\n")
        f.write("="*70 + "\n\n")

# ================== PIPELINE FOR ONE FEATURE ==================
def interpretability_for_feature(layer_num, sae_model, feature_idx, report_path, max_batches=None):
    sae_wrap = SAEWithActs(sae_model).to(device)

    # 1) collect activations for full validation set
    acts_all, texts_all = extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=max_batches)

    # 2) choose top-20 sentences; first 5 for interpretation
    top_acts, top_texts = select_top(acts_all, texts_all, n=20)
    interp_acts, interp_texts = top_acts[:5], top_texts[:5]

    print(f"\nTop 5 sentences for L{layer_num} feature {feature_idx}:")
    for i, s in enumerate(interp_texts, 1):
        print(f"{i}. {s}")

    interpretation = llama_interpretation(interp_texts, interp_acts)
    print("\nInterpretation:", interpretation)

    # 3) evaluation on held-out top-5 (excluding prompt sentences)
    used = set(interp_texts)
    rest = [(a, t) for a, t in zip(acts_all, texts_all) if t not in used]
    if not rest:
        print("No remaining sentences for evaluation; skipping feature.")
        return None

    rest_acts, rest_texts = zip(*rest)
    rest_acts, rest_texts = list(rest_acts), list(rest_texts)
    scores = [np.sum(a) for a in rest_acts]
    idxs = np.argsort(scores)[::-1][:5]
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    print("\nEvaluation sentences:")
    for i, s in enumerate(eval_texts, 1):
        print(f"{i}. {s}")

    pred_scores = [llama_activation_score(s, interpretation) for s in eval_texts]
    corr = pearson_score(eval_acts, pred_scores)
    print(f"Pearson correlation for L{layer_num} feature {feature_idx}: {corr:.4f}")

    save_report(report_path, layer_num, feature_idx, interpretation,
                eval_texts, eval_acts, pred_scores, corr)

    # Cleanup after feature
    del sae_wrap
    torch.cuda.empty_cache()
    gc.collect()

    return corr

# ================== MAIN: LAYERS 9 & 10 (BEST GEMMA LAYERS) ==================
release = "gemma-scope-2b-pt-att-canonical"

# Layer 9: 91.74% accuracy, Layer 10: 90.60% accuracy
layers_to_analyze = [9, 10]

# Your top-10 features from final_summary_gemma.txt
layer_features = {
    9: [7, 18, 19, 37, 38, 64, 70, 79, 80, 86],
    10: [437, 1795, 1915, 2226, 2710, 2712, 3748, 4354, 4463, 4966]
}

for layer in layers_to_analyze:
    print("\n" + "="*70)
    print(f"LAYER {layer}: Loading Gemma Scope SAE and analyzing top-10 sentiment features")
    print("="*70)

    # Clear memory before loading SAE
    torch.cuda.empty_cache()
    gc.collect()

    # Load Gemma Scope SAE for this layer
    sae_id = f"layer_{layer}/width_16k/canonical"
    try:
        sae_model = SAE.from_pretrained(release, sae_id)
    except:
        sae_model = SAE.from_pretrained(release, sae_id)[0]
    sae_model.to(device)
    sae_model.eval()
    print(f"✓ Loaded SAE: {sae_id}")

    # Get pre-identified top features from your analysis
    topk_feats = layer_features[layer]
    print(f"Top-10 features for Layer {layer}: {topk_feats}")

    report_file = f"/kaggle/working/gemma_layer{layer}_top10_interpretability.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"GEMMA-2B INTERPRETABILITY REPORT – Layer {layer}, top-10 sentiment features\n")
        f.write("="*70 + "\n\n")

    # Interpret each top feature
    correlations = []
    for feat in topk_feats:
        print(f"\n--- Layer {layer}, feature {feat} ---")
        corr = interpretability_for_feature(
            layer_num=layer,
            sae_model=sae_model,
            feature_idx=feat,
            report_path=report_file,
            max_batches=None,  # Use full validation set; set to 50 if still OOM
        )
        if corr is not None:
            correlations.append(corr)

    # Summary statistics
    mean_corr = np.mean(correlations) if correlations else 0.0
    print(f"\n✓ Layer {layer} finished. Mean Pearson: {mean_corr:.4f}")
    print(f"Report saved to {report_file}")

    with open(report_file, "a", encoding="utf-8") as f:
        f.write("\n" + "="*70 + "\n")
        f.write(f"SUMMARY: Layer {layer}\n")
        f.write(f"Mean Pearson correlation: {mean_corr:.4f}\n")
        f.write(f"Features analyzed: {len(correlations)}/{len(topk_feats)}\n")
        f.write("="*70 + "\n")

    # Cleanup after layer
    del sae_model
    torch.cuda.empty_cache()
    gc.collect()

print("\n" + "="*70)
print("✓ ALL LAYERS COMPLETE")
print("="*70)

