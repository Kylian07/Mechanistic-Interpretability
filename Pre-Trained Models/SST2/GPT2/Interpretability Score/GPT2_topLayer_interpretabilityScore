# -*- coding: utf-8 -*-
!pip install sae-lens
!pip install groq
!pip install sae-lens transformers datasets torch groq -q
# ================== IMPORTS & BASIC SETUP ==================
import os, re, numpy as np, torch
from collections import Counter
from datasets import load_dataset
from torch.utils.data import DataLoader
from sae_lens import SAE
from transformer_lens import HookedTransformer
from groq import Groq
from scipy.stats import pearsonr
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# ================== GROQ / LLAMA-3 CLIENT ==================
# If running on Kaggle, do this once in another cell:
from kaggle_secrets import UserSecretsClient
user_secrets = UserSecretsClient()
os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")

GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
if GROQ_API_KEY is None:
    raise ValueError("Set GROQ_API_KEY in your environment or secrets.")

client = Groq(api_key=GROQ_API_KEY)

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful interpretability assistant."},
            {"role": "user", "content": prompt},
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# ================== LOAD GPT-2 VIA TRANSFORMERLENS & SST-2 VAL ==================
print("Loading HookedTransformer (GPT-2 Small)...")
model = HookedTransformer.from_pretrained("gpt2-small", device=device)
model.eval()

print("Loading SST-2 validation split...")
val_ds = load_dataset("glue", "sst2", split="validation")
val_sentences = val_ds["sentence"]

# tokenize with TransformerLens helper (handles padding & BOS/EOS)
tokens_all = model.to_tokens(val_sentences, truncate=True)   # shape (N, seq_len)

class TokenDataset(torch.utils.data.Dataset):
    def __init__(self, tokens):
        self.tokens = tokens
    def __len__(self):
        return self.tokens.shape[0]
    def __getitem__(self, idx):
        return self.tokens[idx]

val_tok_ds = TokenDataset(tokens_all)

# ================== SAE WRAPPER ==================
class SAEWithActs(torch.nn.Module):
    """Wrap an SAE to expose latent activations via hook_sae_acts_post."""
    def __init__(self, sae_model):
        super().__init__()
        self.sae = sae_model
        self.activations = None
        hook_module = getattr(self.sae, "hook_sae_acts_post", None)
        if hook_module is None:
            raise ValueError("hook_sae_acts_post not found on SAE model.")
        hook_module.register_forward_hook(self._hook)

    def _hook(self, module, inp, out):
        self.activations = out.detach()

    def forward(self, sae_input):
        _ = self.sae(sae_input)
        return self.activations

    def get_acts(self, sae_input):
        self.eval()
        with torch.no_grad():
            _ = self.forward(sae_input)
            return self.activations

# ================== TOP-K GLOBAL FEATURES (USING blocks.L.attn.hook_z) ==================
def top_k_global_features_for_layer(layer_num, sae_model, k=10, max_batches=None):
    """
    For the pretrained SAE on blocks.{layer}.hook_z, count how many sentences
    each SAE feature is active on (any token > 0), and return the top-k indices.
    """
    sae_wrap = SAEWithActs(sae_model).to(device)
    hook_name = f"blocks.{layer_num}.attn.hook_z"   # NOTE: attn.hook_z

    counter = Counter()
    dataloader = DataLoader(val_tok_ds, batch_size=32)

    for i, batch_tokens in enumerate(tqdm(dataloader, desc=f"Counting features L{layer_num}")):
        if max_batches is not None and i >= max_batches:
            break

        batch_tokens = batch_tokens.to(device)
        with torch.no_grad():
            _, cache = model.run_with_cache(batch_tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]                     # (B, seq, d_model)
            latents = sae_wrap.get_acts(hook_acts)           # (B, seq, d_sae)

        active = (latents > 0).any(dim=1)                    # (B, d_sae)
        for row in active:
            idxs = row.nonzero(as_tuple=True)[0].tolist()
            counter.update(idxs)

    topk = counter.most_common(k)
    print(f"Layer {layer_num} top-{k} global features (idx, count): {topk}")
    return [idx for idx, _ in topk]

# ================== INTERPRETABILITY HELPERS ==================
def extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=None):
    """
    For a given feature_idx and layer, return per-sentence activations and texts.
    """
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    acts_all, texts_all = [], []
    dataloader = DataLoader(val_tok_ds, batch_size=32)

    for i, batch_tokens in enumerate(tqdm(dataloader, desc=f"Extracting L{layer_num} F{feature_idx}")):
        if max_batches is not None and i >= max_batches:
            break

        batch_tokens = batch_tokens.to(device)
        with torch.no_grad():
            _, cache = model.run_with_cache(batch_tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]                 # (B, seq, d_model)
            latents = sae_wrap.get_acts(hook_acts)       # (B, seq, d_sae)

        feat_acts = latents[:, :, feature_idx].detach().cpu().numpy()  # (B, seq)

        start = i * dataloader.batch_size
        batch_texts = val_sentences[start : start + feat_acts.shape[0]]

        for a_sent, txt in zip(feat_acts, batch_texts):
            acts_all.append(a_sent)
            texts_all.append(txt)

    return acts_all, texts_all

def select_top(acts, texts, n=20):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:n]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

def llama_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a sparse autoencoder feature from GPT-2.\n"
        "Each sentence below is accompanied by per-token activation strengths for this feature; "
        "larger numbers indicate stronger activation.\n\n"
        "From this data, give one concise sentence describing what pattern or concept "
        "this feature responds to.\n\n"
    )
    for i, (txt, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{txt}\"\nActivations: {acts.tolist()}\n\n"
    prompt += "Your explanation:"
    return chat_with_llama(prompt)

def llama_activation_score(sentence, interpretation):
    prompt = (
        f'Feature interpretation:\n"{interpretation}"\n\n'
        "On a scale from 0 (not active) to 10 (very active), estimate how strongly "
        "this feature activates on the following sentence. Respond with only a single number.\n\n"
        f'Sentence: \"{sentence}\"\nActivation:'
    )
    resp = chat_with_llama(prompt, max_tokens=16)
    m = re.search(r"\d+(\.\d+)?", resp)
    return float(m.group()) if m else 0.0

def pearson_score(actual_acts, pred_scores):
    actual = np.array([np.mean(a) for a in actual_acts]) * 10.0
    pred = np.array(pred_scores)
    corr, _ = pearsonr(actual, pred)
    return corr

def save_report(path, layer_num, feature_idx, interpretation,
                eval_texts, eval_acts, pred_scores, corr):
    with open(path, "a", encoding="utf-8") as f:
        f.write(f"Layer {layer_num}, Feature {feature_idx}\n")
        f.write("-"*70 + "\n")
        f.write("Interpretation:\n" + interpretation.strip() + "\n\n")
        f.write("Evaluation sentences:\n")
        for i, (txt, acts, pred) in enumerate(zip(eval_texts, eval_acts, pred_scores), 1):
            actual = np.mean(acts) * 10.0
            f.write(f"{i}. {txt}\n   Actual={actual:.3f}, Pred={pred:.3f}\n")
        f.write(f"\nPearson correlation: {corr:.4f}\n")
        f.write("="*70 + "\n\n")

# ================== PIPELINE FOR ONE FEATURE ==================
def interpretability_for_feature(layer_num, sae_model, feature_idx, report_path, max_batches=None):
    sae_wrap = SAEWithActs(sae_model).to(device)

    # 1) collect activations for full validation set
    acts_all, texts_all = extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=max_batches)

    # 2) choose top-20 sentences; first 5 for interpretation
    top_acts, top_texts = select_top(acts_all, texts_all, n=20)
    interp_acts, interp_texts = top_acts[:5], top_texts[:5]

    print(f"\nTop 5 sentences for L{layer_num} feature {feature_idx}:")
    for i, s in enumerate(interp_texts, 1):
        print(f"{i}. {s}")

    interpretation = llama_interpretation(interp_texts, interp_acts)
    print("\nInterpretation:", interpretation)

    # 3) evaluation on held-out top-5 (excluding prompt sentences)
    used = set(interp_texts)
    rest = [(a, t) for a, t in zip(acts_all, texts_all) if t not in used]
    if not rest:
        print("No remaining sentences for evaluation; skipping feature.")
        return None

    rest_acts, rest_texts = zip(*rest)
    rest_acts, rest_texts = list(rest_acts), list(rest_texts)
    scores = [np.sum(a) for a in rest_acts]
    idxs = np.argsort(scores)[::-1][:5]
    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    print("\nEvaluation sentences:")
    for i, s in enumerate(eval_texts, 1):
        print(f"{i}. {s}")

    pred_scores = [llama_activation_score(s, interpretation) for s in eval_texts]
    corr = pearson_score(eval_acts, pred_scores)
    print(f"Pearson correlation for L{layer_num} feature {feature_idx}: {corr:.4f}")

    save_report(report_path, layer_num, feature_idx, interpretation,
                eval_texts, eval_acts, pred_scores, corr)

    return corr

# ================== MAIN: LAYERS 6 & 7, TOP-10 GLOBAL FEATURES ==================
release = "gpt2-small-hook-z-kk"
layers_to_analyze = [6, 7]
TOP_K = 10

for layer in layers_to_analyze:
    print("\n" + "="*70)
    print(f"LAYER {layer}: loading pretrained SAE and finding top-{TOP_K} global features")
    print("="*70)

    # SAE ids are blocks.L.hook_z in this release; do not include attn here
    sae_id = f"blocks.{layer}.hook_z"
    try:
        sae_model = SAE.from_pretrained(release, sae_id)
    except:
        sae_model = SAE.from_pretrained(release, sae_id)[0]
    sae_model.to(device)
    sae_model.eval()

    # 1) find globally most frequent features
    topk_feats = top_k_global_features_for_layer(
        layer_num=layer,
        sae_model=sae_model,
        k=TOP_K,
        max_batches=None,   # set to e.g. 50 to cut compute if needed
    )

    report_file = f"layer{layer}_top{TOP_K}_global_features_interpretability.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"INTERPRETABILITY REPORT â€“ Layer {layer}, top-{TOP_K} global frequent features\n")
        f.write("="*70 + "\n\n")

    # 2) interpret each top feature
    for feat in topk_feats:
        print(f"\n--- Layer {layer}, global feature {feat} ---")
        interpretability_for_feature(
            layer_num=layer,
            sae_model=sae_model,
            feature_idx=feat,
            report_path=report_file,
            max_batches=None,
        )

    print(f"\nLayer {layer}: finished. Report saved to {report_file}")

