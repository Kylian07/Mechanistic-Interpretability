======================================================================
FINAL SUMMARY - GEMMA-2B EUROPARL TRANSLATION ANALYSIS
======================================================================

1. BASELINE
----------------------------------------------------------------------
Baseline Accuracy: 49.80%  (no SAE, zero-shot translation)

2. LAYER PERFORMANCE
----------------------------------------------------------------------
Layer   Acc%     Improv     Eq Feat   Div Feat   Common
----------------------------------------------------------------------
L0     49.70%   -0.10%     16340     16338      16328
L1     48.85%   -0.95%     16384     16384      16384
L2     51.45%   +1.65%     16035     16077      15917
L3     50.50%   +0.70%     15979     15951      15794
L4     52.60%   +2.80%     16208     16226      16146
L5     54.65%   +4.85%     15906     15800      15655
L6     50.85%   +1.05%     12966     12750      12041
L7     51.90%   +2.10%     16145     16144      16103
L8     51.65%   +1.85%     13842     13739      13228
L9     54.25%   +4.45%     15966     15978      15850
L10    52.80%   +3.00%     15382     15254      14972
L11    54.40%   +4.60%     16319     16319      16285
L12    54.10%   +4.30%     16264     16263      16228
L13    51.65%   +1.85%     16378     16379      16377
L14    50.35%   +0.55%     16384     16384      16384
L15    51.30%   +1.50%     16383     16382      16382
L16    50.60%   +0.80%     16379     16380      16378
L17    52.40%   +2.60%     16381     16379      16378

3. KEY RESULTS
----------------------------------------------------------------------
Best Layer: 5  (54.65%, +4.85%)
Top 2 Layers: [5, 11]

4. OBSERVATIONS
----------------------------------------------------------------------
- Translation-relevant representations emerge most strongly at layer 5,
  with sustained high performance through layers 9--12.
- Layers with the highest accuracy correspond to layers with strong
  overlap between equivalent and divergent SAE features, indicating
  robust semantic alignment mechanisms.
- Deeper layers (L11--L12) specialize in discourse-level and institutional
  translation patterns, while mid layers encode compositional phrase and
  semantic equivalence abstractions.
