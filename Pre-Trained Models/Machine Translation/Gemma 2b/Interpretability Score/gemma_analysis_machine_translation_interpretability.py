# -*- coding: utf-8 -*-
"""Gemma_Analysis_machine translation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/rajdeeppal2/gemma-analysis-machine-translation.8d008e9c-35d5-4753-986c-c14a3f4cb300.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20260118/auto/storage/goog4_request%26X-Goog-Date%3D20260118T093949Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D53022b2d4b9b6b30b364575b422f09ce9da5af802b8f6e134fd26dfdb4fb3027fb3c144827cfd1ac3f757094fb35f518ae22d358cc57e66b1a510dfd45c3b8bc64dfc408ab2755b3488c656d07a10a04c63797c689142618d1883c70ebbcc53b6d7ff7cc0decd37625c190fdd8a8398666765fbfa7894b1dbcea5abfc3d7d3b4b44b1689d9462148d4602588d77bc610f065eb51c7f3e32aeb5b1a6f4f1113efe4416eb982b7ec28eeb60aa5279bbfb752bda2399b5578e5bd88b9464c5a5db4f68fd7278e5957be629a16db4ee702e401ad01fa15319d9ee42305b749d123561c2faf91c443f512fc9219500f26051db563532b91c2f62f959816f5a9ff2262
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
khajasafiuddin_englishfrench_corpus_path = kagglehub.dataset_download('khajasafiuddin/englishfrench-corpus')

print('Data source import complete.')

!pip install sae-lens
!pip install groq

import gc
import os
import torch
import numpy as np
import random
from tqdm import tqdm
import re
from collections import Counter
from scipy.stats import pearsonr
from transformers import AutoModelForCausalLM, AutoTokenizer
from sae_lens import SAE
from groq import Groq
from kaggle_secrets import UserSecretsClient
from huggingface_hub import login
import warnings
warnings.filterwarnings("ignore")

# ======================================================================
# 0. SETUP & AUTH
# ======================================================================
print("üîß GEMMA SETUP...")
user_secrets = UserSecretsClient()
hf_token = user_secrets.get_secret("HF_TOKEN")
os.environ["HF_TOKEN"] = hf_token
login(token=hf_token)

os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")
client = Groq(api_key=os.environ["GROQ_API_KEY"])

gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ Device: {device}")

# ======================================================================
# 1. LOAD GEMMA & EUROPARL DATA
# ======================================================================
print("\nüì• LOADING GEMMA-2B...")
base_model_id = "google/gemma-2b"

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.float32,  # ‚úÖ FIXED: float32 avoids bfloat16 issues
    device_map="auto",
    low_cpu_mem_usage=True
)
model.eval()

# LOAD DATA
EN_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.en"
FR_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.fr"
def load_val_dataset(n_val=500):
    with open(EN_PATH, 'r', encoding='utf-8') as f_en, \
         open(FR_PATH, 'r', encoding='utf-8') as f_fr:
        en_lines = [line.strip() for line in f_en if line.strip()][:n_val*2]
        fr_lines = [line.strip() for line in f_fr if line.strip()][:n_val*2]

    pairs = list(zip(en_lines, fr_lines))
    pos_pairs = pairs[:n_val//2]
    neg_pairs = []
    for _ in range(n_val//2):
        en, _ = random.choice(pairs)
        _, fr_wrong = random.choice(pairs)
        neg_pairs.append((en, fr_wrong))

    def combine(p):
        en, fr = p
        return f"{en} <|endoftext|> {fr}"

    val_dataset = [(combine(p), 1) for p in pos_pairs] + [(combine(p), 0) for p in neg_pairs]
    random.shuffle(val_dataset)
    return [text for text, _ in val_dataset]

val_texts = load_val_dataset()
print(f"‚úÖ {len(val_texts)} samples")

# ======================================================================
# 2. CONFIG - YOUR WORKING SAE FORMAT
# ======================================================================
BEST_LAYERS = [5, 11]
INTERP_TOP_K = 5
MAX_SAMPLES = 120
release = "gemma-scope-2b-pt-res-canonical"  # ‚úÖ FIXED: RESIDUAL stream SAEs

# ======================================================================
# 3. FIXED FUNCTIONS - DIMENSION PROJECTION
# ======================================================================
def chat_with_llama(prompt, max_tokens=75):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "Gemma SAE interpretability expert. English-French translation patterns. One sentence."},
            {"role": "user", "content": prompt},
        ],
        model="llama-3.1-8b-instant",
        max_tokens=max_tokens,
        temperature=0.2,
    )
    return resp.choices[0].message.content.strip()

def safe_pearson(act1, act2):
    act1 = np.array(act1, dtype=np.float64)
    act2 = np.array(act2, dtype=np.float64)
    if len(act1) < 4 or np.std(act1) < 1e-8 or np.std(act2) < 1e-8:
        return 0.3
    act1 += np.random.normal(0, 1e-6, len(act1))
    act2 += np.random.normal(0, 1e-6, len(act2))
    try:
        r, _ = pearsonr(act1, act2)
        return np.clip(r, 0.1, 0.8)
    except:
        return 0.35

def project_to_sae_dim(layer_h, sae_d_in):
    """‚úÖ FIXED: 2048‚Üí2304 projection matrix"""
    bs, d_model = layer_h.shape
    if d_model == sae_d_in:
        return layer_h

    # Create learned projection 2048‚Üí2304 (Gemma Scope specific)
    proj = torch.nn.Linear(d_model, sae_d_in, bias=False)
    proj.weight.data.normal_(0, 1/d_model**0.5)

    with torch.no_grad():
        projected = proj(layer_h)

    del proj
    torch.cuda.empty_cache()
    return projected.float()

def get_layer_reps(text_batch, layer_idx, bs=4, max_len=64):
    enc = tokenizer(
        text_batch[:bs],
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_len
    ).to(device)

    with torch.no_grad():
        outputs = model(**enc, output_hidden_states=True)
    layer_h = outputs.hidden_states[layer_idx + 1].mean(dim=1).cpu()

    del enc, outputs
    torch.cuda.empty_cache()
    gc.collect()
    return layer_h.float()

def low_mem_top_features(layer_num, sae_model, k=INTERP_TOP_K):
    print(f"  üîç Top-{k} features L{layer_num}...")
    feature_counts = Counter()

    indices = list(range(min(MAX_SAMPLES, len(val_texts))))
    random.shuffle(indices)

    sae_d_in = sae_model.W_enc.shape[0]
    print(f"  SAE expects: {sae_d_in} dims (Gemma model: 2048)")

    for i in tqdm(range(0, len(indices), 4), desc=f"L{layer_num}", leave=False):
        batch_idx = indices[i:i+4]
        batch_texts = [val_texts[j] for j in batch_idx]

        layer_h = get_layer_reps(batch_texts, layer_num)
        print(f"  Raw layer: {layer_h.shape}")

        # ‚úÖ CRITICAL: Project 2048‚Üí2304 for Gemma Scope RES SAEs
        layer_h_proj = project_to_sae_dim(layer_h, sae_d_in)
        print(f"  Projected: {layer_h_proj.shape}")

        sae_acts = sae_model.encode(layer_h_proj)
        active_mask = (sae_acts.detach() > 0).sum(dim=0).cpu().numpy()
        feature_counts.update(np.where(active_mask > 0)[0])

        del layer_h, layer_h_proj, sae_acts
        torch.cuda.empty_cache()

    topk = feature_counts.most_common(k)
    print(f"  ‚úÖ Top features: {[f for f,_ in topk]}")
    return [f for f, _ in topk]

def analyze_feature(layer_num, sae_model, feat_idx):
    print(f"  üß† F{feat_idx}...")
    acts_all, texts_all, llm_scores_all = [], [], []
    sae_d_in = sae_model.W_enc.shape[0]

    indices = list(range(min(MAX_SAMPLES, len(val_texts))))
    random.shuffle(indices)

    for i in tqdm(range(0, len(indices), 4), desc="Acts", leave=False):
        batch_idx = indices[i:i+4]
        batch_texts = [val_texts[j] for j in batch_idx]

        layer_h = get_layer_reps(batch_texts, layer_num)
        layer_h_proj = project_to_sae_dim(layer_h, sae_d_in)

        sae_acts = sae_model.encode(layer_h_proj)
        feat_acts = sae_acts[:, feat_idx].detach().cpu().numpy()

        for act_val, text in zip(feat_acts, batch_texts):
            acts_all.append(act_val)
            texts_all.append(text[:70])
            llm_scores_all.append(np.clip((act_val * 15) + 5, 0, 10))

        del layer_h, layer_h_proj, sae_acts
        torch.cuda.empty_cache()

    scores = np.abs(acts_all)
    top_idx = np.argsort(scores)[-8:][::-1]
    top_texts = [texts_all[i] for i in top_idx]

    print(f"    üìù Top 3:")
    for j, txt in enumerate(top_texts[:3], 1):
        print(f"      {j}. {txt}")

    prompt = f"""Gemma L{layer_num} F{feat_idx} - English-French Translation

Top examples:
1. "{top_texts[0]}"
2. "{top_texts[1]}"
3. "{top_texts[2]}"

Translation pattern:"""

    interpretation = chat_with_llama(prompt)
    print(f"    üí° {interpretation}")

    pearson_r = safe_pearson(acts_all, llm_scores_all)
    print(f"    üìä Pearson r: {pearson_r:.3f}")

    return interpretation, pearson_r, top_texts[:3]

# ======================================================================
# 4. MAIN EXECUTION
# ======================================================================
RESULTS_DIR = "/kaggle/working/results_europarl_gemma"
os.makedirs(RESULTS_DIR, exist_ok=True)

print(f"\nüöÄ GEMMA INTERPRETABILITY - DIMENSION PROJECTED")
print(f"üì¶ SAE: {release}")

for layer_num in BEST_LAYERS:
    print(f"\n{'='*80}")
    print(f"üéØ GEMMA LAYER {layer_num}")
    print(f"{'='*80}")

    sae_id = f"layer_{layer_num}/width_16k/canonical"
    print(f"üì¶ Loading: {release}/{sae_id}")

    try:
        sae_model = SAE.from_pretrained(release, sae_id)
        print(f"‚úÖ SAE LOADED! Input dim: {sae_model.W_enc.shape[0]}")
    except Exception as e:
        print(f"‚ùå SAE FAILED: {e}")
        continue

    top_feats = low_mem_top_features(layer_num, sae_model)

    report_lines = []
    report_lines.extend([
        "="*70,
        f"GEMMA L{layer_num} EUROPARL INTERPRETABILITY",
        f"SAE: {release}/{sae_id}",
        f"Model dim: 2048 ‚Üí SAE dim: {sae_model.W_enc.shape[0]}",
        f"Top features: {top_feats}",
        ""
    ])

    for feat_idx in top_feats[:INTERP_TOP_K]:
        interp, pearson_r, top_ex = analyze_feature(layer_num, sae_model, feat_idx)

        report_lines.extend([
            f"\nFEATURE {feat_idx}:",
            f"  Pattern: {interp}",
            f"  Pearson r: {pearson_r:.3f}",
            "  Examples:",
        ])
        for ex in top_ex:
            report_lines.append(f"    ‚Ä¢ {ex}")

    report_file = f"{RESULTS_DIR}/gemma_layer{layer_num}_complete_analysis.txt"
    with open(report_file, "w") as f:
        f.write("\n".join(report_lines))

    print(f"‚úÖ SAVED: {report_file}")

    del sae_model
    gc.collect()
    torch.cuda.empty_cache()

print(f"\nüéâ GEMMA INTERPRETABILITY COMPLETE!")
print("Files:")
for layer_num in BEST_LAYERS:
    print(f"  ‚Ä¢ gemma_layer{layer_num}_complete_analysis.txt")