# -*- coding: utf-8 -*-
"""Gemma_Analysis_machine translation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/rajdeeppal2/gemma-analysis-machine-translation.8d008e9c-35d5-4753-986c-c14a3f4cb300.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20260118/auto/storage/goog4_request%26X-Goog-Date%3D20260118T093949Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D53022b2d4b9b6b30b364575b422f09ce9da5af802b8f6e134fd26dfdb4fb3027fb3c144827cfd1ac3f757094fb35f518ae22d358cc57e66b1a510dfd45c3b8bc64dfc408ab2755b3488c656d07a10a04c63797c689142618d1883c70ebbcc53b6d7ff7cc0decd37625c190fdd8a8398666765fbfa7894b1dbcea5abfc3d7d3b4b44b1689d9462148d4602588d77bc610f065eb51c7f3e32aeb5b1a6f4f1113efe4416eb982b7ec28eeb60aa5279bbfb752bda2399b5578e5bd88b9464c5a5db4f68fd7278e5957be629a16db4ee702e401ad01fa15319d9ee42305b749d123561c2faf91c443f512fc9219500f26051db563532b91c2f62f959816f5a9ff2262
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
khajasafiuddin_englishfrench_corpus_path = kagglehub.dataset_download('khajasafiuddin/englishfrench-corpus')

print('Data source import complete.')

!pip install sae-lens
!pip install groq

import os
import gc
import re
import torch
import numpy as np
from collections import Counter
from torch.utils.data import DataLoader
from transformer_lens import HookedTransformer
from sae_lens import SAE
from groq import Groq
from scipy.stats import pearsonr
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import random


# ======================================================================
# 2. Setup - IDENTICAL TO GPT2/EUROPARL
# ======================================================================
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.makedirs('/kaggle/working/results_europarl_gemma', exist_ok=True)

os.environ['HF_HOME'] = '/kaggle/working/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# -*- coding: utf-8 -*-
"""
Gemma-2B SAE Europarl English-French Translation Analysis
Identical structure to GPT-2 SST2/MRPC/Europarl notebooks
"""

# ======================================================================
# 0. HF AUTHENTICATION (REQUIRED FOR GEMMA)
# ======================================================================
from huggingface_hub import login

# Kaggle Secrets (recommended)
try:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    hf_token = user_secrets.get_secret("HF_TOKEN")
    login(token=hf_token)
    print("âœ“ Logged in to Hugging Face via Kaggle Secrets")
except:
    print("Manual HF login required:")
    login()
    print("âœ“ Logged in to Hugging Face manually")

# Groq setup (same as your Gemma MRPC)
user_secrets = UserSecretsClient()
os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")
client = Groq(api_key=os.environ["GROQ_API_KEY"])

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a translation interpretability assistant."},
            {"role": "user", "content": prompt},
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# ======================================================================
# 3. EUROPARL DATA LOADING - FIXED BALANCED CLASSES
# ======================================================================
def load_europarl_pairs(en_path, fr_path, max_samples=30000):
    """Load aligned English-French sentence pairs"""
    print("Loading Europarl parallel corpus...")

    with open(en_path, 'r', encoding='utf-8') as f_en, \
         open(fr_path, 'r', encoding='utf-8') as f_fr:
        en_lines = [line.strip() for line in f_en if line.strip()]
        fr_lines = [line.strip() for line in f_fr if line.strip()]

    pairs = [(en, fr) for en, fr in zip(en_lines[:max_samples], fr_lines[:max_samples])]
    print(f"âœ“ Loaded {len(pairs)} total pairs")
    return pairs

def create_balanced_dataset(pairs, n_train=10000, n_val=2000):
    """Guaranteed 50/50 class balance for translation quality"""

    # Positive examples (real translations = label 1)
    pos_train = random.sample(pairs, n_train//2)
    pos_val = random.sample([p for p in pairs if p not in pos_train], n_val//2)

    # Negative examples (shuffled = label 0)
    neg_train, neg_val = [], []
    used_pairs = set()

    while len(neg_train) < n_train//2:
        en, _ = random.choice(pairs)
        _, wrong_fr = random.choice(pairs)
        pair = (en, wrong_fr)
        if pair not in used_pairs:
            used_pairs.add(pair)
            neg_train.append(pair)

    while len(neg_val) < n_val//2:
        en, _ = random.choice(pairs)
        _, wrong_fr = random.choice(pairs)
        pair = (en, wrong_fr)
        if pair not in used_pairs:
            used_pairs.add(pair)
            neg_val.append(pair)

    def combine_sentences(example):
        en, fr = example
        return f"{en} <|endoftext|> {fr}"

    train_dataset = [(combine_sentences(p), 1) for p in pos_train] + \
                    [(combine_sentences(p), 0) for p in neg_train]
    val_dataset = [(combine_sentences(p), 1) for p in pos_val] + \
                  [(combine_sentences(p), 0) for p in neg_val]

    random.shuffle(train_dataset)
    random.shuffle(val_dataset)

    return train_dataset, val_dataset


EN_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.en"
FR_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.fr"

all_pairs = load_europarl_pairs(EN_PATH, FR_PATH)
train_dataset, val_dataset = create_balanced_dataset(all_pairs)

print(f"Train: {len(train_dataset)} = {Counter([l for _,l in train_dataset])}")
print(f"Val:   {len(val_dataset)} = {Counter([l for _,l in val_dataset])}")

# ======================================================================
# 4. Load Gemma-2B (SAME AS YOUR MRPC CODE)
# ======================================================================
print("\nLoading Gemma-2B base model...")
model = HookedTransformer.from_pretrained("google/gemma-2b", device=device)
model.eval()

n_layers = model.cfg.n_layers
print(f"Model has {n_layers} layers (0-{n_layers-1})")

# ======================================================================
# 5. STEP 1: Baseline - Zero-Shot (IDENTICAL FORMAT)
# ======================================================================
print("\n" + "="*70)
print("STEP 1: BASELINE - ZERO-SHOT TRANSLATION QUALITY (Gemma-2B)")
print("="*70)

baseline_predictions = []
baseline_labels = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Zero-Shot Evaluation"):
        sentence, true_label = val_dataset[idx]

        prompt = f"""Is this a good English-French translation pair?

Pair: {sentence}
Quality (good/bad):"""

        tokens = model.to_tokens(prompt, prepend_bos=True)
        logits = model(tokens)
        next_token_logits = logits[0, -1, :]

        good_id = model.to_tokens(" good", prepend_bos=False)[0, 0]
        bad_id = model.to_tokens(" bad", prepend_bos=False)[0, 0]

        prediction = 1 if next_token_logits[good_id] > next_token_logits[bad_id] else 0

        baseline_predictions.append(prediction)
        baseline_labels.append(true_label)

baseline_acc = accuracy_score(baseline_labels, baseline_predictions)
baseline_p, baseline_r, baseline_f1, _ = precision_recall_fscore_support(
    baseline_labels, baseline_predictions, average='binary'
)

print(f"Baseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"F1-Score: {baseline_f1:.4f}")

with open('/kaggle/working/results_europarl_gemma/baseline_results.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("BASELINE - ZERO-SHOT TRANSLATION QUALITY (Gemma-2B)\n")
    f.write("="*70 + "\n\n")
    f.write(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n")

print("âœ“ Baseline saved")

# ======================================================================
# 6. STEP 2: Layer-wise Analysis (Gemma Scope SAEs)
# ======================================================================
print("\n" + "="*70)
print("STEP 2: LAYER-WISE ANALYSIS (Gemma Scope SAEs)")
print("="*70)

layer_performance = {}
sae_feature_stats = {}
TOP_K = 10
release = "gemma-scope-2b-pt-att-canonical"  # SAME AS YOUR GEMMA MRPC

for layer_num in range(n_layers):
    print(f"\n{'='*70}")
    print(f"LAYER {layer_num} ANALYSIS")
    print(f"{'='*70}")

    hook_name = f"blocks.{layer_num}.attn.hook_z"

    # Part A: Raw representation classifier
    print("\n--- Part A: Raw Representation Classifier ---")

    # Train representations (LIMIT TO 5000 for speed)
    train_reps, train_labels = [], []
    with torch.no_grad():
        for idx in tqdm(range(min(5000, len(train_dataset))), desc=f"Train L{layer_num}"):
            sentence, label = train_dataset[idx]
            tokens = model.to_tokens(sentence, prepend_bos=True)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            train_reps.append(pooled_flat)
            train_labels.append(label)

    # Val representations
    val_reps, val_labels_raw = [], []
    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"Val L{layer_num}"):
            sentence, label = val_dataset[idx]
            tokens = model.to_tokens(sentence, prepend_bos=True)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            val_reps.append(pooled_flat)
            val_labels_raw.append(label)

    X_train, y_train = np.array(train_reps), np.array(train_labels)
    X_val, y_val = np.array(val_reps), np.array(val_labels_raw)

    # SAFETY CHECK
    print(f"Train classes: {np.bincount(y_train.astype(int))}")

    clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
    clf.fit(X_train, y_train)
    predictions = clf.predict(X_val)

    acc = accuracy_score(y_val, predictions)
    p, r, f1, _ = precision_recall_fscore_support(y_val, predictions, average='binary')
    cm = confusion_matrix(y_val, predictions)

    layer_performance[layer_num] = {
        "accuracy": acc, "precision": p, "recall": r, "f1_score": f1,
        "improvement": acc - baseline_acc, "confusion_matrix": cm
    }

    print(f"Layer {layer_num} Accuracy: {acc:.4f} ({acc*100:.2f}%)")

    # Part B: Gemma Scope SAE analysis (SAME SAE ID FORMAT AS YOUR MRPC)
    print("\n--- Part B: Gemma Scope SAE Feature Analysis ---")

    sae_id = f"layer_{layer_num}/width_16k/canonical"

    try:
        sae = SAE.from_pretrained(release, sae_id)
        sae.to(device).eval()
        print(f"âœ“ Loaded SAE: {sae_id}")
    except Exception as e:
        print(f"âœ— SAE load failed: {e}")
        continue

    # SAE feature extraction
    eq_active_features, div_active_features = set(), set()
    eq_counts, div_counts = Counter(), Counter()
    eq_acts, div_acts = {}, {}
    total_eq, total_div = 0, 0

    with torch.no_grad():
        for idx, (sentence, label) in enumerate(tqdm(val_dataset, desc=f"SAE L{layer_num}")):
            tokens = model.to_tokens(sentence, prepend_bos=True)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])

            hook_acts = cache[hook_name]
            sae_acts = sae.encode(hook_acts)
            pooled = sae_acts.mean(dim=1)[0].cpu().numpy()

            active = np.where(pooled > 0)[0]

            if label == 1:  # Equivalent translation
                total_eq += 1
                for feat in active:
                    eq_active_features.add(feat)
                    eq_counts[feat] += 1
                    eq_acts.setdefault(feat, []).append(pooled[feat])
            else:  # Divergent
                total_div += 1
                for feat in active:
                    div_active_features.add(feat)
                    div_counts[feat] += 1
                    div_acts.setdefault(feat, []).append(pooled[feat])

    # Feature statistics (IDENTICAL FORMAT)
    common = eq_active_features & div_active_features
    eq_only = eq_active_features - div_active_features
    div_only = div_active_features - eq_active_features

    topk_eq = eq_counts.most_common(TOP_K)
    topk_div = div_counts.most_common(TOP_K)

    sae_feature_stats[layer_num] = {
        "total_eq_features": len(eq_active_features),
        "total_div_features": len(div_active_features),
        "common_features": len(common),
        "eq_only_features": len(eq_only),
        "div_only_features": len(div_only),
        "topk_eq_by_frequency": topk_eq,
        "topk_div_by_frequency": topk_div,
        "total_eq_samples": total_eq,
        "total_div_samples": total_div
    }

    # Save per-layer file (EXACT SAME FORMAT)
    with open(f"/kaggle/working/results_europarl_gemma/layer_{layer_num}_complete_analysis.txt", "w") as f:
        f.write("="*70 + "\n")
        f.write(f"LAYER {layer_num} - GEMMA-2B EUROPARL ANALYSIS\n")
        f.write("="*70 + "\n\n")

        f.write("A. LAYER PERFORMANCE\n")
        f.write(f"Accuracy: {acc:.4f} ({acc*100:.2f}%)\n")
        f.write(f"Improvement: {(acc-baseline_acc)*100:+.2f}%\n\n")

        f.write("B. SAE FEATURE COUNTS\n")
        f.write(f"EQUIVALENT features: {len(eq_active_features)}\n")
        f.write(f"DIVERGENT features: {len(div_active_features)}\n")
        f.write(f"COMMON: {len(common)}, EQ_ONLY: {len(eq_only)}\n")

    print(f"âœ“ Layer {layer_num} saved")
    gc.collect()
    torch.cuda.empty_cache()

# ======================================================================
# 7. FINAL SUMMARY (IDENTICAL FORMAT)
# ======================================================================
print("\n" + "="*70)
print("FINAL SUMMARY - GEMMA-2B EUROPARL")
print("="*70)

best_layer = max(layer_performance.items(), key=lambda x: x[1]["accuracy"])
top_2_layers = sorted(layer_performance.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:2]
top_2_layer_nums = [l[0] for l in top_2_layers]

print(f"{'Layer':<6} {'Acc%':<8} {'Eq Feat':<10} {'Div Feat':<10}")
print("-"*50)
for layer_num in sorted(layer_performance.keys()):
    perf = layer_performance[layer_num]
    stats = sae_feature_stats.get(layer_num, {})
    print(f"L{layer_num:<5} {perf['accuracy']*100:6.2f}% "
          f"{stats.get('total_eq_features',0):<10} {stats.get('total_div_features',0):<10}")

print(f"\nâœ“ Best: Layer {best_layer[0]} ({best_layer[1]['accuracy']*100:.2f}%)")
print(f"âœ“ Top 2 for interpretability: {top_2_layer_nums}")

# Save summary
with open("/kaggle/working/results_europarl_gemma/final_summary.txt", "w") as f:
    f.write("GEMMA-2B EUROPARL SUMMARY\n")
    f.write(f"Best Layer: {best_layer[0]}\n")
    f.write(f"Top 2 Layers: {top_2_layer_nums}\n")

print("\nðŸŽ‰ GEMMA-2B EUROPARL ANALYSIS COMPLETE!")
print("Files saved to: /kaggle/working/results_europarl_gemma/")