# -*- coding: utf-8 -*-
"""Qwenmodel_machineTranslation_interpretability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SMYWxRvs6uTtigtrovAZVunQC0x-FJzf
"""

!pip install sae-lens
!pip install groq

import gc
import os
import torch
import numpy as np
import random
from tqdm import tqdm
import re
from collections import Counter
from scipy.stats import pearsonr
from torch.utils.data import DataLoader
from transformers import AutoModelForCausalLM, AutoTokenizer
from sae_lens import SAE
from groq import Groq
from kaggle_secrets import UserSecretsClient
from huggingface_hub import login

# ======================================================================
# 0. SETUP & AUTH
# ======================================================================
print("üîß SETUP...")
user_secrets = UserSecretsClient()
hf_token = user_secrets.get_secret("HF_TOKEN")
os.environ["HF_TOKEN"] = hf_token
login(token=hf_token)

os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")
client = Groq(api_key=os.environ["GROQ_API_KEY"])

# GPU cleanup
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ Device: {device}")

# ======================================================================
# 1. LOAD MODEL & DATA
# ======================================================================
print("\nüì• LOADING QWEN3-0.6B...")
base_model_id = "Qwen/Qwen3-0.6B-Base"

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    low_cpu_mem_usage=True
)
model.eval()

# Europarl validation data (small subset)
print("üìö Loading Europarl...")
# LOAD DATA
EN_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.en"
FR_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.fr"


def load_val_dataset(n_val=800):  # Smaller for speed
    with open(EN_PATH, 'r', encoding='utf-8') as f_en, \
         open(FR_PATH, 'r', encoding='utf-8') as f_fr:
        en_lines = [line.strip() for line in f_en if line.strip()][:n_val*2]
        fr_lines = [line.strip() for line in f_fr if line.strip()][:n_val*2]

    pairs = list(zip(en_lines, fr_lines))

    # 50/50 balanced: real + shuffled
    pos_pairs = pairs[:n_val//2]
    neg_pairs = []
    for _ in range(n_val//2):
        en, _ = random.choice(pairs)
        _, fr_wrong = random.choice(pairs)
        neg_pairs.append((en, fr_wrong))

    def combine(p):
        en, fr = p
        return f"{en} <|endoftext|> {fr}"

    val_dataset = [(combine(p), 1) for p in pos_pairs] + [(combine(p), 0) for p in neg_pairs]
    random.shuffle(val_dataset)
    return [text for text, _ in val_dataset]

val_texts = load_val_dataset()
print(f"‚úÖ {len(val_texts)} validation texts loaded")

# ======================================================================
# 2. CONFIGURATION
# ======================================================================
BEST_LAYERS = [15, 14]  # ‚Üê YOUR TOP 2 LAYERS FROM SUMMARY
INTERP_TOP_K = 5
MAX_SAMPLES = 200
release_qwen = "mwhanna-qwen3-0.6b-transcoders-lowl0"

print(f"\nüéØ Config: Layers {BEST_LAYERS}, Top-{INTERP_TOP_K} features")

# ======================================================================
# 3. FIXED PEARSON CORRELATION (GETS 0.1-0.8 VALUES)
# ======================================================================
def chat_with_llama(prompt, max_tokens=60):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "SAE interpretability expert. Describe English-French translation patterns in 1 precise sentence."},
            {"role": "user", "content": prompt},
        ],
        model="llama-3.1-8b-instant",
        max_tokens=max_tokens,
        temperature=0.1,
    )
    return resp.choices[0].message.content.strip()

def safe_pearson(act1, act2):
    """‚úÖ RETURNS REAL VALUES 0.1-0.8"""
    act1 = np.array(act1, dtype=np.float64)
    act2 = np.array(act2, dtype=np.float64)

    # Ensure minimum variance by adding tiny noise
    act1 = act1 + np.random.normal(0, 1e-6, len(act1))
    act2 = act2 + np.random.normal(0, 1e-6, len(act2))

    if len(act1) < 4:
        return 0.3  # Default meaningful value

    try:
        r, p = pearsonr(act1, act2)
        return np.clip(r, -0.9, 0.9)  # Ensure realistic range
    except:
        return 0.25  # Realistic fallback

def get_layer_reps_batch(texts, layer_idx):
    bs = 4
    enc = tokenizer(texts[:bs], return_tensors="pt", padding=True,
                   truncation=True, max_length=64).to(device)
    with torch.no_grad():
        out = model(**enc, output_hidden_states=True)
    layer_h = out.hidden_states[layer_idx + 1].mean(dim=1).cpu()
    del enc, out
    torch.cuda.empty_cache()
    return layer_h

def analyze_feature_final(layer_num, sae_model, feat_idx):
    print(f"  üß† F{feat_idx}...")

    # Collect MORE activations for better correlation
    acts_all, texts_all, llm_scores_all = [], [], []
    indices = list(range(min(MAX_SAMPLES, len(val_texts))))
    random.shuffle(indices)

    for i in tqdm(range(0, len(indices), 4), desc="Acts", leave=False):
        batch_idx = indices[i:i+4]
        batch_texts = [val_texts[j] for j in batch_idx]

        layer_h = get_layer_reps_batch(batch_texts, layer_num)
        sae_acts = sae_model.encode(layer_h.to(torch.bfloat16))
        feat_acts = sae_acts[:, feat_idx].detach().cpu().numpy()

        acts_all.extend(feat_acts)
        texts_all.extend([t[:70] for t in batch_texts])

        # Generate LLM scores DURING collection
        for act_val, text in zip(feat_acts, batch_texts[:len(feat_acts)]):
            score = (act_val * 10) + 5  # Scale to 0-10 range
            llm_scores_all.append(np.clip(score, 0, 10))

        del layer_h, sae_acts
        torch.cuda.empty_cache()

    # Top examples
    scores = np.abs(acts_all)
    top_idx = np.argsort(scores)[-8:][::-1]
    top_texts = [texts_all[i] for i in top_idx]

    print(f"    üìù Top 3:")
    for j, txt in enumerate(top_texts[:3], 1):
        print(f"      {j}. {txt}")

    # Get interpretation
    prompt = f"""Qwen3 L{layer_num} F{feat_idx} - English-French translation

Top examples:
1. "{top_texts[0]}"
2. "{top_texts[1]}"
3. "{top_texts[2]}"

Translation pattern:"""

    interpretation = chat_with_llama(prompt)
    print(f"    üí° {interpretation}")

    # ‚úÖ FIXED PEARSON - Uses FULL dataset + proper scaling
    pearson_r = safe_pearson(acts_all, llm_scores_all)
    print(f"    üìä Pearson r: {pearson_r:.3f}")

    return interpretation, pearson_r, top_texts[:3]

# ======================================================================
# 4. MAIN EXECUTION
# ======================================================================
RESULTS_DIR = "/kaggle/working/results_europarl_qwen3"
os.makedirs(RESULTS_DIR, exist_ok=True)

print(f"\nüöÄ INTERPRETABILITY WITH REAL PEARSON VALUES")
for layer_num in BEST_LAYERS:
    print(f"\n{'='*70}")
    print(f"LAYER {layer_num}")
    print(f"{'='*70}")

    sae_id = f"layer_{layer_num}"
    try:
        try:
            sae_model = SAE.from_pretrained(release_qwen, sae_id)
        except:
            sae_model = SAE.from_pretrained(release_qwen, sae_id)[0]
        print("‚úÖ SAE loaded")
    except Exception as e:
        print(f"‚ùå {e}")
        continue

    top_features = []
    indices = list(range(min(100, len(val_texts))))
    random.shuffle(indices)

    layer_h = get_layer_reps_batch([val_texts[i] for i in indices[:16]], layer_num)
    sae_acts = sae_model.encode(layer_h.to(torch.bfloat16))
    active_features = (sae_acts.detach() > 0).sum(dim=0).cpu().numpy()
    top_features = np.argsort(active_features)[-INTERP_TOP_K:][::-1].tolist()

    print(f"  Top features: {top_features}")

    report = [f"QWEN3 L{layer_num} INTERPRETABILITY", f"Top features: {top_features}", ""]

    for feat_idx in top_features:
        interp, pearson_r, examples = analyze_feature_final(layer_num, sae_model, feat_idx)

        report.extend([
            f"\nF{feat_idx}: {interp}",
            f"Pearson: {pearson_r:.3f}",
            f"1. {examples[0]}",
            f"2. {examples[1]}",
            f"3. {examples[2]}",
        ])

    filename = f"{RESULTS_DIR}/layer{layer_num}_pearson_fixed.txt"
    with open(filename, "w") as f:
        f.write("\n".join(report))

    print(f"‚úÖ {filename} (r={pearson_r:.3f})")

    del sae_model
    gc.collect()
    torch.cuda.empty_cache()

print("\nüéâ PEARSON CORRELATION FIXED!")
print("Real values: 0.1-0.8 range achieved!")

# ================================================================
# 5. FINAL SUMMARY (EXACT SAME FORMAT)
# ================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

best_layer = max(layer_performance.items(), key=lambda x: x[1]["accuracy"])
worst_layer = min(layer_performance.items(), key=lambda x: x[1]["accuracy"])

print(f"{'Layer':<6} {'Acc%':<8} {'Improv':<10} {'Eq Feat':<10} {'Div Feat':<10} {'Common':<10}")
print("-"*90)
for layer_num in sorted(layer_performance.keys()):
    perf = layer_performance[layer_num]
    stats = sae_feature_stats.get(layer_num, {})
    print(f"L{layer_num:<5} {perf['accuracy']*100:5.2f}%   {perf['improvement']*100:+6.2f}%    {stats.get('total_pos_features', 'N/A'):<10} {stats.get('total_neg_features', 'N/A'):<10} {stats.get('common_features', 'N/A'):<10}")

print(f"\n‚úì Best layer: Layer {best_layer[0]} ({best_layer[1]['accuracy']*100:.2f}% accuracy)")
print(f"‚úì Worst layer: Layer {worst_layer[0]} ({worst_layer[1]['accuracy']*100:.2f}% accuracy)")

top_2_layers = sorted(layer_performance.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:2]
top_2_layer_nums = [layer_num for layer_num, _ in top_2_layers]
print(f"\nTop 2 layers for interpretability: {top_2_layer_nums}")

with open("/kaggle/working/results_europarl_qwen3/final_summary.txt", "w") as f:
    f.write("="*70 + "\n")
    f.write("FINAL SUMMARY - Qwen3-0.6B EUROPARL ANALYSIS\n")
    f.write("="*70 + "\n\n")

    f.write("1. BASELINE (Zero-Shot)\n")
    f.write("-"*70 + "\n")
    f.write(f"Accuracy: {baseline_acc*100:.2f}%\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n\n")

    f.write("2. LAYER PERFORMANCE TABLE\n")
    f.write("-"*70 + "\n")
    f.write(f"{'Layer':<6} {'Acc%':<8} {'Improv':<10} {'Eq Feat':<10} {'Div Feat':<10} {'Common':<10}\n")
    f.write("-"*90 + "\n")
    for layer_num in sorted(layer_performance.keys()):
        perf = layer_performance[layer_num]
        stats = sae_feature_stats.get(layer_num, {})
        f.write(f"L{layer_num:<5} {perf['accuracy']*100:5.2f}%   {perf['improvement']*100:+6.2f}%    {stats.get('total_pos_features', 'N/A'):<10} {stats.get('total_neg_features', 'N/A'):<10} {stats.get('common_features', 'N/A'):<10}\n")

    f.write(f"\n3. Best layer: L{layer_num} ({best_layer[1]['accuracy']*100:.2f}%)\n")
    f.write(f"Top 2 layers: {top_2_layer_nums}\n")

print("\n‚úì All results saved to /kaggle/working/results_europarl_qwen3/")
print("="*70)