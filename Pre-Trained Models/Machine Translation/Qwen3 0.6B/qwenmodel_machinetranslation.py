# -*- coding: utf-8 -*-
"""Qwenmodel_machineTranslation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SMYWxRvs6uTtigtrovAZVunQC0x-FJzf
"""

!pip install sae-lens
!pip install groq

# ================================================================
# 0. Auth & imports (EXACT SAME AS YOUR QWEN MRPC)
# ================================================================
from kaggle_secrets import UserSecretsClient
import os
import gc
import torch
import numpy as np
from collections import Counter
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from sae_lens import SAE
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import random
from huggingface_hub import login

# Authenticate
user_secrets = UserSecretsClient()
hf_token = user_secrets.get_secret("HF_TOKEN")
os.environ["HF_TOKEN"] = hf_token
login(token=hf_token)

# Setup directories (EXACT SAME FORMAT)
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.makedirs('/kaggle/working/results_europarl_qwen3', exist_ok=True)
os.environ['HF_HOME'] = '/kaggle/working/hf_cache'

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ================================================================
# 1. EUROPARL DATASET (EXACT SAME FORMAT AS GPT2/GEMMA)
# ================================================================
def load_europarl_pairs(en_path, fr_path, max_samples=30000):
    """Load aligned English-French pairs - SAME AS GPT2/GEMMA"""
    print("Loading Europarl v7 English-French...")

    with open(en_path, 'r', encoding='utf-8') as f_en, \
         open(fr_path, 'r', encoding='utf-8') as f_fr:
        en_lines = [line.strip() for line in f_en if line.strip()]
        fr_lines = [line.strip() for line in f_fr if line.strip()]

    pairs = [(en, fr) for en, fr in zip(en_lines[:max_samples], fr_lines[:max_samples])]
    print(f"✓ Loaded {len(pairs)} aligned pairs")
    return pairs

def create_balanced_dataset(pairs, n_train=2000, n_val=1000):
    """Create 50/50 balanced dataset: 1=equivalent(good), 0=divergent(bad)"""

    # Positive examples (real translations)
    pos_train = random.sample(pairs, n_train//2)
    pos_val = random.sample([p for p in pairs if p not in pos_train], n_val//2)

    # Negative examples (shuffled translations)
    neg_train, neg_val = [], []
    used_pairs = set()

    while len(neg_train) < n_train//2:
        en, _ = random.choice(pairs)
        _, wrong_fr = random.choice(pairs)
        pair = (en, wrong_fr)
        if pair not in used_pairs:
            used_pairs.add(pair)
            neg_train.append(pair)

    while len(neg_val) < n_val//2:
        en, _ = random.choice(pairs)
        _, wrong_fr = random.choice(pairs)
        pair = (en, wrong_fr)
        if pair not in used_pairs:
            used_pairs.add(pair)
            neg_val.append(pair)

    def combine_sentences(example):
        en, fr = example
        return f"{en} <|endoftext|> {fr}"

    train_dataset = [(combine_sentences(p), 1) for p in pos_train] + \
                    [(combine_sentences(p), 0) for p in neg_train]
    val_dataset = [(combine_sentences(p), 1) for p in pos_val] + \
                  [(combine_sentences(p), 0) for p in neg_val]

    random.shuffle(train_dataset)
    random.shuffle(val_dataset)

    return train_dataset, val_dataset

# LOAD DATA
EN_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.en"
FR_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.fr"

all_pairs = load_europarl_pairs(EN_PATH, FR_PATH)
train_dataset, val_dataset = create_balanced_dataset(all_pairs)

print(f"Train: {len(train_dataset)} = {Counter([l for _,l in train_dataset])}")
print(f"Val:   {len(val_dataset)} = {Counter([l for _,l in val_dataset])}")

# ================================================================
# 2. Load Qwen3-0.6B-Base (IDENTICAL TO YOUR MRPC CODE)
# ================================================================
base_model_id = "Qwen/Qwen3-0.6B-Base"
print(f"Loading model: {base_model_id} ...")

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.bfloat16,
    device_map=device,
)
model.eval()

n_layers = model.config.num_hidden_layers
print(f"Model has {n_layers} layers (0-{n_layers-1})")

def combine_sentences(example):
    return example[0]  # Already combined

def get_layer_rep(text, layer_idx):
    """EXACT SAME AS YOUR QWEN MRPC CODE"""
    enc = tokenizer(
        text,
        return_tensors="pt",
        padding=False,
        truncation=True,
        max_length=128,
    ).to(device)

    with torch.no_grad():
        out = model(
            **enc,
            output_hidden_states=True,
            use_cache=False,
        )

    hidden_states = out.hidden_states
    layer_h = hidden_states[layer_idx + 1]
    return layer_h

# ================================================================
# 3. STEP 1: Baseline - Zero-Shot (FIXED VERSION)
# ================================================================
print("\n" + "="*70)
print("STEP 1: BASELINE - ZERO-SHOT TRANSLATION QUALITY")
print("="*70)

baseline_predictions = []
baseline_labels = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Zero-Shot Evaluation"):
        text, true_label = val_dataset[idx]  # ✅ FIXED: Use 'text' not 'sentence'

        prompt = f"""Is this a good English-French translation pair?

Pair: {text}
Answer (good/bad):"""

        enc = tokenizer(
            prompt,
            return_tensors="pt",
            padding=False,
            truncation=True,
            max_length=256,
        ).to(device)

        out = model(**enc)
        logits = out.logits
        next_token_logits = logits[0, -1, :]
        next_token_id = torch.argmax(next_token_logits).item()
        generated_text = tokenizer.decode([next_token_id]).strip().lower()

        if "good" in generated_text or generated_text.startswith("g"):
            prediction = 1
        elif "bad" in generated_text or generated_text.startswith("b"):
            prediction = 0
        else:
            good_id = tokenizer.encode(" good")[-1]
            bad_id = tokenizer.encode(" bad")[-1]
            prediction = 1 if next_token_logits[good_id] > next_token_logits[bad_id] else 0

        baseline_predictions.append(prediction)
        baseline_labels.append(true_label)

baseline_predictions = np.array(baseline_predictions)
baseline_labels = np.array(baseline_labels)

baseline_acc = accuracy_score(baseline_labels, baseline_predictions)
baseline_p, baseline_r, baseline_f1, _ = precision_recall_fscore_support(
    baseline_labels, baseline_predictions, average='binary'
)

print(f"\nBaseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"F1-Score: {baseline_f1:.4f}")

# SAVE BASELINE (EXACT SAME FORMAT)
with open('/kaggle/working/results_europarl_qwen3/baseline_results.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("BASELINE - ZERO-SHOT PROMPTING (Qwen3-0.6B on Europarl)\n")
    f.write("="*70 + "\n\n")
    f.write(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\n")
    f.write(f"Precision: {baseline_p:.4f}\n")
    f.write(f"Recall: {baseline_r:.4f}\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n")

print("✓ Baseline saved - READY TO CONTINUE!")

# ================================================================
# 4. STEP 2: Layer-wise Analysis (EXACT SAME AS YOUR QWEN MRPC)
# ================================================================
print("\n" + "="*70)
print("STEP 2: LAYER-WISE ANALYSIS (Qwen3-0.6B + Transcoders on Europarl)")
print("="*70)

layer_performance = {}
sae_feature_stats = {}
TOP_K = 10
release_qwen = "mwhanna-qwen3-0.6b-transcoders-lowl0"  # SAME AS YOUR MRPC

for layer_num in range(n_layers):
    print(f"\n{'='*70}")
    print(f"LAYER {layer_num} ANALYSIS")
    print(f"{'='*70}")

    # ------------------------------------------------
    # Part A: Raw representation classifier (EXACT SAME)
    # ------------------------------------------------
    print("\n--- Part A: Raw Representation Classifier ---")

    print("Extracting training representations...")
    train_reps = []
    train_labels = []

    with torch.no_grad():
        for idx in tqdm(range(len(train_dataset)), desc=f"Train L{layer_num}"):
            combined = combine_sentences(train_dataset[idx])
            label = train_dataset[idx][1]

            layer_h = get_layer_rep(combined, layer_num)
            pooled = layer_h.mean(dim=1)
            pooled_flat = pooled[0].to(torch.float32).cpu().numpy().flatten()

            train_reps.append(pooled_flat)
            train_labels.append(label)

    X_train = np.array(train_reps)
    y_train = np.array(train_labels)

    print("Extracting validation representations...")
    val_reps = []
    val_labels_raw = []

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"Val L{layer_num}"):
            combined = combine_sentences(val_dataset[idx])
            label = val_dataset[idx][1]

            layer_h = get_layer_rep(combined, layer_num)
            pooled = layer_h.mean(dim=1)
            pooled_flat = pooled[0].to(torch.float32).cpu().numpy().flatten()

            val_reps.append(pooled_flat)
            val_labels_raw.append(label)

    X_val = np.array(val_reps)
    y_val = np.array(val_labels_raw)

    print("Training classifier...")
    clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
    clf.fit(X_train, y_train)

    predictions = clf.predict(X_val)

    acc = accuracy_score(y_val, predictions)
    p, r, f1, _ = precision_recall_fscore_support(y_val, predictions, average='binary')
    cm = confusion_matrix(y_val, predictions)

    layer_performance[layer_num] = {
        "accuracy": acc,
        "precision": p,
        "recall": r,
        "f1_score": f1,
        "improvement": acc - baseline_acc,
        "confusion_matrix": cm,
    }

    print(f"Layer {layer_num} Accuracy: {acc:.4f} ({acc*100:.2f}%)")
    print(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%")

    # ------------------------------------------------
    # Part B: SAE feature analysis (EXACT SAME LOGIC)
    # ------------------------------------------------
    print("\n--- Part B: SAE Feature Analysis (Transcoders) ---")

    sae_id_qwen = f"layer_{layer_num}"
    try:
        try:
            sae_qwen = SAE.from_pretrained(release_qwen, sae_id_qwen)
        except:
            sae_qwen = SAE.from_pretrained(release_qwen, sae_id_qwen)[0]

        sae_qwen.to(device)
        sae_qwen.eval()
        print(f"✓ Loaded Qwen3 SAE: {sae_id_qwen}")
    except Exception as e:
        print(f"❌ Could not load SAE for layer {layer_num}: {e}")
        sae_feature_stats[layer_num] = {}
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        continue

    print("Extracting SAE features...")

    pos_active_features = set()  # Equivalent translation
    neg_active_features = set()  # Divergent translation

    pos_feature_activations = {}
    neg_feature_activations = {}

    pos_feature_counts = Counter()
    neg_feature_counts = Counter()

    total_pos = 0
    total_neg = 0

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"SAE L{layer_num}"):
            combined = combine_sentences(val_dataset[idx])
            label = val_dataset[idx][1]

            layer_h = get_layer_rep(combined, layer_num)
            sae_feats = sae_qwen.encode(layer_h)

            pooled_features = sae_feats.mean(dim=1)[0].to(torch.float32).cpu().numpy().flatten()

            active_indices = np.where(pooled_features > 0)[0]
            active_values = pooled_features[active_indices]

            if label == 1:  # Equivalent (good translation)
                total_pos += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    pos_active_features.add(feat_idx)
                    pos_feature_counts[feat_idx] += 1
                    pos_feature_activations.setdefault(feat_idx, []).append(act_val)
            else:  # Divergent (bad translation)
                total_neg += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    neg_active_features.add(feat_idx)
                    neg_feature_counts[feat_idx] += 1
                    neg_feature_activations.setdefault(feat_idx, []).append(act_val)

    common_features = pos_active_features & neg_active_features
    pos_only_features = pos_active_features - neg_active_features
    neg_only_features = neg_active_features - pos_active_features

    pos_avg_activations = {f: np.mean(acts) for f, acts in pos_feature_activations.items()}
    neg_avg_activations = {f: np.mean(acts) for f, acts in neg_feature_activations.items()}

    top5_pos_by_activation = sorted(
        pos_avg_activations.items(), key=lambda x: x[1], reverse=True
    )[:5]
    top5_neg_by_activation = sorted(
        neg_avg_activations.items(), key=lambda x: x[1], reverse=True
    )[:5]

    topk_pos_by_frequency = pos_feature_counts.most_common(TOP_K)
    topk_neg_by_frequency = neg_feature_counts.most_common(TOP_K)

    top5_pos_by_frequency = pos_feature_counts.most_common(5)
    top5_neg_by_frequency = neg_feature_counts.most_common(5)

    topk_pos_ids = {feat for feat, _ in topk_pos_by_frequency}
    topk_neg_ids = {feat for feat, _ in topk_neg_by_frequency}
    topk_common_ids = topk_pos_ids & topk_neg_ids

    sae_feature_stats[layer_num] = {
        "total_pos_features": len(pos_active_features),
        "total_neg_features": len(neg_active_features),
        "common_features": len(common_features),
        "pos_only_features": len(pos_only_features),
        "neg_only_features": len(neg_only_features),
        "top5_pos_by_activation": top5_pos_by_activation,
        "top5_neg_by_activation": top5_neg_by_activation,
        "top5_pos_by_frequency": top5_pos_by_frequency,
        "top5_neg_by_frequency": top5_neg_by_frequency,
        "topk_pos_by_frequency": topk_pos_by_frequency,
        "topk_neg_by_frequency": topk_neg_by_frequency,
        "topk_common_ids": topk_common_ids,
        "total_pos_samples": total_pos,
        "total_neg_samples": total_neg,
    }

    # ------------------------------------------------
    # Save per-layer analysis (EXACT SAME FORMAT)
    # ------------------------------------------------
    with open(f"/kaggle/working/results_europarl_qwen3/layer_{layer_num}_complete_analysis.txt", "w") as f:
        f.write("="*70 + "\n")
        f.write(f"LAYER {layer_num} - COMPLETE ANALYSIS (Qwen3-0.6B on Europarl)\n")
        f.write("="*70 + "\n\n")

        f.write("A. LAYER PERFORMANCE (Raw Representations)\n")
        f.write("-"*70 + "\n")
        f.write(f"Accuracy: {acc:.4f} ({acc*100:.2f}%)\n")
        f.write(f"Precision: {p:.4f}\n")
        f.write(f"Recall: {r:.4f}\n")
        f.write(f"F1-Score: {f1:.4f}\n")
        f.write(f"Baseline Accuracy: {baseline_acc:.4f}\n")
        f.write(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%\n\n")

        f.write("Confusion Matrix:\n")
        f.write("              Predicted\n")
        f.write("         Bad   Good\n")
        f.write(f"Actual 0  [{cm[0,0]:5d}  {cm[0,1]:5d}]\n")
        f.write(f"       1  [{cm[1,0]:5d}  {cm[1,1]:5d}]\n\n")

        f.write("B. SAE FEATURE COUNTS\n")
        f.write("-"*70 + "\n")
        f.write(f"Total EQUIVALENT samples: {total_pos}\n")
        f.write(f"Total DIVERGENT samples: {total_neg}\n\n")
        f.write(f"Total features activated for EQUIVALENT: {len(pos_active_features)}\n")
        f.write(f"Total features activated for DIVERGENT: {len(neg_active_features)}\n")
        f.write(f"COMMON features: {len(common_features)}\n")
        f.write(f"UNIQUE to EQUIVALENT: {len(pos_only_features)}\n")
        f.write(f"UNIQUE to DIVERGENT: {len(neg_only_features)}\n\n")

        f.write("C. TOP 5 MOST ACTIVATING FEATURES (EQUIVALENT)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, avg_act) in enumerate(top5_pos_by_activation, 1):
            freq = pos_feature_counts[feat_idx]
            pct = freq / total_pos * 100 if total_pos > 0 else 0.0
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(f"{rank}. Feature {feat_idx}: avg_activation={avg_act:.4f}, frequency={freq}/{total_pos} ({pct:.1f}%) | {status}\n")

        f.write("\nD. TOP 5 MOST ACTIVATING FEATURES (DIVERGENT)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, avg_act) in enumerate(top5_neg_by_activation, 1):
            freq = neg_feature_counts[feat_idx]
            pct = freq / total_neg * 100 if total_neg > 0 else 0.0
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(f"{rank}. Feature {feat_idx}: avg_activation={avg_act:.4f}, frequency={freq}/{total_neg} ({pct:.1f}%) | {status}\n")

        f.write("\nE. TOP 5 MOST FREQUENT FEATURES (EQUIVALENT)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, count) in enumerate(top5_pos_by_frequency, 1):
            pct = count / total_pos * 100 if total_pos > 0 else 0.0
            avg_act = pos_avg_activations.get(feat_idx, 0.0)
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(f"{rank}. Feature {feat_idx}: count={count} ({pct:.1f}%), avg_activation={avg_act:.4f} | {status}\n")

        f.write("\nF. TOP 5 MOST FREQUENT FEATURES (DIVERGENT)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, count) in enumerate(top5_neg_by_frequency, 1):
            pct = count / total_neg * 100 if total_neg > 0 else 0.0
            avg_act = neg_avg_activations.get(feat_idx, 0.0)
            status = "COMMON" if feat_idx in common_features else "UNIQUE"
            f.write(f"{rank}. Feature {feat_idx}: count={count} ({pct:.1f}%), avg_activation={avg_act:.4f} | {status}\n")

    print(f"✓ Layer {layer_num} complete analysis saved")

    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ================================================================
# 5. FINAL SUMMARY (EXACT SAME FORMAT)
# ================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

best_layer = max(layer_performance.items(), key=lambda x: x[1]["accuracy"])
worst_layer = min(layer_performance.items(), key=lambda x: x[1]["accuracy"])

print(f"{'Layer':<6} {'Acc%':<8} {'Improv':<10} {'Eq Feat':<10} {'Div Feat':<10} {'Common':<10}")
print("-"*90)
for layer_num in sorted(layer_performance.keys()):
    perf = layer_performance[layer_num]
    stats = sae_feature_stats.get(layer_num, {})
    print(f"L{layer_num:<5} {perf['accuracy']*100:5.2f}%   {perf['improvement']*100:+6.2f}%    {stats.get('total_pos_features', 'N/A'):<10} {stats.get('total_neg_features', 'N/A'):<10} {stats.get('common_features', 'N/A'):<10}")

print(f"\n✓ Best layer: Layer {best_layer[0]} ({best_layer[1]['accuracy']*100:.2f}% accuracy)")
print(f"✓ Worst layer: Layer {worst_layer[0]} ({worst_layer[1]['accuracy']*100:.2f}% accuracy)")

top_2_layers = sorted(layer_performance.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:2]
top_2_layer_nums = [layer_num for layer_num, _ in top_2_layers]
print(f"\nTop 2 layers for interpretability: {top_2_layer_nums}")

with open("/kaggle/working/results_europarl_qwen3/final_summary.txt", "w") as f:
    f.write("="*70 + "\n")
    f.write("FINAL SUMMARY - Qwen3-0.6B EUROPARL ANALYSIS\n")
    f.write("="*70 + "\n\n")

    f.write("1. BASELINE (Zero-Shot)\n")
    f.write("-"*70 + "\n")
    f.write(f"Accuracy: {baseline_acc*100:.2f}%\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n\n")

    f.write("2. LAYER PERFORMANCE TABLE\n")
    f.write("-"*70 + "\n")
    f.write(f"{'Layer':<6} {'Acc%':<8} {'Improv':<10} {'Eq Feat':<10} {'Div Feat':<10} {'Common':<10}\n")
    f.write("-"*90 + "\n")
    for layer_num in sorted(layer_performance.keys()):
        perf = layer_performance[layer_num]
        stats = sae_feature_stats.get(layer_num, {})
        f.write(f"L{layer_num:<5} {perf['accuracy']*100:5.2f}%   {perf['improvement']*100:+6.2f}%    {stats.get('total_pos_features', 'N/A'):<10} {stats.get('total_neg_features', 'N/A'):<10} {stats.get('common_features', 'N/A'):<10}\n")

    f.write(f"\n3. Best layer: L{layer_num} ({best_layer[1]['accuracy']*100:.2f}%)\n")
    f.write(f"Top 2 layers: {top_2_layer_nums}\n")

print("\n✓ All results saved to /kaggle/working/results_europarl_qwen3/")
print("="*70)