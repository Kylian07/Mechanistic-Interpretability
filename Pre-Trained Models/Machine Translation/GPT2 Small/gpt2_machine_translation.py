# -*- coding: utf-8 -*-
"""GPT2 machine translation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/gpt2-machine-translation.ca78d66d-ab87-4bd9-996b-7c6c527baee2.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20260117/auto/storage/goog4_request%26X-Goog-Date%3D20260117T152311Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6164562ef5736e8328aa07498bbc5eb808665ee23c312c0f05378a4d5b6c03c293cd6ba742c42da878cb64c111c06d70b2f38ed5bf72cbf1a36ff5fb051ba2ba0b003b95720d3f589bec02912ca68d0e25a3c077dac22d9d35ebae62c7910e243281dcc2522f3d02225ab3b65c73d9cb72c9d674fb602b3917d9e524f65b2a21dce2433a1abf75e45356bf70f0b434c37423a42a552ca40ac256b674877c0d31e8cfe10d71053d9731c205c052ad8634e1537d0f86c6b09bd28599d74c76a6afe03ac25ab10cfd5d17841d4f0e05b70f96c9d4a3de75a27f1b5135387a00c3c2d6350b3ec54ddf7f94277c4cbb5a2f489de71401cbc41062815827c962732008
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
khajasafiuddin_englishfrench_corpus_path = kagglehub.dataset_download('khajasafiuddin/englishfrench-corpus')

print('Data source import complete.')

!pip uninstall -y numpy pandas
!pip install numpy==1.26.4 pandas==2.1.4 -q

import numpy as np
import pandas as pd

print(np.__version__)
print(pd.__version__)

# -*- coding: utf-8 -*-
"""
English-French Translation Analysis with GPT-2 and SAEs
Kaggle-ready code for machine translation task
"""

# ============================================================================
# INSTALLATION & IMPORTS
# ============================================================================
!pip install sae-lens groq transformers datasets torch scikit-learn sacrebleu -q

import os
import gc
import re
import random
import torch
import numpy as np
from collections import Counter
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from transformer_lens import HookedTransformer
from sae_lens import SAE
from groq import Groq
from scipy.stats import pearsonr
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

# ============================================================================
# SETUP
# ============================================================================
os.makedirs('/kaggle/working/translation_results', exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ================================================================
# 1. SETUP
# ================================================================
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.makedirs('/kaggle/working/results_europarl', exist_ok=True)

os.environ['HF_HOME'] = '/kaggle/working/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'



# ================================================================
# FIXED DATA LOADING - BULLETPROOF VERSION
# ================================================================
def load_europarl_pairs(en_path, fr_path, max_samples=30000):
    with open(en_path, 'r', encoding='utf-8') as f_en, \
         open(fr_path, 'r', encoding='utf-8') as f_fr:
        en_lines = [line.strip() for line in f_en if line.strip()]
        fr_lines = [line.strip() for line in f_fr if line.strip()]

    pairs = [(en, fr) for en, fr in zip(en_lines[:max_samples], fr_lines[:max_samples])]
    return pairs

def create_balanced_dataset(pairs, n_train=10000, n_val=2000):
    """Guaranteed 50/50 class balance"""

    # Positive examples (real translations)
    pos_train = random.sample(pairs, n_train//2)
    pos_val = random.sample([p for p in pairs if p not in pos_train], n_val//2)

    # Negative examples (shuffled)
    neg_train, neg_val = [], []
    used_pairs = set()

    while len(neg_train) < n_train//2:
        en, _ = random.choice(pairs)
        _, wrong_fr = random.choice(pairs)
        pair = (en, wrong_fr)
        if pair not in used_pairs:
            used_pairs.add(pair)
            neg_train.append(pair)

    while len(neg_val) < n_val//2:
        en, _ = random.choice(pairs)
        _, wrong_fr = random.choice(pairs)
        pair = (en, wrong_fr)
        if pair not in used_pairs:
            used_pairs.add(pair)
            neg_val.append(pair)

    # Create datasets
    train_dataset = [(combine_sentences(p), 1) for p in pos_train] + \
                    [(combine_sentences(p), 0) for p in neg_train]
    val_dataset = [(combine_sentences(p), 1) for p in pos_val] + \
                  [(combine_sentences(p), 0) for p in neg_val]

    random.shuffle(train_dataset)
    random.shuffle(val_dataset)

    return train_dataset, val_dataset

# EXECUTE
EN_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.en"
FR_PATH = "/kaggle/input/englishfrench-corpus/europarl-v7.fr-en.fr"

all_pairs = load_europarl_pairs(EN_PATH, FR_PATH)
train_dataset, val_dataset = create_balanced_dataset(all_pairs)

print("âœ… FINAL DATASET:")
print(f"Train: {len(train_dataset)} = {np.unique([l for _,l in train_dataset], return_counts=True)}")
print(f"Val:   {len(val_dataset)} = {np.unique([l for _,l in val_dataset], return_counts=True)}")

# ================================================================
# 3. GROQ SETUP (OPTIONAL - COMMENT OUT IF NO API KEY)
# ================================================================
try:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")
    GROQ_AVAILABLE = True
except:
    GROQ_AVAILABLE = False
    print("No Groq API key found - skipping automated interpretability")

if GROQ_AVAILABLE:
    client = Groq(api_key=os.environ["GROQ_API_KEY"])

    def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
        resp = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "You are a translation interpretability assistant."},
                {"role": "user", "content": prompt},
            ],
            model=model,
            max_tokens=max_tokens,
            temperature=0.7,
        )
        return resp.choices[0].message.content.strip()

# ================================================================
# 3. LOAD GPT-2 SMALL
# ================================================================
print("\nLoading GPT-2 Small model...")
model = HookedTransformer.from_pretrained("gpt2-small", device=device)
model.eval()

n_layers = model.cfg.n_layers
print(f"Model has {n_layers} layers (0-{n_layers-1})")

# ================================================================
# 4. STEP 1: BASELINE - ZERO-SHOT (IDENTICAL FORMAT)
# ================================================================
print("\n" + "="*70)
print("STEP 1: BASELINE - ZERO-SHOT TRANSLATION QUALITY")
print("="*70)

baseline_predictions = []
baseline_labels = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Zero-Shot Evaluation"):
        sentence, true_label = val_dataset[idx]

        prompt = f"""Is this a good English-French translation pair?

Pair: {sentence}
Quality (good/bad):"""

        tokens = model.to_tokens(prompt)
        logits = model(tokens)
        next_token_logits = logits[0, -1, :]

        good_id = model.to_single_token(" good")
        bad_id = model.to_single_token(" bad")

        prediction = 1 if next_token_logits[good_id] > next_token_logits[bad_id] else 0

        baseline_predictions.append(prediction)
        baseline_labels.append(true_label)

baseline_predictions = np.array(baseline_predictions)
baseline_labels = np.array(baseline_labels)

baseline_acc = accuracy_score(baseline_labels, baseline_predictions)
baseline_p, baseline_r, baseline_f1, _ = precision_recall_fscore_support(
    baseline_labels, baseline_predictions, average='binary'
)

print(f"\nBaseline Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"F1-Score: {baseline_f1:.4f}")

with open('/kaggle/working/results_europarl/baseline_results.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("BASELINE - ZERO-SHOT TRANSLATION QUALITY\n")
    f.write("="*70 + "\n\n")
    f.write(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\n")
    f.write(f"Precision: {baseline_p:.4f}\n")
    f.write(f"Recall: {baseline_r:.4f}\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n")

print("âœ“ Baseline saved")

# ================================================================
# 5. STEP 2: LAYER-WISE ANALYSIS (PROBES + SAEs) - IDENTICAL FORMAT
# ================================================================
print("\n" + "="*70)
print("STEP 2: LAYER-WISE ANALYSIS")
print("="*70)

layer_performance = {}
sae_feature_stats = {}
TOP_K = 10
release = "gpt2-small-hook-z-kk"

for layer_num in range(n_layers):
    print(f"\n{'='*70}")
    print(f"LAYER {layer_num} ANALYSIS")
    print(f"{'='*70}")

    hook_name = f"blocks.{layer_num}.attn.hook_z"

    # --------------------------------------------------------------
    # Part A: Raw representation classifier (linear probe)
    # --------------------------------------------------------------
    print("\n--- Part A: Raw Representation Classifier ---")

    print("Extracting training representations...")
    train_reps = []
    train_labels = []

    with torch.no_grad():
        for idx in tqdm(range(min(5000, len(train_dataset))), desc=f"Train L{layer_num}"):
            sentence, label = train_dataset[idx]
            tokens = model.to_tokens(sentence)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            train_reps.append(pooled_flat)
            train_labels.append(label)

    X_train = np.array(train_reps)
    y_train = np.array(train_labels)

    print("Extracting validation representations...")
    val_reps = []
    val_labels_raw = []

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"Val L{layer_num}"):
            sentence, label = val_dataset[idx]
            tokens = model.to_tokens(sentence)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])

            layer_acts = cache[hook_name]
            pooled = layer_acts.mean(dim=1)
            pooled_flat = pooled.cpu().numpy().flatten()

            val_reps.append(pooled_flat)
            val_labels_raw.append(label)

    X_val = np.array(val_reps)
    y_val = np.array(val_labels_raw)

    print("Training classifier...")
    clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')
    clf.fit(X_train, y_train)
    predictions = clf.predict(X_val)

    acc = accuracy_score(y_val, predictions)
    p, r, f1, _ = precision_recall_fscore_support(y_val, predictions, average='binary')
    cm = confusion_matrix(y_val, predictions)

    layer_performance[layer_num] = {
        "accuracy": acc,
        "precision": p,
        "recall": r,
        "f1_score": f1,
        "improvement": acc - baseline_acc,
        "confusion_matrix": cm,
    }

    print(f"Layer {layer_num} Accuracy: {acc:.4f} ({acc*100:.2f}%)")
    print(f"Improvement over baseline: {(acc - baseline_acc)*100:+.2f}%")

    # --------------------------------------------------------------
    # Part B: SAE feature analysis
    # --------------------------------------------------------------
    print("\n--- Part B: SAE Feature Analysis ---")

    try:
        sae_id = f"blocks.{layer_num}.hook_z"
        try:
            sae = SAE.from_pretrained(release, sae_id)
        except:
            sae = SAE.from_pretrained(release, sae_id)[0]

        sae.to(device)
        sae.eval()
        print(f"Loaded SAE: {sae_id}")

    except Exception as e:
        print(f"Could not load SAE for layer {layer_num}: {e}")
        continue

    print("Extracting SAE features...")

    eq_active_features = set()  # Translation equivalence (label=1)
    div_active_features = set()  # Divergent (label=0)

    eq_feature_activations = {}
    div_feature_activations = {}

    eq_feature_counts = Counter()
    div_feature_counts = Counter()

    total_eq = 0
    total_div = 0

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"SAE L{layer_num}"):
            sentence, label = val_dataset[idx]

            tokens = model.to_tokens(sentence)
            _, cache = model.run_with_cache(tokens, names_filter=[hook_name])

            hook_acts = cache[hook_name]
            sae_feature_acts = sae.encode(hook_acts)

            pooled_features = sae_feature_acts.mean(dim=1)
            pooled_features = pooled_features.cpu().numpy().flatten()

            active_indices = np.where(pooled_features > 0)[0]
            active_values = pooled_features[active_indices]

            if label == 1:  # Equivalence
                total_eq += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    eq_active_features.add(feat_idx)
                    eq_feature_counts[feat_idx] += 1
                    eq_feature_activations.setdefault(feat_idx, []).append(act_val)
            else:  # Divergent
                total_div += 1
                for feat_idx, act_val in zip(active_indices, active_values):
                    div_active_features.add(feat_idx)
                    div_feature_counts[feat_idx] += 1
                    div_feature_activations.setdefault(feat_idx, []).append(act_val)

    # Feature statistics
    common_features = eq_active_features & div_active_features
    eq_only_features = eq_active_features - div_active_features
    div_only_features = div_active_features - eq_active_features

    eq_avg_activations = {f: np.mean(acts) for f, acts in eq_feature_activations.items()}
    div_avg_activations = {f: np.mean(acts) for f, acts in div_feature_activations.items()}

    top5_eq_by_activation = sorted(eq_avg_activations.items(), key=lambda x: x[1], reverse=True)[:5]
    top5_div_by_activation = sorted(div_avg_activations.items(), key=lambda x: x[1], reverse=True)[:5]

    topk_eq_by_frequency = eq_feature_counts.most_common(TOP_K)
    topk_div_by_frequency = div_feature_counts.most_common(TOP_K)

    top5_eq_by_frequency = eq_feature_counts.most_common(5)
    top5_div_by_frequency = div_feature_counts.most_common(5)

    topk_eq_ids = {feat for feat, _ in topk_eq_by_frequency}
    topk_div_ids = {feat for feat, _ in topk_div_by_frequency}
    topk_common_ids = topk_eq_ids & topk_div_ids

    sae_feature_stats[layer_num] = {
        "total_eq_features": len(eq_active_features),
        "total_div_features": len(div_active_features),
        "common_features": len(common_features),
        "eq_only_features": len(eq_only_features),
        "div_only_features": len(div_only_features),
        "top5_eq_by_activation": top5_eq_by_activation,
        "top5_div_by_activation": top5_div_by_activation,
        "top5_eq_by_frequency": top5_eq_by_frequency,
        "top5_div_by_frequency": top5_div_by_frequency,
        "topk_eq_by_frequency": topk_eq_by_frequency,
        "topk_div_by_frequency": topk_div_by_frequency,
        "topk_common_ids": topk_common_ids,
        "total_eq_samples": total_eq,
        "total_div_samples": total_div,
    }

    # Print summary
    print(f"\n=== SAE Feature Statistics for Layer {layer_num} ===")
    print(f"Total EQUIVALENT samples: {total_eq}")
    print(f"Total DIVERGENT samples: {total_div}\n")
    print(f"EQUIVALENCE features: {len(eq_active_features)}")
    print(f"DIVERGENT features: {len(div_active_features)}")
    print(f"COMMON: {len(common_features)}, EQ_ONLY: {len(eq_only_features)}, DIV_ONLY: {len(div_only_features)}")

    # Save per-layer detailed file - EXACT SAME FORMAT
    with open(f"/kaggle/working/results_europarl/layer_{layer_num}_complete_analysis.txt", "w") as f:
        f.write("="*70 + "\n")
        f.write(f"LAYER {layer_num} - COMPLETE ANALYSIS (Europarl Translation)\n")
        f.write("="*70 + "\n\n")

        f.write("A. LAYER PERFORMANCE (Raw Representations)\n")
        f.write("-"*70 + "\n")
        f.write(f"Accuracy: {acc:.4f} ({acc*100:.2f}%)\n")
        f.write(f"Precision: {p:.4f}\n")
        f.write(f"Recall: {r:.4f}\n")
        f.write(f"F1-Score: {f1:.4f}\n")
        f.write(f"Baseline: {baseline_acc:.4f}\n")
        f.write(f"Improvement: {(acc - baseline_acc)*100:+.2f}%\n\n")

        f.write("Confusion Matrix:\n")
        f.write("              Predicted\n")
        f.write("         Div   Eq\n")
        f.write(f"Actual Div [{cm[0,0]:5d} {cm[0,1]:5d}]\n")
        f.write(f"      Eq  [{cm[1,0]:5d} {cm[1,1]:5d}]\n\n")

        f.write("B. SAE FEATURE COUNTS\n")
        f.write("-"*70 + "\n")
        f.write(f"EQUIVALENT samples: {total_eq}\n")
        f.write(f"DIVERGENT samples: {total_div}\n")
        f.write(f"EQUIVALENCE features: {len(eq_active_features)}\n")
        f.write(f"DIVERGENT features: {len(div_active_features)}\n")
        f.write(f"COMMON: {len(common_features)}\n")
        f.write(f"EQ_ONLY: {len(eq_only_features)}\n")
        f.write(f"DIV_ONLY: {len(div_only_features)}\n\n")

        # Top 5 by activation
        f.write("C. TOP 5 EQUIVALENCE FEATURES (by avg activation)\n")
        f.write("-"*70 + "\n")
        for rank, (feat, avg_act) in enumerate(top5_eq_by_activation, 1):
            freq = eq_feature_counts[feat]
            pct = freq/total_eq*100
            status = "COMMON" if feat in common_features else "UNIQUE"
            f.write(f"{rank}. F{feat}: {avg_act:.4f}, freq={freq} ({pct:.1f}%) | {status}\n")

        f.write("\nD. TOP 5 DIVERGENT FEATURES (by avg activation)\n")
        f.write("-"*70 + "\n")
        for rank, (feat, avg_act) in enumerate(top5_div_by_activation, 1):
            freq = div_feature_counts[feat]
            pct = freq/total_div*100
            status = "COMMON" if feat in common_features else "UNIQUE"
            f.write(f"{rank}. F{feat}: {avg_act:.4f}, freq={freq} ({pct:.1f}%) | {status}\n")

        # Top-K frequency
        f.write(f"\nG. TOP {TOP_K} EQUIVALENCE FEATURES (frequency)\n")
        for rank, (feat, cnt) in enumerate(topk_eq_by_frequency, 1):
            f.write(f"{rank}. F{feat}: {cnt}\n")

        f.write(f"\nH. TOP {TOP_K} DIVERGENT FEATURES (frequency)\n")
        for rank, (feat, cnt) in enumerate(topk_div_by_frequency, 1):
            f.write(f"{rank}. F{feat}: {cnt}\n")

    print(f"âœ“ Layer {layer_num} analysis saved")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ================================================================
# 6. FINAL SUMMARY - IDENTICAL FORMAT
# ================================================================
print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

best_layer = max(layer_performance.items(), key=lambda x: x[1]["accuracy"])
worst_layer = min(layer_performance.items(), key=lambda x: x[1]["accuracy"])

print(f"{'Layer':<6} {'Acc%':<8} {'Improv':<10} {'Eq Feat':<10} {'Div Feat':<10} {'Common':<10}")
print("-"*80)
for layer_num in sorted(layer_performance.keys()):
    perf = layer_performance[layer_num]
    stats = sae_feature_stats.get(layer_num, {})
    print(f"L{layer_num:<5} {perf['accuracy']*100:6.2f}% {(perf['improvement']*100):+7.2f}% "
          f"{stats.get('total_eq_features','N/A'):<10} {stats.get('total_div_features','N/A'):<10} "
          f"{stats.get('common_features','N/A'):<10}")

print(f"\nâœ“ Best: Layer {best_layer[0]} ({best_layer[1]['accuracy']*100:.2f}%)")
print(f"âœ“ Worst: Layer {worst_layer[0]} ({worst_layer[1]['accuracy']*100:.2f}%)")

# Save final summary
top_2_layers = sorted(layer_performance.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:2]
top_2_layer_nums = [l[0] for l in top_2_layers]

with open("/kaggle/working/results_europarl/final_summary.txt", "w") as f:
    f.write("="*70 + "\n")
    f.write("FINAL SUMMARY - EUROPARL TRANSLATION ANALYSIS\n")
    f.write("="*70 + "\n\n")

    f.write("1. BASELINE\n")
    f.write(f"Accuracy: {baseline_acc*100:.2f}%\n\n")

    f.write("2. LAYER PERFORMANCE\n")
    f.write("-"*70 + "\n")
    for layer_num in sorted(layer_performance.keys()):
        perf = layer_performance[layer_num]
        stats = sae_feature_stats.get(layer_num, {})
        f.write(f"L{layer_num}: {perf['accuracy']*100:.2f}% (+{perf['improvement']*100:+.2f}%) | "
                f"Eq:{stats.get('total_eq_features',0)} Div:{stats.get('total_div_features',0)}\n")

    f.write(f"\n3. KEY RESULTS\n")
    f.write(f"Best Layer: {best_layer[0]} ({best_layer[1]['accuracy']*100:.2f}%)\n")
    f.write(f"Top 2 Layers: {top_2_layer_nums}\n")

print("\nâœ“ All results saved to /kaggle/working/results_europarl/")
print("="*70)

# ================================================================
# 8. GROQ INTERPRETABILITY FOR TOP 2 LAYERS - IDENTICAL TO MRPC
# ================================================================
print("\n" + "="*70)
print("STEP 3: GROQ INTERPRETABILITY FOR TOP 2 LAYERS")
print("="*70)

# Get top 2 layers (already computed in final summary)
top_2_layers = sorted(layer_performance.items(), key=lambda x: x[1]["accuracy"], reverse=True)[:2]
top_2_layer_nums = [layer_num for layer_num, _ in top_2_layers]

print(f"Top 2 layers for interpretability: {top_2_layer_nums}")

# Dataset wrapper for val_combined (same as MRPC)
class TextDataset(torch.utils.data.Dataset):
    def __init__(self, texts):
        self.texts = texts
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        return self.texts[idx]

val_text_ds = TextDataset([text for text, _ in val_dataset])

# SAE wrapper with activation capture (same as MRPC)
class SAEWithActs(torch.nn.Module):
    def __init__(self, sae_model):
        super().__init__()
        self.sae = sae_model
        self.activations = None
        hook_module = getattr(self.sae, "hook_sae_acts_post", None)
        if hook_module is None:
            raise ValueError("hook_sae_acts_post not found on SAE model.")
        hook_module.register_forward_hook(self._hook)

    def _hook(self, module, inp, out):
        self.activations = out.detach()

    def forward(self, sae_input):
        _ = self.sae(sae_input)
        return self.activations

    def get_acts(self, sae_input):
        self.eval()
        with torch.no_grad():
            _ = self.forward(sae_input)
            return self.activations

# GROQ API setup (same as MRPC)
try:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    os.environ["GROQ_API_KEY"] = user_secrets.get_secret("GROQ_API_KEY")
    GROQ_AVAILABLE = True
    client = Groq(api_key=os.environ["GROQ_API_KEY"])
except:
    GROQ_AVAILABLE = False
    print("âš ï¸ No Groq API key - skipping interpretability")
    print("Results still saved in /kaggle/working/results_europarl/")
    exit()

def chat_with_llama(prompt, model="llama-3.1-8b-instant", max_tokens=150):
    resp = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a translation interpretability assistant analyzing GPT-2 SAE features on English-French translation pairs."},
            {"role": "user", "content": prompt},
        ],
        model=model,
        max_tokens=max_tokens,
        temperature=0.7,
    )
    return resp.choices[0].message.content.strip()

# Global feature counting (same as MRPC)
def top_k_global_features_for_layer(layer_num, sae_model, k=10, max_batches=None):
    sae_wrap = SAEWithActs(sae_model).to(device)
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    counter = Counter()
    dataloader = DataLoader(val_text_ds, batch_size=32)

    for i, batch_texts in enumerate(tqdm(dataloader, desc=f"Counting features L{layer_num}")):
        if max_batches and i >= max_batches:
            break

        batch_tokens = model.to_tokens(batch_texts)
        with torch.no_grad():
            _, cache = model.run_with_cache(batch_tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]
            latents = sae_wrap.get_acts(hook_acts)

        active = (latents > 0).any(dim=1)
        for row in active:
            idxs = row.nonzero(as_tuple=True)[0].tolist()
            counter.update(idxs)

    topk = counter.most_common(k)
    print(f"Layer {layer_num} top-{k} global features: {topk}")
    return [idx for idx, _ in topk]

# Feature extraction (same as MRPC)
def extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=None):
    hook_name = f"blocks.{layer_num}.attn.hook_z"
    acts_all, texts_all = [], []
    dataloader = DataLoader(val_text_ds, batch_size=32)

    for i, batch_texts in enumerate(tqdm(dataloader, desc=f"Extracting L{layer_num} F{feature_idx}")):
        if max_batches and i >= max_batches:
            break

        batch_tokens = model.to_tokens(batch_texts)
        with torch.no_grad():
            _, cache = model.run_with_cache(batch_tokens, names_filter=[hook_name])
            hook_acts = cache[hook_name]
            latents = sae_wrap.get_acts(hook_acts)

        feat_acts = latents[:, :, feature_idx].detach().cpu().numpy()
        for a_sent, txt in zip(feat_acts, batch_texts):
            acts_all.append(a_sent)
            texts_all.append(txt)

    return acts_all, texts_all

def select_top(acts, texts, n=20):
    scores = [np.sum(a) for a in acts]
    idxs = np.argsort(scores)[::-1][:n]
    return [acts[i] for i in idxs], [texts[i] for i in idxs]

# Translation-specific interpretability prompts
def llama_interpretation(top_texts, top_acts):
    prompt = (
        "You are analyzing a sparse autoencoder feature from GPT-2 on English-French translation pairs.\n"
        "Format: 'English_sentence <|endoftext|> French_sentence'\n\n"
        "From these maximally activating examples, give ONE concise sentence (max 20 words) describing:\n"
        "1. What translation pattern this feature detects (lexical, syntactic, semantic)\n"
        "2. Whether it signals EQUIVALENCE (good translation) or DIVERGENCE (bad translation)\n\n"
    )
    for i, (txt, acts) in enumerate(zip(top_texts, top_acts), 1):
        prompt += f"{i}. \"{txt}\"\nActivations: {acts.tolist()[:10]}...\n\n"
    prompt += "Explanation:"
    return chat_with_llama(prompt)

def llama_activation_score(sentence, interpretation):
    prompt = (
        f'Feature interpretation: "{interpretation}"\n\n'
        "Score from 0 (inactive) to 10 (very active) how strongly this feature activates on:\n"
        f'"{sentence}"\n\nScore (single number):'
    )
    resp = chat_with_llama(prompt, max_tokens=16)
    import re
    m = re.search(r"\d+(\.\d+)?", resp)
    return float(m.group()) if m else 0.0

def pearson_score(actual_acts, pred_scores):
    actual = np.array([np.mean(a) for a in actual_acts]) * 10.0  # Scale to 0-10
    pred = np.array(pred_scores)
    corr, _ = pearsonr(actual, pred)
    return corr

# Save interpretability report (same format as MRPC)
def save_interp_report(path, layer_num, feature_idx, interpretation,
                       eval_texts, eval_acts, pred_scores, corr):
    with open(path, "a", encoding="utf-8") as f:
        f.write(f"\nLayer {layer_num}, Feature {feature_idx}\n")
        f.write("-"*70 + "\n")
        f.write("Interpretation: " + interpretation.strip() + "\n\n")
        f.write("Evaluation examples:\n")
        for i, (txt, acts, pred) in enumerate(zip(eval_texts, eval_acts, pred_scores), 1):
            actual = np.mean(acts) * 10.0
            f.write(f"{i}. {txt[:100]}...\n")
            f.write(f"   Actual={actual:.3f}, Predicted={pred:.3f}\n")
        f.write(f"Pearson correlation: {corr:.4f}\n")
        f.write("="*70 + "\n")

# Main interpretability pipeline for one feature (identical to MRPC)
def interpretability_for_feature(layer_num, sae_model, feature_idx, report_path, max_batches=10):
    sae_wrap = SAEWithActs(sae_model).to(device)

    # Extract activations
    acts_all, texts_all = extract_feature_acts(layer_num, sae_wrap, feature_idx, max_batches=max_batches)

    # Top activating examples for interpretation
    top_acts, top_texts = select_top(acts_all, texts_all, n=20)
    interp_acts, interp_texts = top_acts[:5], top_texts[:5]

    print(f"\nTop 5 examples for L{layer_num} F{feature_idx}:")
    for i, txt in enumerate(interp_texts, 1):
        print(f"  {i}. {txt[:80]}...")

    # Get LLM interpretation
    interpretation = llama_interpretation(interp_texts, interp_acts)
    print(f"\nðŸ§  Interpretation: {interpretation}")

    # Evaluation on held-out examples
    used = set(interp_texts)
    rest = [(a, t) for a, t in zip(acts_all, texts_all) if t not in used]
    if len(rest) < 5:
        print("âš ï¸ Insufficient held-out examples")
        return None

    rest_acts, rest_texts = zip(*rest[:100])  # Limit for speed
    scores = [np.sum(a) for a in rest_acts]
    idxs = np.argsort(scores)[::-1][:5]

    eval_acts = [rest_acts[i] for i in idxs]
    eval_texts = [rest_texts[i] for i in idxs]

    pred_scores = [llama_activation_score(txt, interpretation) for txt in eval_texts]
    corr = pearson_score(eval_acts, pred_scores)

    print(f"ðŸ“Š Pearson correlation: {corr:.4f}")

    # Save results
    save_interp_report(report_path, layer_num, feature_idx, interpretation,
                       eval_texts, eval_acts, pred_scores, corr)

    return corr

# ================================================================
# EXECUTE INTERPRETABILITY FOR TOP 2 LAYERS
# ================================================================
INTERP_TOP_K = 5  # Top 5 features per layer (reduced for speed)
release = "gpt2-small-hook-z-kk"

for layer_idx in top_2_layer_nums:
    print(f"\n{'='*70}")
    print(f"LAYER {layer_idx}: GROQ INTERPRETABILITY (Top-{INTERP_TOP_K} features)")
    print(f"{'='*70}")

    sae_id = f"blocks.{layer_idx}.hook_z"
    try:
        try:
            sae_model = SAE.from_pretrained(release, sae_id)
        except:
            sae_model = SAE.from_pretrained(release, sae_id)[0]
        sae_model.to(device).eval()
        print(f"âœ“ Loaded SAE: {sae_id}")
    except Exception as e:
        print(f"âŒ Could not load SAE for layer {layer_idx}: {e}")
        continue

    # Get globally most frequent features
    topk_feats = top_k_global_features_for_layer(
        layer_num=layer_idx,
        sae_model=sae_model,
        k=INTERP_TOP_K,
        max_batches=20  # Process ~640 examples per layer
    )

    # Interpretability report
    report_file = f"/kaggle/working/results_europarl/layer{layer_idx}_top{INTERP_TOP_K}_interpretability.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"TRANSLATION INTERPRETABILITY REPORT\n")
        f.write(f"Layer {layer_idx} - Top {INTERP_TOP_K} Global Features\n")
        f.write("="*70 + "\n\n")

    # Analyze each top feature
    correlations = []
    for feat_idx in topk_feats:
        print(f"\nðŸ” Analyzing L{layer_idx} Feature {feat_idx}...")
        corr = interpretability_for_feature(
            layer_num=layer_idx,
            sae_model=sae_model,
            feature_idx=feat_idx,
            report_path=report_file,
            max_batches=10  # ~320 examples per feature
        )
        if corr is not None:
            correlations.append(corr)

    # Summary statistics
    if correlations:
        avg_corr = np.mean(correlations)
        print(f"\nðŸ“ˆ Layer {layer_idx} average correlation: {avg_corr:.4f}")
        with open(report_file, "a") as f:
            f.write(f"\nSUMMARY: Average Pearson correlation = {avg_corr:.4f}\n")
    else:
        print(f"âš ï¸ No valid correlations for layer {layer_idx}")

    # Cleanup
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ================================================================
# FINAL INTERPRETABILITY SUMMARY
# ================================================================
print("\n" + "="*70)
print("âœ“ INTERPRETABILITY ANALYSIS COMPLETE")
print("="*70)
print("Generated files:")
print("  /kaggle/working/results_europarl/layerX_top5_interpretability.txt")
print(f"  Top layers analyzed: {top_2_layer_nums}")
print("\nðŸŽ‰ FULL EUROPARL PIPELINE COMPLETE!")
print("All files match SST2/MRPC format perfectly!")