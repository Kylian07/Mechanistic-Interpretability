# -*- coding: utf-8 -*-
"""Pretrained GPT2_SAE_SAElens

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kylian007/pretrained-gpt2-sae-saelens.23486761-8142-456d-b73b-2f042327f5bb.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251204/auto/storage/goog4_request%26X-Goog-Date%3D20251204T214846Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5e9b89da980671b45242c9e4de2d8e460967ef17f06cde1dab166bb1c98e7d921a9875e9ccd237602afbaf52aa26835bac232c688ed9931b5f728cfb073ef05439b26acbabc5a792018356b849a0e0f0fc063c95f27bc7821cfaa9c75b4e602681560e5cab83c069f740ab36861965933f99716c0944a3307503b9d2b8f1f43496c7759aadd0a5154c9bddcd31b605e358cd4b72dbe235957f78a8ba361965d73ccb9ebd01af475e03ac4d81a04a9b9624134a72e3c202899d1960769a25053b0cfe7d707a7219e0ccbc70a10659f39ba71dc7fad96ac0d591aa37f66f1668bb2fba4051077c8c5f5e84c3819046a7355f2b16a1f9f0e73baf487cd6c43c4883
"""

!pip install sae-lens

import torch
from datasets import load_dataset
from transformer_lens import HookedTransformer
from sae_lens import SAE
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import numpy as np
from collections import Counter
import os

# Kaggle directories
os.makedirs('/kaggle/working/hf_cache', exist_ok=True)
os.makedirs('/kaggle/working/results', exist_ok=True)

os.environ['HF_HOME'] = '/kaggle/working/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/kaggle/working/hf_cache'

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load GPT-2 Small
print("Loading GPT-2 Small model...")
model = HookedTransformer.from_pretrained("gpt2-small", device=device)
model.eval()

print(f"Model has {model.cfg.n_layers} layers (0-{model.cfg.n_layers-1})")

# Load SST-2 dataset - ONLY VALIDATION SET
print("Loading SST-2 validation dataset...")
dataset = load_dataset("glue", "sst2")
val_dataset = dataset["validation"]

print(f"Validation samples: {len(val_dataset)}")

# ============================================================================
# STEP 1: Record Baseline Performance - Zero-Shot Prompting
# ============================================================================
print("\n" + "="*70)
print("STEP 1: BASELINE - ZERO-SHOT PROMPTING")
print("="*70)

baseline_predictions = []
baseline_labels = []

with torch.no_grad():
    for idx in tqdm(range(len(val_dataset)), desc="Zero-Shot Evaluation"):
        sentence = val_dataset[idx]["sentence"]
        true_label = val_dataset[idx]["label"]

        prompt = f"""Classify the sentiment as positive or negative.

Sentence: {sentence}
Sentiment:"""

        tokens = model.to_tokens(prompt)
        logits = model(tokens)

        next_token_logits = logits[0, -1, :]
        next_token_id = torch.argmax(next_token_logits).item()
        generated_text = model.tokenizer.decode([next_token_id]).strip().lower()

        if "positive" in generated_text or generated_text.startswith("pos"):
            prediction = 1
        elif "negative" in generated_text or generated_text.startswith("neg"):
            prediction = 0
        else:
            pos_token_id = model.tokenizer.encode(" positive")[0]
            neg_token_id = model.tokenizer.encode(" negative")[0]

            if next_token_logits[pos_token_id] > next_token_logits[neg_token_id]:
                prediction = 1
            else:
                prediction = 0

        baseline_predictions.append(prediction)
        baseline_labels.append(true_label)

baseline_predictions = np.array(baseline_predictions)
baseline_labels = np.array(baseline_labels)

baseline_acc = accuracy_score(baseline_labels, baseline_predictions)
baseline_p, baseline_r, baseline_f1, _ = precision_recall_fscore_support(
    baseline_labels, baseline_predictions, average='binary'
)
baseline_cm = confusion_matrix(baseline_labels, baseline_predictions)

print("\n### BASELINE RESULTS ###")
print(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
print(f"Precision: {baseline_p:.4f}")
print(f"Recall: {baseline_r:.4f}")
print(f"F1-Score: {baseline_f1:.4f}")
print(f"\nConfusion Matrix:")
print(f"              Neg    Pos")
print(f"Actual Neg  [{baseline_cm[0,0]:5d}  {baseline_cm[0,1]:5d}]")
print(f"       Pos  [{baseline_cm[1,0]:5d}  {baseline_cm[1,1]:5d}]")

# Save baseline results to TXT
with open('/kaggle/working/results/baseline_results.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("BASELINE RESULTS - ZERO-SHOT PROMPTING\n")
    f.write("="*70 + "\n\n")
    f.write("Method: Pretrained GPT-2 only (no training)\n\n")
    f.write(f"Accuracy: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\n")
    f.write(f"Precision: {baseline_p:.4f}\n")
    f.write(f"Recall: {baseline_r:.4f}\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n\n")
    f.write("Confusion Matrix:\n")
    f.write("              Predicted\n")
    f.write("              Neg    Pos\n")
    f.write(f"Actual Neg  [{baseline_cm[0,0]:5d}  {baseline_cm[0,1]:5d}]\n")
    f.write(f"       Pos  [{baseline_cm[1,0]:5d}  {baseline_cm[1,1]:5d}]\n")

print("✓ Baseline saved to baseline_results.txt")

# ============================================================================
# STEP 2: Extract SAE Features and Analyze (All Layers)
# ============================================================================
print("\n" + "="*70)
print("STEP 2: SAE FEATURE ANALYSIS - ALL LAYERS")
print("="*70)

all_layers_analysis = {}

for layer_num in range(model.cfg.n_layers):
    print(f"\n{'='*70}")
    print(f"ANALYZING LAYER {layer_num}")
    print(f"{'='*70}")

    # Load pretrained SAE for this layer
    try:
        release = "gpt2-small-hook-z-kk"
        sae_id = f"blocks.{layer_num}.hook_z"

        try:
            sae = SAE.from_pretrained(release, sae_id)
        except:
            sae = SAE.from_pretrained(release, sae_id)[0]

        sae.to(device)
        sae.eval()

        hook_name = f"blocks.{layer_num}.attn.hook_z"
        print(f"Using SAE: {sae_id}, Hook: {hook_name}")
        print(f"SAE feature dimension: {sae.cfg.d_sae}")

    except Exception as e:
        print(f"Could not load SAE for layer {layer_num}: {e}")
        continue

    # Extract SAE features
    print("Extracting SAE feature activations...")
    val_feature_details = []

    with torch.no_grad():
        for idx in tqdm(range(len(val_dataset)), desc=f"Layer {layer_num}"):
            sentence = val_dataset[idx]["sentence"]
            label = val_dataset[idx]["label"]

            tokens = model.to_tokens(sentence)
            _, cache = model.run_with_cache(tokens)

            hook_acts = cache[hook_name]
            sae_feature_acts = sae.encode(hook_acts)

            # Pool features across sequence
            pooled_features = sae_feature_acts.mean(dim=1).squeeze()

            # Get active features
            active_features = torch.where(pooled_features > 0)[0]
            active_values = pooled_features[active_features]

            val_feature_details.append({
                'true_label': label,
                'sentence': sentence,
                'active_features': active_features.cpu().numpy(),
                'active_values': active_values.cpu().numpy(),
                'num_active': len(active_features)
            })

    # Analyze feature patterns
    print(f"\nAnalyzing feature patterns for Layer {layer_num}...")

    # Separate features by TRUE sentiment label
    pos_features = []
    neg_features = []

    for detail in val_feature_details:
        if detail['true_label'] == 1:
            pos_features.extend(detail['active_features'])
        else:
            neg_features.extend(detail['active_features'])

    pos_feature_counts = Counter(pos_features)
    neg_feature_counts = Counter(neg_features)

    pos_unique = set(pos_features)
    neg_unique = set(neg_features)
    common_features = pos_unique & neg_unique
    pos_only = pos_unique - neg_unique
    neg_only = neg_unique - pos_unique

    jaccard = len(common_features) / len(pos_unique | neg_unique) if (pos_unique | neg_unique) else 0

    total_pos = sum(d['true_label'] == 1 for d in val_feature_details)
    total_neg = sum(d['true_label'] == 0 for d in val_feature_details)

    # Uncommon features
    pos_uncommon = {f: c for f, c in pos_feature_counts.items() if c / total_pos < 0.05}
    neg_uncommon = {f: c for f, c in neg_feature_counts.items() if c / total_neg < 0.05}

    # Very rare features
    pos_very_rare = {f: c for f, c in pos_feature_counts.items() if c / total_pos < 0.01}
    neg_very_rare = {f: c for f, c in neg_feature_counts.items() if c / total_neg < 0.01}

    # Average active features
    pos_avg_active = np.mean([d['num_active'] for d in val_feature_details if d['true_label'] == 1])
    neg_avg_active = np.mean([d['num_active'] for d in val_feature_details if d['true_label'] == 0])

    # Determine similarity interpretation
    if jaccard < 0.3:
        similarity = "VERY DIFFERENT - Strong feature separation"
    elif jaccard < 0.5:
        similarity = "MODERATELY DIFFERENT - Some distinct patterns"
    elif jaccard < 0.7:
        similarity = "MODERATELY SIMILAR - Significant overlap"
    else:
        similarity = "VERY SIMILAR - Mostly shared features"

    # Top 5 common discriminative features
    common_discriminative = []
    for feat_idx in common_features:
        pos_freq = pos_feature_counts[feat_idx] / total_pos
        neg_freq = neg_feature_counts[feat_idx] / total_neg
        diff = abs(pos_freq - neg_freq)
        common_discriminative.append((feat_idx, diff, pos_freq, neg_freq))

    common_discriminative.sort(key=lambda x: x[1], reverse=True)

    # Save layer analysis to TXT
    with open(f'/kaggle/working/results/layer_{layer_num}_analysis.txt', 'w') as f:
        f.write("="*70 + "\n")
        f.write(f"LAYER {layer_num} - SAE FEATURE ANALYSIS\n")
        f.write("="*70 + "\n\n")

        f.write("A. OVERALL STATISTICS\n")
        f.write("-"*70 + "\n")
        f.write(f"Total samples - POSITIVE: {total_pos}, NEGATIVE: {total_neg}\n")
        f.write(f"Unique features - POSITIVE: {len(pos_unique)}, NEGATIVE: {len(neg_unique)}\n")
        f.write(f"Common features (used by both): {len(common_features)}\n")
        f.write(f"POSITIVE-only features: {len(pos_only)}\n")
        f.write(f"NEGATIVE-only features: {len(neg_only)}\n")
        f.write(f"Jaccard similarity: {jaccard:.4f}\n")
        f.write(f"→ {similarity}\n\n")

        f.write("B. UNCOMMON FEATURES\n")
        f.write("-"*70 + "\n")
        f.write(f"Uncommon POSITIVE (<5%): {len(pos_uncommon)} ({len(pos_uncommon)/len(pos_unique)*100:.1f}%)\n")
        f.write(f"Uncommon NEGATIVE (<5%): {len(neg_uncommon)} ({len(neg_uncommon)/len(neg_unique)*100:.1f}%)\n")
        f.write(f"Very rare POSITIVE (<1%): {len(pos_very_rare)}\n")
        f.write(f"Very rare NEGATIVE (<1%): {len(neg_very_rare)}\n\n")

        f.write("C. ACTIVATION PATTERNS\n")
        f.write("-"*70 + "\n")
        f.write(f"Average active features - POSITIVE: {pos_avg_active:.2f}, NEGATIVE: {neg_avg_active:.2f}\n")
        f.write(f"Difference: {abs(pos_avg_active - neg_avg_active):.2f} features\n\n")

        f.write("D. TOP 5 POSITIVE FEATURES (by frequency)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, count) in enumerate(pos_feature_counts.most_common(5), 1):
            freq_pct = (count / total_pos) * 100
            neg_count = neg_feature_counts.get(feat_idx, 0)
            in_common = "COMMON" if feat_idx in common_features else "EXCLUSIVE"
            f.write(f"{rank}. Feature {feat_idx:5d}: {count:4d} activations ({freq_pct:.2f}%) | "
                   f"In NEG: {neg_count} | {in_common}\n")

        f.write("\nE. TOP 5 NEGATIVE FEATURES (by frequency)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, count) in enumerate(neg_feature_counts.most_common(5), 1):
            freq_pct = (count / total_neg) * 100
            pos_count = pos_feature_counts.get(feat_idx, 0)
            in_common = "COMMON" if feat_idx in common_features else "EXCLUSIVE"
            f.write(f"{rank}. Feature {feat_idx:5d}: {count:4d} activations ({freq_pct:.2f}%) | "
                   f"In POS: {pos_count} | {in_common}\n")

        f.write("\nF. TOP 5 COMMON FEATURES (most discriminative)\n")
        f.write("-"*70 + "\n")
        for rank, (feat_idx, diff, pos_freq, neg_freq) in enumerate(common_discriminative[:5], 1):
            bias = "POSITIVE-biased" if pos_freq > neg_freq else "NEGATIVE-biased"
            f.write(f"{rank}. Feature {feat_idx:5d}: {bias} | "
                   f"POS:{pos_freq*100:.2f}%, NEG:{neg_freq*100:.2f}% (diff:{diff*100:.2f}%)\n")

    # Store for summary
    all_layers_analysis[layer_num] = {
        'jaccard': jaccard,
        'common': len(common_features),
        'pos_only': len(pos_only),
        'neg_only': len(neg_only),
        'similarity': similarity
    }

    print(f"✓ Layer {layer_num} analysis saved to layer_{layer_num}_analysis.txt")

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("\n" + "="*70)
print("FINAL SUMMARY - ALL LAYERS")
print("="*70)

# Find layer with most different features
most_different_layer = min(all_layers_analysis.items(), key=lambda x: x[1]['jaccard'])
most_similar_layer = max(all_layers_analysis.items(), key=lambda x: x[1]['jaccard'])

# Save final summary to TXT
with open('/kaggle/working/results/final_summary.txt', 'w') as f:
    f.write("="*70 + "\n")
    f.write("FINAL SUMMARY - ALL LAYERS\n")
    f.write("="*70 + "\n\n")

    f.write("BASELINE PERFORMANCE (Zero-Shot Prompting)\n")
    f.write("-"*70 + "\n")
    f.write(f"Accuracy: {baseline_acc*100:.2f}%\n")
    f.write(f"F1-Score: {baseline_f1:.4f}\n")
    f.write(f"Method: Pretrained GPT-2 only, no training\n\n")

    f.write("FEATURE ANALYSIS SUMMARY ACROSS ALL LAYERS\n")
    f.write("-"*70 + "\n")
    f.write(f"{'Layer':<8} {'Jaccard':<12} {'Common':<10} {'POS-only':<12} {'NEG-only':<12} {'Interpretation'}\n")
    f.write("-"*90 + "\n")

    for layer_num in sorted(all_layers_analysis.keys()):
        res = all_layers_analysis[layer_num]
        interp = "DIFFERENT" if res['jaccard'] < 0.5 else "SIMILAR"
        f.write(f"Layer {layer_num:<2}  {res['jaccard']:.4f}      "
               f"{res['common']:<10} {res['pos_only']:<12} "
               f"{res['neg_only']:<12} {interp}\n")

    f.write("\n" + "="*70 + "\n")
    f.write("KEY FINDINGS\n")
    f.write("="*70 + "\n\n")

    f.write(f"→ Layer with MOST DIFFERENT features: Layer {most_different_layer[0]}\n")
    f.write(f"  Jaccard similarity: {most_different_layer[1]['jaccard']:.4f}\n")
    f.write(f"  Interpretation: Strongest separation between positive and negative sentiments\n\n")

    f.write(f"→ Layer with MOST SIMILAR features: Layer {most_similar_layer[0]}\n")
    f.write(f"  Jaccard similarity: {most_similar_layer[1]['jaccard']:.4f}\n")
    f.write(f"  Interpretation: Least separation between positive and negative sentiments\n\n")

    f.write("="*70 + "\n")
    f.write("NOTES\n")
    f.write("="*70 + "\n")
    f.write("- Baseline uses zero-shot prompting with pretrained GPT-2\n")
    f.write("- SAE features extracted from pretrained model (no training)\n")
    f.write("- Lower Jaccard = more distinct features for pos vs neg\n")
    f.write("- Higher Jaccard = more overlapping features\n")

print("\n✓ Final summary saved to final_summary.txt")

print("\n" + "="*70)
print("✓ Complete!")
print("✓ All results saved as TXT files in /kaggle/working/results/")
print("="*70)

print("\nGenerated files:")
print("  - baseline_results.txt")
for layer_num in sorted(all_layers_analysis.keys()):
    print(f"  - layer_{layer_num}_analysis.txt")
print("  - final_summary.txt")

