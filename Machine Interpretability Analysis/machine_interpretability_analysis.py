# -*- coding: utf-8 -*-
"""machine-interpretability-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DioPF542v4Qxpjo6ZJRG2ZiL-6T_aIdY
"""

!pip install sae-lens transformer_lens transformers datasets torch matplotlib scikit-learn

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2Model
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import numpy as np
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
gpt2 = GPT2Model.from_pretrained("gpt2", output_hidden_states=True).to(device)
gpt2.eval()

dataset = load_dataset("glue", "sst2")

pos_samples = [(s, l) for s, l in zip(dataset["validation"]["sentence"], dataset["validation"]["label"]) if l == 1][:25]
neg_samples = [(s, l) for s, l in zip(dataset["validation"]["sentence"], dataset["validation"]["label"]) if l == 0][:25]
samples = pos_samples + neg_samples

texts, labels = zip(*samples)

enc = tokenizer(list(texts), padding=True, truncation=True, max_length=64, return_tensors="pt")
input_ids = enc["input_ids"].to(device)
attention_mask = enc["attention_mask"].to(device)

class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=512, sparsity_weight=1e-3):
        super().__init__()
        self.encoder = nn.Linear(input_dim, hidden_dim)
        self.decoder = nn.Linear(hidden_dim, input_dim)
        self.sparsity_weight = sparsity_weight

    def forward(self, x):
        z = torch.relu(self.encoder(x))
        x_hat = self.decoder(z)
        return z, x_hat

    def loss_fn(self, x, x_hat, z):
        recon_loss = torch.nn.functional.mse_loss(x_hat, x)
        sparsity_loss = torch.mean(torch.abs(z))
        return recon_loss + self.sparsity_weight * sparsity_loss

with torch.no_grad():
    outputs = gpt2(input_ids, attention_mask=attention_mask)
    hidden_states = outputs.hidden_states  # tuple of (layer_count, batch, seq_len, hidden_dim)

colors = ["green" if l == 1 else "red" for l in labels]

for layer_idx, layer_hidden in enumerate(hidden_states):
    print(f"\nAnalyzing Layer {layer_idx} ...")

    pooled = layer_hidden.mean(dim=1).to(device)

    sae = SparseAutoencoder(768, hidden_dim=512).to(device)
    opt = torch.optim.Adam(sae.parameters(), lr=1e-4)

    loop = tqdm(range(100), desc=f"Training SAE (Layer {layer_idx})")
    for _ in loop:
        z, x_hat = sae(pooled)
        loss = sae.loss_fn(pooled, x_hat, z)
        opt.zero_grad()
        loss.backward()
        opt.step()
        loop.set_postfix(loss=loss.item())

    with torch.no_grad():
        sae_codes, _ = sae(pooled)

    sae_codes = sae_codes.cpu().numpy()

    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(sae_codes)

    tsne = TSNE(n_components=2, perplexity=10, random_state=42)
    tsne_result = tsne.fit_transform(sae_codes)

    plt.figure(figsize=(12,5))
    plt.subplot(1,2,1)
    for i, (x,y) in enumerate(pca_result):
        plt.scatter(x, y, c=colors[i], s=60)
        plt.text(x+0.2, y+0.2, f"S{i+1}", fontsize=8)
    plt.title(f"PCA of SAE codes - Layer {layer_idx}")
    plt.subplot(1,2,2)
    for i, (x,y) in enumerate(tsne_result):
        plt.scatter(x, y, c=colors[i], s=60)
        plt.text(x+0.2, y+0.2, f"S{i+1}", fontsize=8)
    plt.title(f"t-SNE of SAE codes - Layer {layer_idx}")
    plt.tight_layout()

    # Save the figure as jpeg
    plt.savefig(f'/kaggle/working/sae_pca_tsne_layer_{layer_idx}.jpeg', format='jpeg', dpi=300)

    plt.show()

print("\nSentences used for visualization (50 samples):")
for i, (sent, label) in enumerate(zip(texts, labels)):
    sentiment = "Positive" if label == 1 else "Negative"
    print(f"S{i+1} ({sentiment}): {sent}")

